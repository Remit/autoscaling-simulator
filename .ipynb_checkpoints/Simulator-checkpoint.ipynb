{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The <Noname> simulator's goal is to provide the fast and cheap testing ground\n",
    "# for the autoscaling policies/mechanism used in interactive transaction-based\n",
    "# applications deployed in the cloud using the containers.\n",
    "# The following models constitute the simulator:\n",
    "# 1) application model - determines the logical structure of the application, i.e.\n",
    "# which services communicate, how much time is required to process particular requests,\n",
    "# how large are the buffers for storing the queries awaiting the processing. Overall,\n",
    "# the application is represented as a general networked queueing model.\n",
    "# The components of the model:\n",
    "# - static -> the connections forming the logic of the application + processing times\n",
    "# - dynamic -> the amount of instances of the application services (containers)\n",
    "# 2) workload/load model - determines the load generated by the users of the application,\n",
    "# i.e. requests times, distribution of the requests in time (e.g. diurnal, seasonal),\n",
    "# the composition of the workload mixture (i.e. distribution of requests in terms of\n",
    "# required processing times, e.g. 80% small reqs, 20% large ones)\n",
    "# 3) scaling model - determines the scaling behaviour of the application, i.e. how\n",
    "# much time might be required to take/conduct the scaling decision.\n",
    "# 4) service level model - determines the expected level(s) of the service provided by\n",
    "# the application as a whole, e.g. in terms of the response times or distirbutions thereof,\n",
    "# in terms of services availability.\n",
    "# 5) platform model (hardware/ virtual machines) - models the most relevant characteristics\n",
    "# of the platform in terms of performance, e.g. the number of hw threads/cores that might\n",
    "# be needed to known to accomodate the demands of the logical service instance in terms\n",
    "# of threads. Note: we are not solving the placement problem! This is something done\n",
    "# by the cloud services provider.\n",
    "# 6) cost model - models the cost of the platform resource used during the simulation.\n",
    "# 7) failure/availability model - determines the failure mode of the platform/app, s.t.\n",
    "# the scaling procedure should be able to compensate for the unpredictably failing nodes.\n",
    "# 8) performance interference/tenancy model - determines, how much CPU stealing can happen\n",
    "# on the platform shared between the simulated application and some other application\n",
    "# co-deployed in the cloud on the same infra.\n",
    "# 9) scaling policy - provides a scaling plan that is executed by the simulation,\n",
    "# follows a particular instance or the combination thereof of scaling policies.\n",
    "# 10) network model - determines link latencies and bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulator's assumptions:\n",
    "# 1) Requests chains separation -> requests made by the user do not depend on other\n",
    "# requests by the user, although they can generate multiple other requests.\n",
    "# 2) Simulation step is smaller or equal to the processing duration at the smallest\n",
    "# component.\n",
    "# 3) No retries are performed if requests fails due to some issue, e.g. overfull buffers,\n",
    "# not enough throughput on the links, timeout reached\n",
    "# 4) All the nodes in the same geographical region. This assumption is valid since\n",
    "# the autoscalers often act independently on each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from collections import deque\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicedRequestsNumDistribution(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for generating the random number of requests\n",
    "    based on the corresponding distribution registered with it.\n",
    "    The class registered with it should define own generate method.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def generate(self):\n",
    "        pass\n",
    "    \n",
    "class NormalDistribution:\n",
    "    \"\"\"\n",
    "    Generates the random number of requests in the time slice\n",
    "    according to the normal distribution. Wraps the corresponding\n",
    "    call to the np.random.normal.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def generate(self, num = 1):\n",
    "        return np.random.normal(self.mu, self.sigma, num)\n",
    "\n",
    "# Registering the derived sliced request number generators\n",
    "SlicedRequestsNumDistribution.register(NormalDistribution)\n",
    "\n",
    "class WorkloadModel:\n",
    "    \"\"\"\n",
    "    Represents the workload generation model.\n",
    "    The parameters are taken from the corresponding JSON file passed to the ctor.\n",
    "    \n",
    "    Properties:\n",
    "        reqs_types_ratios (dict): ratio of requests (value) of the given request type (key) in the mixture\n",
    "        reqs_generators (dict): random sliced requests num generator (value) for the given request type (key)\n",
    "        \n",
    "        workload (dict): array of the numbers of requests generated for the timestamp (value) for the given\n",
    "                         request type\n",
    "                     \n",
    "    Methods:\n",
    "        generate_requests (timestamp): generates a mixture of requests (list) using the reqs_types_ratios and\n",
    "                                       reqs_generators with the provided timestamp.\n",
    "    \n",
    "    Usage:\n",
    "        wkldmdl = WorkloadModel(filename = 'experiments/test/workload.json')\n",
    "        len(wkldmdl.generate_requests(100))\n",
    "        \n",
    "    TODO:\n",
    "        implement support for the seasonal reqs num generation component\n",
    "        implement support for holidays etc.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 reqs_types_ratios = None,\n",
    "                 filename = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.reqs_types_ratios = {}\n",
    "        self.reqs_generators = {}\n",
    "        \n",
    "        if filename is None:\n",
    "            raise ValueError('Configuration file not provided for the WorkloadModel.')\n",
    "        else:\n",
    "            with open(filename) as f:\n",
    "                config = json.load(f)\n",
    "                for conf in config[\"workloads_configs\"]:\n",
    "                    req_type = conf[\"request_type\"]\n",
    "                    req_ratio = conf[\"workload_config\"][\"ratio\"]\n",
    "                    if req_ratio < 0.0 or req_ratio > 1.0:\n",
    "                        raise ValueError('Unacceptable ratio value for the request of type {}.'.format(req_type))\n",
    "                    self.reqs_types_ratios[req_type] = req_ratio\n",
    "                     \n",
    "                    req_distribution_type = conf[\"workload_config\"][\"sliced_distribution\"][\"type\"]\n",
    "                    req_distribution_params = conf[\"workload_config\"][\"sliced_distribution\"][\"params\"]\n",
    "                    \n",
    "                    if req_distribution_type == \"normal\":\n",
    "                        mu = 0.0\n",
    "                        sigma = 0.1\n",
    "                        \n",
    "                        if len(req_distribution_params) > 0:\n",
    "                            mu = req_distribution_params[0]\n",
    "                        if len(req_distribution_params) > 1:\n",
    "                            sigma = req_distribution_params[1]\n",
    "                        \n",
    "                        self.reqs_generators[req_type] = NormalDistribution(mu, sigma)\n",
    "                    \n",
    "                    # TODO: processing of the \"seasonal_pattern_p_h\"\n",
    "                    \n",
    "        # Dynamic state  \n",
    "        self.workload = {}    \n",
    "        \n",
    "    def generate_requests(self,\n",
    "                          timestamp):\n",
    "        gen_reqs = []\n",
    "        \n",
    "        for req_type, ratio in self.reqs_types_ratios.items():\n",
    "            num_reqs = self.reqs_generators[req_type].generate()\n",
    "            req_types_reqs_num = int(ratio * num_reqs)\n",
    "            for i in range(req_types_reqs_num):\n",
    "                req = Request(req_type)\n",
    "                gen_reqs.append(req)\n",
    "                \n",
    "            if req_type in self.workload:\n",
    "                self.workload[req_type].append((timestamp, req_types_reqs_num))\n",
    "            else:\n",
    "                self.workload[req_type] = [(timestamp, req_types_reqs_num)]\n",
    "                \n",
    "        return gen_reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    def __init__(self,\n",
    "                 request_type,\n",
    "                 request_id = None):\n",
    "        # Static state\n",
    "        self.request_type = request_type\n",
    "        if request_id is None:\n",
    "            self.request_id = uuid.uuid1()\n",
    "        else:\n",
    "            self.request_id = request_id\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.processing_left_ms = 0\n",
    "        self.cumulative_time_ms = 0\n",
    "        self.upstream = True\n",
    "        self.replies_expected = 1 # to implement the fan-in on the level of service\n",
    "        \n",
    "    def set_downstream(self):\n",
    "        self.upstream = False\n",
    "    \n",
    "class RequestProcessingInfo:\n",
    "    def __init__(self,\n",
    "                 request_type,\n",
    "                 entry_service,\n",
    "                 processing_times,\n",
    "                 timeout_ms,\n",
    "                 request_size_b,\n",
    "                 response_size_b,\n",
    "                 request_operation_type):\n",
    "        \n",
    "        self.request_type = request_type\n",
    "        self.entry_service = entry_service\n",
    "        self.processing_times = processing_times\n",
    "        self.timeout_ms = timeout_ms\n",
    "        self.request_size_b = request_size_b\n",
    "        self.response_size_b = response_size_b\n",
    "        self.request_operation_type = request_operation_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkBuffer:\n",
    "    \"\"\"\n",
    "    Combines link and the buffer. The requests in the link advance when the step method is called.\n",
    "    If the processing time left (i.e. waiting on the link) is over, the request proceeds to the\n",
    "    buffer, where it can be extracted from for the further processing in a service.\n",
    "    If the used throughput reached the throughput limit, the request is lost. The same happens\n",
    "    if upon the transition to the buffer, the buffer is full for the given request type.\n",
    "    \n",
    "    Properties:\n",
    "    \n",
    "        As buffer:\n",
    "            capacity_by_request_type (dict) - holds capacity of the buffer by request type, in terms of\n",
    "                                              requests currently placed in the buffer\n",
    "            [STUB] policy                   - policy used for moving requests in the buffer, e.g. FIFO/LIFO\n",
    "            \n",
    "            requests (collections.deque)    - holds current requests in the buffer\n",
    "            reqs_cnt (dict)                 - holds current request count by the request type, used to rapidly check\n",
    "                                              if more requests of the given type can be accomodated in the buffer\n",
    "            \n",
    "        As link:\n",
    "            latency_ms (int)                - latency of the link in milliseconds, taken from the config\n",
    "            throughput_mbps (int)           - throughput of the link in Megabytes per sec, taken from the config\n",
    "            request_processing_infos (dict) - holds requests processing information to compute the used\n",
    "                                              throughput etc.\n",
    "            \n",
    "            requests_in_transfer (list)     - holds requests that are currently \"transferred\" by this link\n",
    "            used_throughput_mbps (int)      - throughput currently used on this link by the \"transferred\" reqs\n",
    "        \n",
    "    Methods:\n",
    "    \n",
    "        As buffer:\n",
    "            append_left (req)               - puts the request req at the beginning of the buffer to give other\n",
    "                                              requests opportunity to be processed if the current request waits\n",
    "                                              for other replies to get processed (fan-in)\n",
    "            pop                             - takes the last added request out of the buffer for processing (LIFO)\n",
    "            pop_left                        - takes the first added request out of the buffer for processing (FIFO)\n",
    "            size                            - returns size of the buffer\n",
    "            add_cumulative_time (delta)     - adds time delta to every request in the buffer\n",
    "            remove_by_id (request_id)       - removes all the requests with request id request_id from the buffer\n",
    "            \n",
    "        As link:\n",
    "            put (req)                       - puts a new request req on a link, i.e. in requests_in_transfer,\n",
    "                                              if there is enough spare throughput; otherwise drops the request\n",
    "            step (simulation_step_ms)       - makes a discrete simulation time step of the length simulation_step_ms\n",
    "                                              to advance the requests held on the link, i.e. in requests_in_transfer,\n",
    "                                              and to put them into the buffer if possible (capacity left). In case of\n",
    "                                              no spare capacity in the buffer, the request is also dropped.\n",
    "        \n",
    "            _req_occupied_mbps (req)        - private method that computes the throughput used by the request req\n",
    "    \n",
    "    TODO:\n",
    "        implementing scaling of the links? e.g. according to the added instances of services.\n",
    "        implement wrapping of the lower-level details into policies like LIFO, FIFO, etc. hide append, pop etc.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 capacity_by_request_type,\n",
    "                 request_processing_infos,\n",
    "                 latency_ms = 10, # TODO: remove val\n",
    "                 throughput_mbps = 1000, # TODO: remove val\n",
    "                 policy = \"FIFO\"): \n",
    "        \n",
    "        # Static state\n",
    "        # Buffer:\n",
    "        self.capacity_by_request_type = capacity_by_request_type\n",
    "        self.policy = policy\n",
    "        \n",
    "        # Link:\n",
    "        self.latency_ms = latency_ms\n",
    "        self.throughput_mbps = throughput_mbps\n",
    "        self.request_processing_infos = request_processing_infos\n",
    "        \n",
    "        # Dynamic state\n",
    "        # Buffer:\n",
    "        self.requests = deque([])\n",
    "        self.reqs_cnt = {}\n",
    "        for request_type in capacity_by_request_type.keys():\n",
    "            self.reqs_cnt[request_type] = 0\n",
    "            \n",
    "        # Link:\n",
    "        self.requests_in_transfer = []\n",
    "        self.used_throughput_mbps = 0\n",
    "    \n",
    "    def step(self, simulation_step_ms):\n",
    "        \"\"\" Processing requests to bring them from the link into the buffer \"\"\"\n",
    "        for req in self.requests_in_transfer:\n",
    "            min_time_to_subtract_ms = min(req.processing_left_ms, simulation_step_ms)\n",
    "            req.processing_left_ms -= min_time_to_subtract_ms\n",
    "            if req.processing_left_ms <= 0:\n",
    "                capacity = self.capacity_by_request_type[req.request_type]\n",
    "                self.used_throughput_mbps -= self._req_occupied_mbps(req)\n",
    "                \n",
    "                if self.reqs_cnt[req.request_type] == capacity:\n",
    "                    del req # dropping the request if no spare capacity\n",
    "                else:\n",
    "                    req.cumulative_time_ms += min_time_to_subtract_ms\n",
    "                    if req.cumulative_time_ms >= self.request_processing_infos[req.request_type].timeout_ms:\n",
    "                        del req\n",
    "                    else:\n",
    "                        self.requests.append(req)\n",
    "                        self.reqs_cnt[req.request_type] += 1 \n",
    "    \n",
    "    def put(self, req):\n",
    "        req_size_b_mbps = self._req_occupied_mbps(req)\n",
    "\n",
    "        if self.throughput_mbps - self.used_throughput_mbps >= req_size_b_mbps:\n",
    "            self.used_throughput_mbps += req_size_b_mbps\n",
    "            req.processing_left_ms = self.latency_ms\n",
    "            self.requests_in_transfer.append(req)\n",
    "        else:\n",
    "            del req\n",
    "    \n",
    "    def append_left(self, req):\n",
    "        self.requests.appendLeft(req)\n",
    "    \n",
    "    def pop(self):\n",
    "        req = None\n",
    "        if len(self.requests) > 0:\n",
    "            req = self.requests.pop()\n",
    "            self.reqs_cnt[req.request_type] -= 1\n",
    "            \n",
    "        return req\n",
    "    \n",
    "    def pop_left(self):\n",
    "        req = None\n",
    "        if len(self.requests) > 0:\n",
    "            req = self.requests.popLeft()\n",
    "            self.reqs_cnt[req.request_type] -= 1\n",
    "        \n",
    "        return req\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.requests)\n",
    "    \n",
    "    def add_cumulative_time(self, delta):\n",
    "        for req in self.requests:\n",
    "            req.cumulative_time_ms += delta\n",
    "    \n",
    "    def remove_by_id(self, request_id):\n",
    "        for req in reversed(self.requests):\n",
    "            if req.request_id == request_id:\n",
    "                self.requests.remove(req)\n",
    "                \n",
    "    def _req_occupied_mbps(self, req):\n",
    "        req_size_b = 0\n",
    "        if req.upstream:\n",
    "            req_size_b = self.request_processing_infos[req.request_type].request_size_b\n",
    "        else:\n",
    "            req_size_b = self.request_processing_infos[req.request_type].response_size_b\n",
    "        req_size_b_mb = req_size_b / (1024 * 1024)\n",
    "        req_size_b_mbps = req_size_b_mb * (self.latency_ms / 1000) # taking channel for that long\n",
    "        return req_size_b_mbps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Service:\n",
    "    def __init__(self,\n",
    "                 buffer_capacity_by_request_type,\n",
    "                 threads_per_instance,\n",
    "                 request_processing_infos,\n",
    "                 instances = 1,\n",
    "                 state_mb = 0):\n",
    "        \n",
    "        # Static state\n",
    "        # TODO: should replace with the real platform info below!\n",
    "        self.hw_threads_per_instance = 4\n",
    "        self.threads_per_instance = threads_per_instance\n",
    "        # If state_mb is 0, then the service is stateless\n",
    "        self.state_mb = state_mb\n",
    "        \n",
    "        # Dynamic state\n",
    "        # Upstream and downstream links/buffers of the service            \n",
    "        self.upstream_buf = LinkBuffer(buffer_capacity_by_request_type, request_processing_infos)\n",
    "        self.downstream_buf = LinkBuffer(buffer_capacity_by_request_type, request_processing_infos)\n",
    "        # Scaling-related\n",
    "        self.instances = instances\n",
    "        # requests that are currently in simultaneous processing\n",
    "        self.in_processing_simultaneous = []\n",
    "        # requests that are processed in this step, they can proceed\n",
    "        self.out = []\n",
    "        \n",
    "    def add_request(self, req):\n",
    "        # decide where to put the request\n",
    "        if req.upstream:\n",
    "            self.upstream_buf.put(req)\n",
    "        else:\n",
    "            self.downstream_buf.put(req)\n",
    "        \n",
    "    def step(self, simulation_step_ms):\n",
    "        processing_time_left_at_step = simulation_step_ms\n",
    "        \n",
    "        # Propagating requests in the link\n",
    "        self.downstream_buf.step(simulation_step_ms)  \n",
    "        self.upstream_buf.step(simulation_step_ms)\n",
    "        \n",
    "        while(processing_time_left_at_step > 0):\n",
    "           \n",
    "            if (self.downstream_buf.size() == 0) and (self.upstream_buf.size() == 0):\n",
    "                processing_time_left_at_step = 0\n",
    "                continue\n",
    "                \n",
    "            if len(self.in_processing_simultaneous) > 0:\n",
    "                # Find minimal leftover duration, subtract it,\n",
    "                # and propagate the request\n",
    "                min_leftover_time = min([req.processing_left_ms for req in self.in_processing_simultaneous])\n",
    "                min_time_to_subtract = min(min_leftover_time, processing_time_left_at_step)\n",
    "                new_in_processing_simultaneous = []\n",
    "                \n",
    "                for req in self.in_processing_simultaneous:\n",
    "                    new_time_left = req.processing_left_ms - min_time_to_subtract\n",
    "                    req.cumulative_time_ms += min_time_to_subtract\n",
    "                    if new_time_left > 0:\n",
    "                        req.processing_left_ms = new_time_left\n",
    "                        new_in_processing_simultaneous.append(req)\n",
    "                    else:\n",
    "                        # Request is put into the out buffer to be\n",
    "                        # processed further according to the app structure\n",
    "                        req.processing_left_ms = 0  \n",
    "                        self.out.append(req)\n",
    "                \n",
    "                processing_time_left_at_step -= min_time_to_subtract\n",
    "                self.in_processing_simultaneous = new_in_processing_simultaneous\n",
    "                \n",
    "            spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            # Assumption: first we try to process the downstream reqs to\n",
    "            # provide the response faster, but overall it is application-dependent\n",
    "            while ((self.downstream_buf.size() > 0) or (self.upstream_buf.size() > 0)) and (spare_capacity > 0):\n",
    "                if self.downstream_buf.size() > 0:\n",
    "                    req = self.downstream_buf.requests[-1]\n",
    "                    # Processing fan-in case\n",
    "                    if req.replies_expected > 1:\n",
    "                        req_id_ref = req.request_id\n",
    "                        reqs_present = 1\n",
    "                        for req_lookup in self.downstream_buf.requests[:-1]:\n",
    "                            if req_lookup.request_id == req_id_ref:\n",
    "                                reqs_present += 1\n",
    "                                \n",
    "                        if reqs_present == req.replies_expected:\n",
    "                            req = self.downstream_buf.pop()\n",
    "                            # Removing all the related requests\n",
    "                            self.downstream_buf.remove_by_id(req_id_ref)\n",
    "                        else:\n",
    "                            # pushing to the beginning of the deque to enable\n",
    "                            # progress in processing the downstream reqs\n",
    "                            req = self.downstream_buf.pop()\n",
    "                            self.downstream_buf.append_left(req)\n",
    "                            req = None\n",
    "                            \n",
    "                    else: \n",
    "                        req = self.downstream_buf.pop()\n",
    "                    \n",
    "                    if not req is None:\n",
    "                        req.replies_expected = 1\n",
    "                        self.in_processing_simultaneous.append(req)\n",
    "                        spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            \n",
    "                if self.upstream_buf.size() > 0:\n",
    "                    req = self.upstream_buf.pop()\n",
    "                    self.in_processing_simultaneous.append(req)\n",
    "                    spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            \n",
    "        # Increase the cumulative time for all the reqs left in the buffers waiting\n",
    "        self.upstream_buf.add_cumulative_time(simulation_step_ms)\n",
    "        self.downstream_buf.add_cumulative_time(simulation_step_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application model incorps params of the requests since it depends on the\n",
    "# structure and the processing logic of the app, whereas the workload model\n",
    "# quantifies the amount and the distribution of the requests in time/volume\n",
    "# so, propagation chain of the request goes into the app model\n",
    "\n",
    "# Services: {'frontend': service1, 'appserver': service2, 'db': service3}\n",
    "# Structure: {'frontend': ['appserver'], 'appserver': ['db'], 'db': None}\n",
    "# Starting points: ['frontend']\n",
    "# reqs_processing_infos: {'auth': req_proc_info1} -> also store the reverse path!\n",
    "class ApplicationModel:\n",
    "    def __init__(self,\n",
    "                 filename = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.name = None\n",
    "        self.services = {}\n",
    "        self.structure = {}\n",
    "        self.reqs_processing_infos = {}\n",
    "        \n",
    "        if filename is None:\n",
    "            raise ValueError('Configuration file not provided for the ApplicationModel.')\n",
    "        else:\n",
    "            with open(filename) as f:\n",
    "                config = json.load(f)\n",
    "                \n",
    "                self.name = config[\"app_name\"]\n",
    "                \n",
    "                for request_info in config[\"requests\"]:\n",
    "                    request_type = request_info[\"request_type\"]\n",
    "                    entry_service = request_info[\"entry_service\"]\n",
    "                    \n",
    "                    processing_times = {}\n",
    "                    for processing_time_service_entry in request_info[\"processing_times_ms\"]:\n",
    "                        service_name = processing_time_service_entry[\"service\"]\n",
    "                        upstream_time = processing_time_service_entry[\"upstream\"]\n",
    "                        if upstream_time < 0:\n",
    "                            raise ValueError('The upstream time for the request {} when passing through the service {} is negative.'.format(request_type, service_name))\n",
    "                        downstream_time = processing_time_service_entry[\"downstream\"]\n",
    "                        if downstream_time < 0:\n",
    "                            raise ValueError('The downstream time for the request {} when passing through the service {} is negative.'.format(request_type, service_name))\n",
    "                        \n",
    "                        processing_times[service_name] = [upstream_time, downstream_time]\n",
    "                    \n",
    "                    timeout_ms = request_info[\"timeout_ms\"]\n",
    "                    if timeout_ms < 0:\n",
    "                        raise ValueError('The timeout value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    request_size_b = request_info[\"request_size_b\"]\n",
    "                    if request_size_b < 0:\n",
    "                        raise ValueError('The request size value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    response_size_b = request_info[\"response_size_b\"]\n",
    "                    if response_size_b < 0:\n",
    "                        raise ValueError('The response size value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    request_operation_type = request_info[\"operation_type\"]\n",
    "                    \n",
    "                    req_proc_info = RequestProcessingInfo(request_type,\n",
    "                                                          entry_service,\n",
    "                                                          processing_times,\n",
    "                                                          timeout_ms,\n",
    "                                                          request_size_b,\n",
    "                                                          response_size_b,\n",
    "                                                          request_operation_type)\n",
    "                    self.reqs_processing_infos[request_type] = req_proc_info\n",
    "                \n",
    "                for service_config in config[\"services\"]:\n",
    "                    # Creating & adding the service:\n",
    "                    service_name = service_config[\"name\"]\n",
    "                    \n",
    "                    buffer_capacity_by_request_type = {}\n",
    "                    for buffer_capacity_config in service_config[\"buffer_capacity_by_request_type\"]:\n",
    "                        request_type = buffer_capacity_config[\"request_type\"]\n",
    "                        capacity = buffer_capacity_config[\"capacity\"]\n",
    "                        if capacity <= 0:\n",
    "                            raise ValueError('Buffer capacity is not positive for request type {} of service {}.'.format(request_type, service_name))\n",
    "                        buffer_capacity_by_request_type[request_type] = capacity\n",
    "                        \n",
    "                    threads_per_instance = service_config[\"threads_per_instance\"]\n",
    "                    if threads_per_instance <= 0:\n",
    "                        raise ValueError('Threads per instance is not positive for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    starting_instances_num = service_config[\"starting_instances_num\"]\n",
    "                    if starting_instances_num <= 0:\n",
    "                        raise ValueError('Number of service instances to start with is not positive for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    state_mb = service_config[\"state_mb\"]\n",
    "                    if state_mb < 0:\n",
    "                        raise ValueError('The state size is negative for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    service = Service(buffer_capacity_by_request_type = buffer_capacity_by_request_type,\n",
    "                                      threads_per_instance = threads_per_instance,\n",
    "                                      request_processing_infos = self.reqs_processing_infos,\n",
    "                                      instances = starting_instances_num,\n",
    "                                      state_mb = state_mb)\n",
    "                    \n",
    "                    self.services[service_name] = service\n",
    "                    \n",
    "                    # Adding the links of the given service to the structure.\n",
    "                    # TODO: think of whether the broken simmetry of the links\n",
    "                    # is appropriate.\n",
    "                    next_services = service_config[\"next\"]\n",
    "                    prev_services = service_config[\"prev\"]\n",
    "                    if len(next_services) == 0:\n",
    "                        next_services = None\n",
    "                    if len(prev_services) == 0:\n",
    "                        prev_services = None\n",
    "                    self.structure[service_name] = {'next': next_services, 'prev': prev_services}\n",
    "                    \n",
    "                \n",
    "        \n",
    "        # Dynamic state\n",
    "        self.new_requests = []\n",
    "        self.response_times_by_request = {}\n",
    "        \n",
    "    def step(self, simulation_step_ms):\n",
    "        if len(self.new_requests) > 0:\n",
    "            for req in self.new_requests:\n",
    "                entry_service = self.reqs_processing_infos[req.request_type].entry_service\n",
    "                req.processing_left_ms = self.reqs_processing_infos[req.request_type].processing_times[entry_service][1]\n",
    "                self.services[entry_service].add_request(req)\n",
    "                \n",
    "            self.new_requests = []\n",
    "        \n",
    "        # Proceed through the services // fan-in merge and fan-out copying\n",
    "        # is done in the app logic since it knows the structure and times\n",
    "        # IMPORTANT: the simulation step should be small for the following\n",
    "        # processing to work correctly ~5-10 ms.\n",
    "        for service_name, service in self.services.items():\n",
    "            service.step(simulation_step_ms)\n",
    "            \n",
    "            while len(service.out) > 0:\n",
    "                req = service.out.pop()\n",
    "                req_info = self.reqs_processing_infos[req.request_type]\n",
    "                \n",
    "                if req.upstream:\n",
    "                    next_services_names = self.structure[service_name]['next']\n",
    "                    if not next_services_names is None:\n",
    "                        for next_service_name in next_services_names:\n",
    "                            if next_service_name in req_info.processing_times:\n",
    "                                req_cpy = req\n",
    "                                req_cpy.processing_left_ms = req_info.processing_times[next_service_name][0]\n",
    "                                self.services[next_service_name].add_request(req_cpy)\n",
    "                    else:\n",
    "                        # Sending response\n",
    "                        req.upstream = False\n",
    "                        \n",
    "                if not req.upstream:\n",
    "                    prev_services_names = self.structure[service_name]['prev']\n",
    "                    \n",
    "                    if not prev_services_names is None:\n",
    "                        replies_expected = 0\n",
    "                        for prev_service_name in prev_services_names:\n",
    "                            if prev_service_name in req_info.processing_times:\n",
    "                                replies_expected += 1\n",
    "                        \n",
    "                        for prev_service_name in prev_services_names:\n",
    "                            if prev_service_name in req_info.processing_times:\n",
    "                                req_cpy = req\n",
    "                                req_cpy.processing_left_ms = req_info.processing_times[prev_service_name][1]\n",
    "                                req_cpy.replies_expected = replies_expected\n",
    "                                self.services[prev_service_name].add_request(req_cpy)\n",
    "                    else:\n",
    "                        # Response received by the user\n",
    "                        if req.request_type in self.response_times_by_request:\n",
    "                            self.response_times_by_request[req.request_type].append(req.cumulative_time_ms)\n",
    "                        else:\n",
    "                            self.response_times_by_request[req.request_type] = [req.cumulative_time_ms]\n",
    "                        del req\n",
    "                        \n",
    "    def enter_requests(self, new_requests):\n",
    "        self.new_requests = new_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    \"\"\"\n",
    "    Manages the high-level simulation process and stores all the simulation-relevant variables.\n",
    "    Before the time time_to_simulate_ms is reached, each simulation_step_ms milliseconds a new\n",
    "    simulation step is taken by calling _step method. At each simulations step:\n",
    "    1) new requests entering the simulation are generated by the workload_model,\n",
    "    2) the generated requests are added to the application_model,\n",
    "    3) a simulation step of the application model is taken, which implies taking the corresponding\n",
    "       simulation steps on its services and links/buffers.\n",
    "    If the results_dir is provided then the resulting response times for the requests and the\n",
    "    workload generated per timestamp is stored in a pickle file marked with the application name\n",
    "    taken from the config file and the date and time of the simulation.\n",
    "    The progress of the simulation is tracked and presented by the progress bar.\n",
    "    \n",
    "    Properties:\n",
    "    \n",
    "        workload_model                  - the model that is used to generate the workload\n",
    "        application_model               - the application model that is simulated by calling its step() method\n",
    "        time_to_simulate_ms (int)       - the interval of time that should be simulated, in milliseconds\n",
    "        simulation_step_ms (int)        - the discretion of the simulation. IMPORTANT: in order to apporach the\n",
    "                                          asynchronous behaviour of the real applications as close as possible\n",
    "                                          this parameter should be as small as possible. Meaningful numbers for\n",
    "                                          this parameters are around 10-20 ms, smaller ones may result in\n",
    "                                          very large execution time of the simulation. Ideally, this step should\n",
    "                                          be not larger than the smallest time required to process any request\n",
    "                                          on link/in service.\n",
    "        stat_updates_every_round (int)  - every stat_updates_every_round<th> round a stat summary is printed.\n",
    "                                          If equals 0 then only the progress bar is available, which makes\n",
    "                                          impossible to observe the time spent at each stat_updates_every_round\n",
    "                                          rounds.\n",
    "        results_dir (string)            - a directory used to store the results of the simulation, i.e.\n",
    "                                          reponse times of individual requests and the time series of the\n",
    "                                          generated workload. If dir does not exist, it is created.\n",
    "        \n",
    "        cur_simulation_time_ms (int)    - current simulation time in milliseconds; on the initialization of the\n",
    "                                          simulation it is set up to equal simulation_start_datetime converted\n",
    "                                          to ms. Increments by simulation_step_ms on each simulation step. \n",
    "        sim_round (int)                 - current simulation round.\n",
    "    \n",
    "    Methods:\n",
    "    \n",
    "        start                           - main simulation loop which continues until the simulation runs\n",
    "                                          out of time, i.e. cur_simulation_time_ms > time_to_simulate_ms.\n",
    "        _step                           - a private simulation step method that should only be called\n",
    "                                          inside the start method.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 workload_model,\n",
    "                 application_model,\n",
    "                 simulation_start_datetime,\n",
    "                 time_to_simulate_days = 0.0005,\n",
    "                 simulation_step_ms = 10,\n",
    "                 stat_updates_every_round = 0,\n",
    "                 results_dir = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.workload_model = workload_model\n",
    "        self.application_model = application_model\n",
    "        self.time_to_simulate_ms = int(simulation_start_datetime.timestamp() * 1000) + int(time_to_simulate_days * 24 * 60 * 60 * 1000)\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        self.stat_updates_every_round = stat_updates_every_round\n",
    "        self.results_dir = results_dir\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.cur_simulation_time_ms = int(simulation_start_datetime.timestamp() * 1000)\n",
    "        self.sim_round = 0\n",
    "        \n",
    "    def start(self):\n",
    "        \n",
    "        left_to_simulate_ms = self.time_to_simulate_ms - self.cur_simulation_time_ms\n",
    "        left_to_simulate_steps = left_to_simulate_ms // self.simulation_step_ms\n",
    "        \n",
    "        with tqdm(total = left_to_simulate_steps) as progress_bar:\n",
    "            while(self.cur_simulation_time_ms <= self.time_to_simulate_ms):\n",
    "                self.cur_simulation_time_ms += self.simulation_step_ms\n",
    "                self._step()\n",
    "                self.sim_round += 1\n",
    "                if self.stat_updates_every_round > 0:\n",
    "                    if self.sim_round % self.stat_updates_every_round == 0:\n",
    "                        left_to_simulate_ms = self.time_to_simulate_ms - self.cur_simulation_time_ms\n",
    "                        left_to_simulate_steps = left_to_simulate_ms // self.simulation_step_ms\n",
    "                        print('[{}] Left to simulate: {} min or {} steps'.format(datetime.now(),\n",
    "                                                                                 int(left_to_simulate_ms / (1000 * 60)),\n",
    "                                                                                 left_to_simulate_steps))\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        # Storing the simulation results on a disk\n",
    "        if not self.results_dir is None:\n",
    "            filename = self.application_model.name + \"DT\" + datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\") + \".pkl\"\n",
    "            if not os.path.exists(self.results_dir):\n",
    "                os.mkdir(self.results_dir)\n",
    "                full_filename = os.path.join(results_dir, filename)\n",
    "                results_to_store = {\"response_times_by_request\": self.application_model.response_times_by_request,\n",
    "                                    \"workload\": self.workload_model.workload}\n",
    "                \n",
    "                with open(full_filename, 'wb') as f:\n",
    "                    pickle.dump(results_to_store, f)\n",
    "    \n",
    "    def _step(self):\n",
    "        new_requests = self.workload_model.generate_requests(self.cur_simulation_time_ms)\n",
    "        self.application_model.enter_requests(new_requests)\n",
    "        self.application_model.step(self.simulation_step_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1117/4319 [01:25<04:05, 13.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-aebacec6e295>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mappm_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mApplicationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'experiments/test/application.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwlm_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappm_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# TODO: check how to make the duration of the step ~constant, now it increases - 3, 5, 8 mins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-52345ffab0e5>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_simulation_time_ms\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_to_simulate_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_simulation_time_ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation_step_ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_round\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat_updates_every_round\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-52345ffab0e5>\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnew_requests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkload_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_simulation_time_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplication_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_requests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplication_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation_step_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-56fc849d43e7>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, simulation_step_ms)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# processing to work correctly ~5-10 ms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mservice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mservice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_step_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-02d2d75ba61e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, simulation_step_ms)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Propagating requests in the link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownstream_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_step_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupstream_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_step_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-af959388ebbb>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, simulation_step_ms)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_time_ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmin_time_to_subtract_ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_time_ms\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_processing_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                         \u001b[0;32mdel\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wlm_test = WorkloadModel(filename = 'experiments/test/workload.json')\n",
    "appm_test = ApplicationModel('experiments/test/application.json')\n",
    "sim = Simulation(wlm_test, appm_test, datetime.now())\n",
    "sim.start()\n",
    "# TODO: check how to make the duration of the step ~constant, now it increases - 3, 5, 8 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2784., 1395., 1397., 2790., 1392., 1402., 1392., 2794., 1401.,\n",
       "        1390., 1401., 2791., 1389., 1410., 1387., 2795., 1405.,  720.,\n",
       "        5965., 3116., 3571., 3373., 6936., 3588., 3374., 3574., 7021.,\n",
       "        3791., 8017., 1073.]),\n",
       " array([ 30.        ,  42.66666667,  55.33333333,  68.        ,\n",
       "         80.66666667,  93.33333333, 106.        , 118.66666667,\n",
       "        131.33333333, 144.        , 156.66666667, 169.33333333,\n",
       "        182.        , 194.66666667, 207.33333333, 220.        ,\n",
       "        232.66666667, 245.33333333, 258.        , 270.66666667,\n",
       "        283.33333333, 296.        , 308.66666667, 321.33333333,\n",
       "        334.        , 346.66666667, 359.33333333, 372.        ,\n",
       "        384.66666667, 397.33333333, 410.        ]),\n",
       " <a list of 30 Patch objects>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n_bins = 30\n",
    "\n",
    "# Generate a normal distribution, center at x=0 and y=5\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs[0].hist(sim.application_model.response_times_by_request['auth'], bins=n_bins)\n",
    "axs[1].hist(sim.application_model.response_times_by_request['buy'], bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.median(sim.application_model.response_times_by_request['auth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        \n",
    "rt = [Test(), Test(), Test()]\n",
    "for t in rt:\n",
    "    t.num += 4\n",
    "\n",
    "for t in rt:\n",
    "    print(t.num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt = [1, 2, 3]\n",
    "rt.remove(1)\n",
    "rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider scaling links?\n",
    "class Link:\n",
    "    def __init__(self,\n",
    "                 simulation_step_ms,\n",
    "                 latency,\n",
    "                 bandwidth = 1000):\n",
    "        # Static state\n",
    "        self.latency = latency\n",
    "        self.bandwidth = bandwidth\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.requests = [] \n",
    "        \n",
    "    def put_request(self, req):\n",
    "        if len(self.requests) < self.bandwidth:\n",
    "            req.processing_left_ms = self.latency\n",
    "            self.requests.append(req)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_requests(self):\n",
    "        # Called every simulation step thus updating reqs\n",
    "        reqs_to_give = []\n",
    "        for req in self.requests:\n",
    "            min_time_left = min(req.processing_left_ms, self.simulation_step_ms)\n",
    "            if req.processing_left_ms - min_time_left <= 0:\n",
    "                req.processing_left_ms = 0\n",
    "                req.cumulative_time_ms += min_time_left\n",
    "                reqs_to_give.append(req)\n",
    "            else:\n",
    "                req.processing_left_ms -= min_time_left\n",
    "                \n",
    "        return reqs_to_give\n",
    "\n",
    "class NetworkModel:\n",
    "    def __init__(self,\n",
    "                 links_dict_in = None):\n",
    "        self.links_dict_in = links_dict_in\n",
    "        self.links_dict_out = {}\n",
    "        \n",
    "        # TODO: from file\n",
    "        for link_start, outs in self.links_dict.items():\n",
    "            for out, link in outs.items():\n",
    "                if out in self.links_dict_out:\n",
    "                    self.links_dict_out[out].append(link)\n",
    "                else:\n",
    "                    self.links_dict_out[out] = [link]\n",
    "                    \n",
    "    def put_request(self,\n",
    "                    start_service_lbl,\n",
    "                    end_service_lbl,\n",
    "                    req):\n",
    "        \n",
    "        self.links_dict_in[start_service_lbl][end_service_lbl].put_request(req)\n",
    "    \n",
    "    def get_requests(self,\n",
    "                     end_service_lbl):\n",
    "        reqs = []\n",
    "        links = self.links_dict_out[end_service_lbl]\n",
    "        for link in links:\n",
    "            reqs.extend(link.get_requests())\n",
    "            \n",
    "        return reqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
