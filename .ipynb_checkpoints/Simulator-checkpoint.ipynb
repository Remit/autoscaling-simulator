{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The <Noname> simulator's goal is to provide the fast and cheap testing ground\n",
    "# for the autoscaling policies/mechanism used in interactive transaction-based\n",
    "# applications deployed in the cloud using the containers.\n",
    "# The following models constitute the simulator:\n",
    "# 1) application model - determines the logical structure of the application, i.e.\n",
    "# which services communicate, how much time is required to process particular requests,\n",
    "# how large are the buffers for storing the queries awaiting the processing. Overall,\n",
    "# the application is represented as a general networked queueing model.\n",
    "# The components of the model:\n",
    "# - static -> the connections forming the logic of the application + processing times\n",
    "# - dynamic -> the amount of instances of the application services (containers)\n",
    "# 2) workload/load model - determines the load generated by the users of the application,\n",
    "# i.e. requests times, distribution of the requests in time (e.g. diurnal, seasonal),\n",
    "# the composition of the workload mixture (i.e. distribution of requests in terms of\n",
    "# required processing times, e.g. 80% small reqs, 20% large ones)\n",
    "# 3) scaling model - determines the scaling behaviour of the application, i.e. how\n",
    "# much time might be required to take/conduct the scaling decision.\n",
    "# 4) service level model - determines the expected level(s) of the service provided by\n",
    "# the application as a whole, e.g. in terms of the response times or distirbutions thereof,\n",
    "# in terms of services availability.\n",
    "# 5) platform model (hardware/ virtual machines) - models the most relevant characteristics\n",
    "# of the platform in terms of performance, e.g. the number of hw threads/cores that might\n",
    "# be needed to known to accomodate the demands of the logical service instance in terms\n",
    "# of threads. Note: we are not solving the placement problem! This is something done\n",
    "# by the cloud services provider.\n",
    "# 6) cost model - models the cost of the platform resource used during the simulation.\n",
    "# 7) failure/availability model - determines the failure mode of the platform/app, s.t.\n",
    "# the scaling procedure should be able to compensate for the unpredictably failing nodes.\n",
    "# 8) performance interference/tenancy model - determines, how much CPU stealing can happen\n",
    "# on the platform shared between the simulated application and some other application\n",
    "# co-deployed in the cloud on the same infra.\n",
    "# 9) scaling policy - provides a scaling plan that is executed by the simulation,\n",
    "# follows a particular instance or the combination thereof of scaling policies.\n",
    "# 10) network model - determines link latencies and bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import random\n",
    "# Assumptions:\n",
    "# 1) Requests chains separation -> requests made by the user do not depend on other\n",
    "# requests by the user, although they can generate multiple other requests.\n",
    "# 2) Simulation step is smaller or equal to the processing duration at the smallest\n",
    "# component.\n",
    "\n",
    "# simulation at each clock cycle:\n",
    "# 1) generates requests and tries to put into the system\n",
    "# 2) iterates over the reqs in the system and updates their stats\n",
    "# 3) according to the scaling policy may update the dyn config of the app\n",
    "\n",
    "# reqs_types_ratios = {'auth': 0.8, 'buy': 0.2}\n",
    "class WorkloadModel:\n",
    "    def __init__(self,\n",
    "                 reqs_types_ratios,\n",
    "                 sampling_ms = 0):\n",
    "        \n",
    "        self.reqs_types_ratios = reqs_types_ratios\n",
    "        self.workload = {}\n",
    "        \n",
    "    def generate_requests(self, timestamp = 0):\n",
    "        gen_reqs = []\n",
    "        \n",
    "        num_reqs = random.randint(0, 10)\n",
    "        for req_type, ratio in self.reqs_types_ratios.items():\n",
    "            req_types_reqs_num = int(ratio * num_reqs)\n",
    "            for i in range(req_types_reqs_num):\n",
    "                req = Request(req_type)\n",
    "                gen_reqs.append(req)\n",
    "                \n",
    "        return gen_reqs\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "# application model incorps params of the requests since it depends on the\n",
    "# structure and the processing logic of the app, whereas the workload model\n",
    "# quantifies the amount and the distribution of the requests in time/volume\n",
    "# so, propagation chain of the request goes into the app model\n",
    "\n",
    "# The request can be fanned out if needed -> multiple request with the\n",
    "# same ID are created then.\n",
    "class Request:\n",
    "    def __init__(self,\n",
    "                 request_type,\n",
    "                 request_id = None):\n",
    "        # Static state\n",
    "        self.request_type = request_type\n",
    "        if request_id is None:\n",
    "            self.request_id = uuid.uuid1()\n",
    "        else:\n",
    "            self.request_id = request_id\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.processing_left_ms = 0\n",
    "        self.cumulative_time_ms = 0\n",
    "        self.done = False\n",
    "        self.upstream = True\n",
    "        \n",
    "    def set_done(self):\n",
    "        self.done = True\n",
    "        \n",
    "    def set_downstream(self):\n",
    "        self.upstream = False\n",
    "    \n",
    "# Example inputs:\n",
    "# request_type = \"auth\"\n",
    "# processing_times = {\"frontend\": [10, 0], \"appserver\": [50, 20], \"db\": [10,0], \"dba\": [20,0]}\n",
    "# in array of processing times: [<upstream>, <downstream>]\n",
    "# Static structure that defines how the requests are processed\n",
    "class RequestProcessingInfo:\n",
    "    def __init__(self,\n",
    "                 request_type,\n",
    "                 entry_service,\n",
    "                 processing_times):\n",
    "        \n",
    "        self.request_type = request_type\n",
    "        self.entry_service = entry_service\n",
    "        self.processing_times = processing_times\n",
    "\n",
    "class Queue:\n",
    "    # combines link and the queue\n",
    "    # link has a property of rejecting the msg\n",
    "    # TODO: think of implementing other disciplines, not just FIFO\n",
    "    def __init__(self,\n",
    "                 capacity_by_request_type):\n",
    "        \n",
    "        self.capacity_by_request_type = capacity_by_request_type\n",
    "        self.requests = []\n",
    "        \n",
    "    def enqueue(self, req):\n",
    "        self.requests.append(req)\n",
    "        \n",
    "    def pop(self):\n",
    "        return self.requests.pop()\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.requests)\n",
    "    \n",
    "    def add_cumulative_time(self, delta):\n",
    "        for req in self.requests:\n",
    "            req.cumulative_time_ms += delta\n",
    "        \n",
    "class Service:\n",
    "    def __init__(self,\n",
    "                 queue_capacity_by_request_type,\n",
    "                 threads_per_instance,\n",
    "                 simulation_step_ms,\n",
    "                 instances = 1):\n",
    "        \n",
    "        self.upstream_queue = Queue(queue_capacity_by_request_type)\n",
    "        self.downstream_queue = Queue(queue_capacity_by_request_type)\n",
    "        self.replies = [] # Replies that we got from the upstream services\n",
    "        \n",
    "        self.threads_per_instance = threads_per_instance\n",
    "        # TODO: should replace with the real platform info below!\n",
    "        self.hw_threads_per_instance = 4\n",
    "        self.instances = instances\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        \n",
    "        # requests that are currently in simultaneous processing\n",
    "        self.in_processing_simultaneous = []\n",
    "        # requests that are processed in this step, they can proceed\n",
    "        self.out = []\n",
    "        \n",
    "    def add_request(self, req):\n",
    "        # decide where to put the request\n",
    "        if req.upstream:\n",
    "            self.upstream_queue.enqueue(req)\n",
    "        else:\n",
    "            self.downstream_queue.enqueue(req)\n",
    "        \n",
    "    def step(self):\n",
    "        processing_time_left_at_step = self.simulation_step_ms\n",
    "        \n",
    "        while(processing_time_left_at_step > 0):\n",
    "            if (self.downstream_queue.size() == 0) and (self.upstream_queue.size() == 0):\n",
    "                processing_time_left_at_step = 0\n",
    "                continue\n",
    "                \n",
    "            if len(self.in_processing_simultaneous) > 0:\n",
    "                # Find minimal leftover duration, subtract it,\n",
    "                # and propagate the request\n",
    "                min_leftover_time = min([req.processing_left_ms for req in self.in_processing_simultaneous])\n",
    "                min_time_to_subtract = min(min_leftover_time, processing_time_left_at_step)\n",
    "                new_in_processing_simultaneous = []\n",
    "                \n",
    "                for req in self.in_processing_simultaneous:\n",
    "                    new_time_left = req.processing_left_ms - min_time_to_subtract\n",
    "                    if new_time_left > 0:\n",
    "                        req.processing_left_ms = new_time_left\n",
    "                        new_in_processing_simultaneous.append(req)\n",
    "                    else:\n",
    "                        # Request is put into the out buffer to be\n",
    "                        # processed further according to the app structure\n",
    "                        req.processing_left_ms = 0\n",
    "                        req.cumulative_time_ms += min_time_to_subtract  \n",
    "                        self.out.append(req)\n",
    "                \n",
    "                processing_time_left_at_step -= min_time_to_subtract\n",
    "                self.in_processing_simultaneous = new_in_processing_simultaneous\n",
    "                \n",
    "            spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            # Assumption: first we try to process the downstream reqs to\n",
    "            # provide the response faster, but overall it is application-dependent\n",
    "            while ((self.downstream_queue.size() > 0) or (self.upstream_queue.size() > 0)) and (spare_capacity > 0):\n",
    "                # TODO: consider fan-in\n",
    "                if self.downstream_queue.size() > 0:\n",
    "                    req = self.downstream_queue.pop()\n",
    "                    self.in_processing_simultaneous.append(req)\n",
    "                    spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            \n",
    "                if self.upstream_queue.size() > 0:\n",
    "                    req = self.upstream_queue.pop()\n",
    "                    self.in_processing_simultaneous.append(req)\n",
    "                    spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            \n",
    "        # Increase the cumulative time for all the reqs left in the queues waiting\n",
    "        self.upstream_queue.add_cumulative_time(self.simulation_step_ms)\n",
    "        self.downstream_queue.add_cumulative_time(self.simulation_step_ms)\n",
    "\n",
    "\n",
    "        \n",
    "# Services: {'frontend': service1, 'appserver': service2, 'db': service3}\n",
    "# Structure: {'frontend': ['appserver'], 'appserver': ['db'], 'db': None}\n",
    "# Starting points: ['frontend']\n",
    "# reqs_processing_infos: {'auth': req_proc_info1} -> also store the reverse path!\n",
    "class ApplicationModel:\n",
    "    def __init__(self,\n",
    "                 services,\n",
    "                 structure,\n",
    "                 reqs_processing_infos,\n",
    "                 filename = None):\n",
    "        # TODO: alternative from file\n",
    "        self.services = services\n",
    "        self.structure = structure\n",
    "        self.reqs_processing_infos = reqs_processing_infos\n",
    "        self.new_requests = []\n",
    "        self.response_times_by_request = {}\n",
    "        \n",
    "    def step(self):\n",
    "        if len(self.new_requests) > 0:\n",
    "            for req in self.new_requests:\n",
    "                entry_service = self.reqs_processing_infos[req.request_type].entry_service\n",
    "                req.processing_left_ms = self.reqs_processing_infos[req.request_type].processing_times[entry_service][1]\n",
    "                services[entry_service].add_request(req)\n",
    "                \n",
    "            self.new_requests = []\n",
    "        \n",
    "        # Proceed through the services // fan-in merge and fan-out copying\n",
    "        # is done in the app logic since it knows the structure and times\n",
    "        # IMPORTANT: the simulation step should be small for the following\n",
    "        # processing to work correctly ~5-10 ms.\n",
    "        for service_name, service in services.items():\n",
    "            service.step()\n",
    "            \n",
    "            while len(service.out) > 0:\n",
    "                req = service.out.pop()\n",
    "                req_info = self.reqs_processing_infos[req.request_type]\n",
    "                \n",
    "                if req.upstream:\n",
    "                    next_services_names = self.structure[service_name]['next']\n",
    "                    if not next_services_names is None:\n",
    "                        for next_service_name in next_services_names:\n",
    "                            if next_service_name in req_info.processing_times:\n",
    "                                req_cpy = req\n",
    "                                req_cpy.processing_left_ms = req_info.processing_times[next_service_name][0]\n",
    "                                self.services[next_service_name].add_request(req_cpy)\n",
    "                    else:\n",
    "                        # Sending response\n",
    "                        req.upstream = False\n",
    "                        \n",
    "                if not req.upstream:\n",
    "                    prev_services_names = self.structure[service_name]['prev']\n",
    "                    if not prev_services_names is None:\n",
    "                        for prev_service_name in prev_services_names:\n",
    "                            if prev_service_name in req_info.processing_times:\n",
    "                                req_cpy = req\n",
    "                                req_cpy.processing_left_ms = req_info.processing_times[prev_service_name][1]\n",
    "                                self.services[prev_service_name].add_request(req_cpy)\n",
    "                    else:\n",
    "                        # Response received by the user\n",
    "                        if req.request_type in self.response_times_by_request:\n",
    "                            self.response_times_by_request[req.request_type].append(req.cumulative_time_ms)\n",
    "                        else:\n",
    "                            self.response_times_by_request[req.request_type] = [req.cumulative_time_ms]\n",
    "            \n",
    "        #for req_type, req_processing_info in self.reqs_processing_infos.items():\n",
    "        #    cur_service_lbl = req_processing_info.entry_service\n",
    "        #    prev_service_lbl = None\n",
    "        #   \n",
    "        #    while not cur_service_lbl is None:\n",
    "        #        cur_service = self.services[cur_service_lbl]\n",
    "        #        cur_service.step()\n",
    "        #        while len(cur_service.out[req_type]) > 0:\n",
    "        #            req = cur_service.out[req_type].pop()\n",
    "        #            if req.upstream:\n",
    "        #                \n",
    "        #            next_service_lbl\n",
    "        #           # determine link and put there\n",
    "                \n",
    "        # processing_times = {\"frontend\": [10, 0], \"appserver\": [50, 20], \"db\": [10,0], \"dba\": [20,0]}\n",
    "        # Services: {'frontend': service1, 'appserver': service2, 'db': service3}\n",
    "        # Structure: {'frontend': {'next': ['appserver'], 'prev': None},\n",
    "        #             'appserver': {'next': ['db'], 'prev': ['frontend']},\n",
    "        #             'db': {'next': None, 'prev': ['appserver']}}\n",
    "    def enter_requests(self, new_requests):\n",
    "        self.new_requests = new_requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    def __init__(self,\n",
    "                 workload_model,\n",
    "                 application_model,\n",
    "                 time_to_simulate_days = 0.01,\n",
    "                 simulation_step_ms = 10):\n",
    "        \n",
    "        # add starting time\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        self.time_to_simulate_ms = int(time_to_simulate_days * 24 * 60 * 60 * 1000)\n",
    "        self.cur_simulation_time_ms = 0\n",
    "        self.workload_model = workload_model\n",
    "        self.application_model = application_model\n",
    "        self.sim_round = 0\n",
    "        \n",
    "    def _step(self):\n",
    "        # 1. Generate requests from the workload model, add them to the app model\n",
    "        new_requests = self.workload_model.generate_requests()\n",
    "        self.application_model.enter_requests(new_requests)\n",
    "        self.application_model.step()\n",
    "        \n",
    "        # 3. Collect metrics (proc. times etc.)\n",
    "        \n",
    "    def start(self):\n",
    "        while(self.cur_simulation_time_ms <= self.time_to_simulate_ms):\n",
    "            self.cur_simulation_time_ms += self.simulation_step_ms\n",
    "            self._step()\n",
    "            self.sim_round += 1\n",
    "            if self.sim_round % 1000 == 0:\n",
    "                left_to_simulate_ms = self.time_to_simulate_ms - self.cur_simulation_time_ms\n",
    "                print('Time left: {} min'.format(int(left_to_simulate_ms / (1000 * 60))))\n",
    "            # TODO: print leftover steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time left: 14 min\n",
      "Time left: 14 min\n",
      "Time left: 13 min\n",
      "Time left: 13 min\n",
      "Time left: 13 min\n",
      "Time left: 13 min\n",
      "Time left: 13 min\n",
      "Time left: 13 min\n",
      "Time left: 12 min\n",
      "Time left: 12 min\n",
      "Time left: 12 min\n",
      "Time left: 12 min\n",
      "Time left: 12 min\n",
      "Time left: 12 min\n",
      "Time left: 11 min\n",
      "Time left: 11 min\n",
      "Time left: 11 min\n",
      "Time left: 11 min\n",
      "Time left: 11 min\n",
      "Time left: 11 min\n",
      "Time left: 10 min\n",
      "Time left: 10 min\n",
      "Time left: 10 min\n",
      "Time left: 10 min\n",
      "Time left: 10 min\n",
      "Time left: 10 min\n",
      "Time left: 9 min\n",
      "Time left: 9 min\n",
      "Time left: 9 min\n",
      "Time left: 9 min\n",
      "Time left: 9 min\n",
      "Time left: 9 min\n",
      "Time left: 8 min\n",
      "Time left: 8 min\n",
      "Time left: 8 min\n",
      "Time left: 8 min\n",
      "Time left: 8 min\n",
      "Time left: 8 min\n",
      "Time left: 7 min\n",
      "Time left: 7 min\n",
      "Time left: 7 min\n",
      "Time left: 7 min\n",
      "Time left: 7 min\n",
      "Time left: 7 min\n",
      "Time left: 6 min\n",
      "Time left: 6 min\n",
      "Time left: 6 min\n",
      "Time left: 6 min\n",
      "Time left: 6 min\n",
      "Time left: 6 min\n",
      "Time left: 5 min\n",
      "Time left: 5 min\n",
      "Time left: 5 min\n",
      "Time left: 5 min\n",
      "Time left: 5 min\n",
      "Time left: 5 min\n",
      "Time left: 4 min\n",
      "Time left: 4 min\n",
      "Time left: 4 min\n",
      "Time left: 4 min\n",
      "Time left: 4 min\n",
      "Time left: 4 min\n",
      "Time left: 3 min\n",
      "Time left: 3 min\n",
      "Time left: 3 min\n",
      "Time left: 3 min\n",
      "Time left: 3 min\n",
      "Time left: 3 min\n",
      "Time left: 2 min\n",
      "Time left: 2 min\n",
      "Time left: 2 min\n",
      "Time left: 2 min\n",
      "Time left: 2 min\n",
      "Time left: 2 min\n",
      "Time left: 1 min\n",
      "Time left: 1 min\n",
      "Time left: 1 min\n",
      "Time left: 1 min\n",
      "Time left: 1 min\n",
      "Time left: 1 min\n",
      "Time left: 0 min\n",
      "Time left: 0 min\n",
      "Time left: 0 min\n",
      "Time left: 0 min\n",
      "Time left: 0 min\n",
      "Time left: 0 min\n"
     ]
    }
   ],
   "source": [
    "wlm_test = WorkloadModel({'auth': 0.8, 'buy': 0.2})\n",
    "\n",
    "frontend = Service(queue_capacity_by_request_type = {'auth': 20, 'buy': 10},\n",
    "                   threads_per_instance = 2,\n",
    "                   simulation_step_ms = 10,\n",
    "                   instances = 1)\n",
    "\n",
    "appserver = Service(queue_capacity_by_request_type = {'auth': 20, 'buy': 10},\n",
    "                   threads_per_instance = 4,\n",
    "                   simulation_step_ms = 10,\n",
    "                   instances = 1)\n",
    "\n",
    "db = Service(queue_capacity_by_request_type = {'auth': 20, 'buy': 10},\n",
    "             threads_per_instance = 2,\n",
    "             simulation_step_ms = 10,\n",
    "             instances = 1)\n",
    "\n",
    "services = {'frontend': frontend, 'appserver': appserver, 'db': db}\n",
    "structure = {'frontend': {'next': ['appserver'], 'prev': None},\n",
    "             'appserver': {'next': ['db'], 'prev': ['frontend']},\n",
    "             'db': {'next': None, 'prev': ['appserver']}}\n",
    "\n",
    "auth_processing_times = {\"frontend\": [10, 0], \"appserver\": [50, 20], \"db\": [10,0]}\n",
    "buy_processing_times = {\"frontend\": [10, 0], \"appserver\": [100, 30], \"db\": [20,0]}\n",
    "auth_rpi = RequestProcessingInfo('auth', 'frontend', auth_processing_times)\n",
    "buy_rpi = RequestProcessingInfo('buy', 'frontend', buy_processing_times)\n",
    "reqs_processing_infos = {'auth': auth_rpi, 'buy': buy_rpi}\n",
    "\n",
    "\n",
    "appm_test = ApplicationModel(services, structure, reqs_processing_infos)\n",
    "\n",
    "sim = Simulation(wlm_test, appm_test)\n",
    "sim.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([311.,   0.,   0.,   0.,   0., 302.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  15.,   0.,   0.,   0.,  18.]),\n",
       " array([ 70.,  72.,  74.,  76.,  78.,  80.,  82.,  84.,  86.,  88.,  90.,\n",
       "         92.,  94.,  96.,  98., 100., 102., 104., 106., 108., 110., 112.,\n",
       "        114., 116., 118., 120., 122., 124., 126., 128., 130.]),\n",
       " <a list of 30 Patch objects>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n_bins = 30\n",
    "\n",
    "# Generate a normal distribution, center at x=0 and y=5\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs[0].hist(sim.application_model.response_times_by_request['auth'], bins=n_bins)\n",
    "axs[1].hist(sim.application_model.response_times_by_request['buy'], bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.median(sim.application_model.response_times_by_request['auth'])\n",
    "# TODO: probably a bug - response time of some requests (auth) is smaller than the critical path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        \n",
    "rt = [Test(), Test(), Test()]\n",
    "for t in rt:\n",
    "    t.num += 4\n",
    "\n",
    "for t in rt:\n",
    "    print(t.num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 8]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt = [1, 2, 3]\n",
    "rt.append(8)\n",
    "rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider scaling links?\n",
    "class Link:\n",
    "    def __init__(self,\n",
    "                 simulation_step_ms,\n",
    "                 latency,\n",
    "                 bandwidth = 1000):\n",
    "        # Static state\n",
    "        self.latency = latency\n",
    "        self.bandwidth = bandwidth\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.requests = [] \n",
    "        \n",
    "    def put_request(self, req):\n",
    "        if len(self.requests) < self.bandwidth:\n",
    "            req.processing_left_ms = self.latency\n",
    "            self.requests.append(req)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_requests(self):\n",
    "        # Called every simulation step thus updating reqs\n",
    "        reqs_to_give = []\n",
    "        for req in self.requests:\n",
    "            min_time_left = min(req.processing_left_ms, self.simulation_step_ms)\n",
    "            if req.processing_left_ms - min_time_left <= 0:\n",
    "                req.processing_left_ms = 0\n",
    "                req.cumulative_time_ms += min_time_left\n",
    "                reqs_to_give.append(req)\n",
    "            else:\n",
    "                req.processing_left_ms -= min_time_left\n",
    "                \n",
    "        return reqs_to_give\n",
    "\n",
    "class NetworkModel:\n",
    "    def __init__(self,\n",
    "                 links_dict_in = None):\n",
    "        self.links_dict_in = links_dict_in\n",
    "        self.links_dict_out = {}\n",
    "        \n",
    "        # TODO: from file\n",
    "        for link_start, outs in self.links_dict.items():\n",
    "            for out, link in outs.items():\n",
    "                if out in self.links_dict_out:\n",
    "                    self.links_dict_out[out].append(link)\n",
    "                else:\n",
    "                    self.links_dict_out[out] = [link]\n",
    "                    \n",
    "    def put_request(self,\n",
    "                    start_service_lbl,\n",
    "                    end_service_lbl,\n",
    "                    req):\n",
    "        \n",
    "        self.links_dict_in[start_service_lbl][end_service_lbl].put_request(req)\n",
    "    \n",
    "    def get_requests(self,\n",
    "                     end_service_lbl):\n",
    "        reqs = []\n",
    "        links = self.links_dict_out[end_service_lbl]\n",
    "        for link in links:\n",
    "            reqs.extend(link.get_requests())\n",
    "            \n",
    "        return reqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
