{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The <Noname> simulator's goal is to provide the fast and cheap testing ground\n",
    "# for the autoscaling policies/mechanism used in interactive transaction-based\n",
    "# applications deployed in the cloud using the containers.\n",
    "# The following models constitute the simulator:\n",
    "# 1) application model - determines the logical structure of the application, i.e.\n",
    "# which services communicate, how much time is required to process particular requests,\n",
    "# how large are the buffers for storing the queries awaiting the processing. Overall,\n",
    "# the application is represented as a general networked queueing model.\n",
    "# The components of the model:\n",
    "# - static -> the connections forming the logic of the application + processing times\n",
    "# - dynamic -> the amount of instances of the application services (containers)\n",
    "# 2) workload/load model - determines the load generated by the users of the application,\n",
    "# i.e. requests times, distribution of the requests in time (e.g. diurnal, seasonal),\n",
    "# the composition of the workload mixture (i.e. distribution of requests in terms of\n",
    "# required processing times, e.g. 80% small reqs, 20% large ones)\n",
    "# 3) scaling model - determines the scaling behaviour of the application, i.e. how\n",
    "# much time might be required to take/conduct the scaling decision.\n",
    "# 4) service level model - determines the expected level(s) of the service provided by\n",
    "# the application as a whole, e.g. in terms of the response times or distirbutions thereof,\n",
    "# in terms of services availability.\n",
    "# 5) platform model (hardware/ virtual machines) - models the most relevant characteristics\n",
    "# of the platform in terms of performance, e.g. the number of hw threads/cores that might\n",
    "# be needed to known to accomodate the demands of the logical service instance in terms\n",
    "# of threads. Note: we are not solving the placement problem! This is something done\n",
    "# by the cloud services provider.\n",
    "# 6) cost model - models the cost of the platform resource used during the simulation.\n",
    "# 7) failure/availability model - determines the failure mode of the platform/app, s.t.\n",
    "# the scaling procedure should be able to compensate for the unpredictably failing nodes.\n",
    "# 8) performance interference/tenancy model - determines, how much CPU stealing can happen\n",
    "# on the platform shared between the simulated application and some other application\n",
    "# co-deployed in the cloud on the same infra.\n",
    "# 9) scaling policy - provides a scaling plan that is executed by the simulation,\n",
    "# follows a particular instance or the combination thereof of scaling policies.\n",
    "# 10) network model - determines link latencies and bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulator's assumptions:\n",
    "# 1) Requests chains separation -> requests made by the user do not depend on other\n",
    "# requests by the user, although they can generate multiple other requests.\n",
    "# 2) Simulation step is smaller or equal to the processing duration at the smallest\n",
    "# component.\n",
    "# 3) No retries are performed if requests fails due to some issue, e.g. overfull buffers,\n",
    "# not enough throughput on the links, timeout reached\n",
    "# 4) All the nodes in the same geographical region. This assumption is valid since\n",
    "# the autoscalers often act independently on each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicedRequestsNumDistribution(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for generating the random number of requests\n",
    "    based on the corresponding distribution registered with it.\n",
    "    The class registered with it should define own generate method.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def generate(self):\n",
    "        pass\n",
    "    \n",
    "class NormalDistribution:\n",
    "    \"\"\"\n",
    "    Generates the random number of requests in the time slice\n",
    "    according to the normal distribution. Wraps the corresponding\n",
    "    call to the np.random.normal.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def generate(self, num = 1):\n",
    "        return np.random.normal(self.mu, self.sigma, num)\n",
    "\n",
    "# Registering the derived sliced request number generators\n",
    "SlicedRequestsNumDistribution.register(NormalDistribution)\n",
    "\n",
    "class WorkloadModel:\n",
    "    \"\"\"\n",
    "    Represents the workload generation model.\n",
    "    The parameters are taken from the corresponding JSON file passed to the ctor.\n",
    "    \n",
    "    Properties:\n",
    "        reqs_types_ratios (dict): ratio of requests (value) of the given request type (key) in the mixture\n",
    "        reqs_generators (dict): random sliced requests num generator (value) for the given request type (key)\n",
    "        \n",
    "        workload (dict): array of the numbers of requests generated for the timestamp (value) for the given\n",
    "                         request type\n",
    "                     \n",
    "    Methods:\n",
    "        generate_requests (timestamp): generates a mixture of requests (list) using the reqs_types_ratios and\n",
    "                                       reqs_generators with the provided timestamp.\n",
    "    \n",
    "    Usage:\n",
    "        wkldmdl = WorkloadModel(filename = 'experiments/test/workload.json')\n",
    "        len(wkldmdl.generate_requests(100))\n",
    "        \n",
    "    TODO:\n",
    "        implement support for the seasonal reqs num generation component\n",
    "        implement support for holidays etc.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 reqs_types_ratios = None,\n",
    "                 filename = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.reqs_types_ratios = {}\n",
    "        self.reqs_generators = {}\n",
    "        \n",
    "        if filename is None:\n",
    "            raise ValueError('Configuration file not provided for the WorkloadModel.')\n",
    "        else:\n",
    "            with open(filename) as f:\n",
    "                config = json.load(f)\n",
    "                for conf in config[\"workloads_configs\"]:\n",
    "                    req_type = conf[\"request_type\"]\n",
    "                    req_ratio = conf[\"workload_config\"][\"ratio\"]\n",
    "                    if req_ratio < 0.0 or req_ratio > 1.0:\n",
    "                        raise ValueError('Unacceptable ratio value for the request of type {}.'.format(req_type))\n",
    "                    self.reqs_types_ratios[req_type] = req_ratio\n",
    "                     \n",
    "                    req_distribution_type = conf[\"workload_config\"][\"sliced_distribution\"][\"type\"]\n",
    "                    req_distribution_params = conf[\"workload_config\"][\"sliced_distribution\"][\"params\"]\n",
    "                    \n",
    "                    if req_distribution_type == \"normal\":\n",
    "                        mu = 0.0\n",
    "                        sigma = 0.1\n",
    "                        \n",
    "                        if len(req_distribution_params) > 0:\n",
    "                            mu = req_distribution_params[0]\n",
    "                        if len(req_distribution_params) > 1:\n",
    "                            sigma = req_distribution_params[1]\n",
    "                        \n",
    "                        self.reqs_generators[req_type] = NormalDistribution(mu, sigma)\n",
    "                    \n",
    "                    # TODO: processing of the \"seasonal_pattern_p_h\"\n",
    "                    \n",
    "        # Dynamic state  \n",
    "        self.workload = {}    \n",
    "        \n",
    "    def generate_requests(self,\n",
    "                          timestamp):\n",
    "        gen_reqs = []\n",
    "        \n",
    "        for req_type, ratio in self.reqs_types_ratios.items():\n",
    "            num_reqs = self.reqs_generators[req_type].generate()\n",
    "            req_types_reqs_num = int(ratio * num_reqs)\n",
    "            for i in range(req_types_reqs_num):\n",
    "                req = Request(req_type)\n",
    "                gen_reqs.append(req)\n",
    "                \n",
    "            if req_type in self.workload:\n",
    "                self.workload[req_type].append((timestamp, req_types_reqs_num))\n",
    "            else:\n",
    "                self.workload[req_type] = [(timestamp, req_types_reqs_num)]\n",
    "                \n",
    "        return gen_reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    def __init__(self,\n",
    "                 request_type,\n",
    "                 request_id = None):\n",
    "        # Static state\n",
    "        self.request_type = request_type\n",
    "        if request_id is None:\n",
    "            self.request_id = uuid.uuid1()\n",
    "        else:\n",
    "            self.request_id = request_id\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.processing_left_ms = 0\n",
    "        self.cumulative_time_ms = 0\n",
    "        self.upstream = True\n",
    "        self.replies_expected = 1 # to implement the fan-in on the level of service\n",
    "        \n",
    "    def set_downstream(self):\n",
    "        self.upstream = False\n",
    "    \n",
    "class RequestProcessingInfo:\n",
    "    def __init__(self,\n",
    "                 request_type,\n",
    "                 entry_service,\n",
    "                 processing_times,\n",
    "                 timeout_ms,\n",
    "                 request_size_b,\n",
    "                 response_size_b,\n",
    "                 request_operation_type):\n",
    "        \n",
    "        self.request_type = request_type\n",
    "        self.entry_service = entry_service\n",
    "        self.processing_times = processing_times\n",
    "        self.timeout_ms = timeout_ms\n",
    "        self.request_size_b = request_size_b\n",
    "        self.response_size_b = response_size_b\n",
    "        self.request_operation_type = request_operation_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkBuffer:\n",
    "    \"\"\"\n",
    "    Combines link and the buffer. The requests in the link advance when the step method is called.\n",
    "    If the processing time left (i.e. waiting on the link) is over, the request proceeds to the\n",
    "    buffer, where it can be extracted from for the further processing in a service.\n",
    "    If the used throughput reached the throughput limit, the request is lost. The same happens\n",
    "    if upon the transition to the buffer, the buffer is full for the given request type.\n",
    "    \n",
    "    Properties:\n",
    "    \n",
    "        As buffer:\n",
    "            capacity_by_request_type (dict) - holds capacity of the buffer by request type, in terms of\n",
    "                                              requests currently placed in the buffer\n",
    "            [STUB] policy                   - policy used for moving requests in the buffer, e.g. FIFO/LIFO\n",
    "            \n",
    "            requests (collections.deque)    - holds current requests in the buffer\n",
    "            reqs_cnt (dict)                 - holds current request count by the request type, used to rapidly check\n",
    "                                              if more requests of the given type can be accomodated in the buffer\n",
    "            \n",
    "        As link:\n",
    "            latency_ms (int)                - latency of the link in milliseconds, taken from the config\n",
    "            throughput_mbps (int)           - throughput of the link in Megabytes per sec, taken from the config\n",
    "            request_processing_infos (dict) - holds requests processing information to compute the used\n",
    "                                              throughput etc.\n",
    "            \n",
    "            requests_in_transfer (list)     - holds requests that are currently \"transferred\" by this link\n",
    "            used_throughput_mbps (int)      - throughput currently used on this link by the \"transferred\" reqs\n",
    "        \n",
    "    Methods:\n",
    "    \n",
    "        As buffer:\n",
    "            append_left (req)               - puts the request req at the beginning of the buffer to give other\n",
    "                                              requests opportunity to be processed if the current request waits\n",
    "                                              for other replies to get processed (fan-in)\n",
    "            pop                             - takes the last added request out of the buffer for processing (LIFO)\n",
    "            pop_left                        - takes the first added request out of the buffer for processing (FIFO)\n",
    "            size                            - returns size of the buffer\n",
    "            add_cumulative_time (delta)     - adds time delta to every request in the buffer\n",
    "            remove_by_id (request_id)       - removes all the requests with request id request_id from the buffer\n",
    "            \n",
    "        As link:\n",
    "            put (req)                       - puts a new request req on a link, i.e. in requests_in_transfer,\n",
    "                                              if there is enough spare throughput; otherwise drops the request\n",
    "            step (simulation_step_ms)       - makes a discrete simulation time step of the length simulation_step_ms\n",
    "                                              to advance the requests held on the link, i.e. in requests_in_transfer,\n",
    "                                              and to put them into the buffer if possible (capacity left). In case of\n",
    "                                              no spare capacity in the buffer, the request is also dropped.\n",
    "        \n",
    "            _req_occupied_mbps (req)        - private method that computes the throughput used by the request req\n",
    "    \n",
    "    TODO:\n",
    "        implementing scaling of the links? e.g. according to the added instances of services.\n",
    "        implement wrapping of the lower-level details into policies like LIFO, FIFO, etc. hide append, pop etc.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 capacity_by_request_type,\n",
    "                 request_processing_infos,\n",
    "                 latency_ms = 10, # TODO: remove val\n",
    "                 throughput_mbps = 1000, # TODO: remove val\n",
    "                 policy = \"FIFO\"): \n",
    "        \n",
    "        # Static state\n",
    "        # Buffer:\n",
    "        self.capacity_by_request_type = capacity_by_request_type\n",
    "        self.policy = policy\n",
    "        \n",
    "        # Link:\n",
    "        self.latency_ms = latency_ms\n",
    "        self.throughput_mbps = throughput_mbps\n",
    "        self.request_processing_infos = request_processing_infos\n",
    "        \n",
    "        # Dynamic state\n",
    "        # Buffer:\n",
    "        self.requests = deque([])\n",
    "        self.reqs_cnt = {}\n",
    "        for request_type in capacity_by_request_type.keys():\n",
    "            self.reqs_cnt[request_type] = 0\n",
    "            \n",
    "        # Link:\n",
    "        self.requests_in_transfer = []\n",
    "        self.used_throughput_mbps = 0\n",
    "    \n",
    "    def step(self, simulation_step_ms):\n",
    "        \"\"\" Processing requests to bring them from the link into the buffer \"\"\"\n",
    "        for req in self.requests_in_transfer:\n",
    "            min_time_to_subtract_ms = min(req.processing_left_ms, simulation_step_ms)\n",
    "            req.processing_left_ms -= min_time_to_subtract_ms\n",
    "            if req.processing_left_ms <= 0:\n",
    "                capacity = self.capacity_by_request_type[req.request_type]\n",
    "                self.used_throughput_mbps -= self._req_occupied_mbps(req)\n",
    "                \n",
    "                if self.reqs_cnt[req.request_type] == capacity:\n",
    "                    del req # dropping the request if no spare capacity\n",
    "                else:\n",
    "                    req.cumulative_time_ms += min_time_to_subtract_ms\n",
    "                    if req.cumulative_time_ms >= self.request_processing_infos[req.request_type].timeout_ms:\n",
    "                        del req\n",
    "                    else:\n",
    "                        self.requests.append(req)\n",
    "                        self.reqs_cnt[req.request_type] += 1 \n",
    "    \n",
    "    def put(self, req):\n",
    "        req_size_b_mbps = self._req_occupied_mbps(req)\n",
    "\n",
    "        if self.throughput_mbps - self.used_throughput_mbps >= req_size_b_mbps:\n",
    "            self.used_throughput_mbps += req_size_b_mbps\n",
    "            req.processing_left_ms = self.latency_ms\n",
    "            self.requests_in_transfer.append(req)\n",
    "        else:\n",
    "            del req\n",
    "    \n",
    "    def append_left(self, req):\n",
    "        self.requests.appendLeft(req)\n",
    "    \n",
    "    def pop(self):\n",
    "        req = None\n",
    "        if len(self.requests) > 0:\n",
    "            req = self.requests.pop()\n",
    "            self.reqs_cnt[req.request_type] -= 1\n",
    "            \n",
    "        return req\n",
    "    \n",
    "    def pop_left(self):\n",
    "        req = None\n",
    "        if len(self.requests) > 0:\n",
    "            req = self.requests.popLeft()\n",
    "            self.reqs_cnt[req.request_type] -= 1\n",
    "        \n",
    "        return req\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.requests)\n",
    "    \n",
    "    def add_cumulative_time(self, delta):\n",
    "        for req in self.requests:\n",
    "            req.cumulative_time_ms += delta\n",
    "    \n",
    "    def remove_by_id(self, request_id):\n",
    "        for req in reversed(self.requests):\n",
    "            if req.request_id == request_id:\n",
    "                self.requests.remove(req)\n",
    "                \n",
    "    def _req_occupied_mbps(self, req):\n",
    "        req_size_b = 0\n",
    "        if req.upstream:\n",
    "            req_size_b = self.request_processing_infos[req.request_type].request_size_b\n",
    "        else:\n",
    "            req_size_b = self.request_processing_infos[req.request_type].response_size_b\n",
    "        req_size_b_mb = req_size_b / (1024 * 1024)\n",
    "        req_size_b_mbps = req_size_b_mb * (self.latency_ms / 1000) # taking channel for that long\n",
    "        return req_size_b_mbps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Service:\n",
    "    def __init__(self,\n",
    "                 buffer_capacity_by_request_type,\n",
    "                 threads_per_instance,\n",
    "                 request_processing_infos,\n",
    "                 instances = 1,\n",
    "                 state_mb = 0):\n",
    "        \n",
    "        # Static state\n",
    "        # TODO: should replace with the real platform info below!\n",
    "        self.hw_threads_per_instance = 4\n",
    "        self.threads_per_instance = threads_per_instance\n",
    "        # If state_mb is 0, then the service is stateless\n",
    "        self.state_mb = state_mb\n",
    "        \n",
    "        # Dynamic state\n",
    "        # Upstream and downstream links/buffers of the service            \n",
    "        self.upstream_buf = LinkBuffer(buffer_capacity_by_request_type, request_processing_infos)\n",
    "        self.downstream_buf = LinkBuffer(buffer_capacity_by_request_type, request_processing_infos)\n",
    "        # Scaling-related\n",
    "        self.instances = instances\n",
    "        # requests that are currently in simultaneous processing\n",
    "        self.in_processing_simultaneous = []\n",
    "        # requests that are processed in this step, they can proceed\n",
    "        self.out = []\n",
    "        \n",
    "    def add_request(self, req):\n",
    "        # decide where to put the request\n",
    "        if req.upstream:\n",
    "            self.upstream_buf.put(req)\n",
    "        else:\n",
    "            self.downstream_buf.put(req)\n",
    "        \n",
    "    def step(self, simulation_step_ms):\n",
    "        processing_time_left_at_step = simulation_step_ms\n",
    "        \n",
    "        # Propagating requests in the link\n",
    "        self.downstream_buf.step(simulation_step_ms)  \n",
    "        self.upstream_buf.step(simulation_step_ms)\n",
    "        \n",
    "        while(processing_time_left_at_step > 0):\n",
    "           \n",
    "            if (self.downstream_buf.size() == 0) and (self.upstream_buf.size() == 0):\n",
    "                processing_time_left_at_step = 0\n",
    "                continue\n",
    "                \n",
    "            if len(self.in_processing_simultaneous) > 0:\n",
    "                # Find minimal leftover duration, subtract it,\n",
    "                # and propagate the request\n",
    "                min_leftover_time = min([req.processing_left_ms for req in self.in_processing_simultaneous])\n",
    "                min_time_to_subtract = min(min_leftover_time, processing_time_left_at_step)\n",
    "                new_in_processing_simultaneous = []\n",
    "                \n",
    "                for req in self.in_processing_simultaneous:\n",
    "                    new_time_left = req.processing_left_ms - min_time_to_subtract\n",
    "                    req.cumulative_time_ms += min_time_to_subtract\n",
    "                    if new_time_left > 0:\n",
    "                        req.processing_left_ms = new_time_left\n",
    "                        new_in_processing_simultaneous.append(req)\n",
    "                    else:\n",
    "                        # Request is put into the out buffer to be\n",
    "                        # processed further according to the app structure\n",
    "                        req.processing_left_ms = 0  \n",
    "                        self.out.append(req)\n",
    "                \n",
    "                processing_time_left_at_step -= min_time_to_subtract\n",
    "                self.in_processing_simultaneous = new_in_processing_simultaneous\n",
    "                \n",
    "            spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            # Assumption: first we try to process the downstream reqs to\n",
    "            # provide the response faster, but overall it is application-dependent\n",
    "            while ((self.downstream_buf.size() > 0) or (self.upstream_buf.size() > 0)) and (spare_capacity > 0):\n",
    "                if self.downstream_buf.size() > 0:\n",
    "                    req = self.downstream_buf.requests[-1]\n",
    "                    # Processing fan-in case\n",
    "                    if req.replies_expected > 1:\n",
    "                        req_id_ref = req.request_id\n",
    "                        reqs_present = 1\n",
    "                        for req_lookup in self.downstream_buf.requests[:-1]:\n",
    "                            if req_lookup.request_id == req_id_ref:\n",
    "                                reqs_present += 1\n",
    "                                \n",
    "                        if reqs_present == req.replies_expected:\n",
    "                            req = self.downstream_buf.pop()\n",
    "                            # Removing all the related requests\n",
    "                            self.downstream_buf.remove_by_id(req_id_ref)\n",
    "                        else:\n",
    "                            # pushing to the beginning of the deque to enable\n",
    "                            # progress in processing the downstream reqs\n",
    "                            req = self.downstream_buf.pop()\n",
    "                            self.downstream_buf.append_left(req)\n",
    "                            req = None\n",
    "                            \n",
    "                    else: \n",
    "                        req = self.downstream_buf.pop()\n",
    "                    \n",
    "                    if not req is None:\n",
    "                        req.replies_expected = 1\n",
    "                        self.in_processing_simultaneous.append(req)\n",
    "                        spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            \n",
    "                if self.upstream_buf.size() > 0:\n",
    "                    req = self.upstream_buf.pop()\n",
    "                    self.in_processing_simultaneous.append(req)\n",
    "                    spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            \n",
    "        # Increase the cumulative time for all the reqs left in the buffers waiting\n",
    "        self.upstream_buf.add_cumulative_time(simulation_step_ms)\n",
    "        self.downstream_buf.add_cumulative_time(simulation_step_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application model incorps params of the requests since it depends on the\n",
    "# structure and the processing logic of the app, whereas the workload model\n",
    "# quantifies the amount and the distribution of the requests in time/volume\n",
    "# so, propagation chain of the request goes into the app model\n",
    "\n",
    "# Services: {'frontend': service1, 'appserver': service2, 'db': service3}\n",
    "# Structure: {'frontend': ['appserver'], 'appserver': ['db'], 'db': None}\n",
    "# Starting points: ['frontend']\n",
    "# reqs_processing_infos: {'auth': req_proc_info1} -> also store the reverse path!\n",
    "class ApplicationModel:\n",
    "    def __init__(self,\n",
    "                 filename = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.services = {}\n",
    "        self.structure = {}\n",
    "        self.reqs_processing_infos = {}\n",
    "        \n",
    "        if filename is None:\n",
    "            raise ValueError('Configuration file not provided for the ApplicationModel.')\n",
    "        else:\n",
    "            with open(filename) as f:\n",
    "                config = json.load(f)\n",
    "                \n",
    "                for request_info in config[\"requests\"]:\n",
    "                    request_type = request_info[\"request_type\"]\n",
    "                    entry_service = request_info[\"entry_service\"]\n",
    "                    \n",
    "                    processing_times = {}\n",
    "                    for processing_time_service_entry in request_info[\"processing_times_ms\"]:\n",
    "                        service_name = processing_time_service_entry[\"service\"]\n",
    "                        upstream_time = processing_time_service_entry[\"upstream\"]\n",
    "                        if upstream_time < 0:\n",
    "                            raise ValueError('The upstream time for the request {} when passing through the service {} is negative.'.format(request_type, service_name))\n",
    "                        downstream_time = processing_time_service_entry[\"downstream\"]\n",
    "                        if downstream_time < 0:\n",
    "                            raise ValueError('The downstream time for the request {} when passing through the service {} is negative.'.format(request_type, service_name))\n",
    "                        \n",
    "                        processing_times[service_name] = [upstream_time, downstream_time]\n",
    "                    \n",
    "                    timeout_ms = request_info[\"timeout_ms\"]\n",
    "                    if timeout_ms < 0:\n",
    "                        raise ValueError('The timeout value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    request_size_b = request_info[\"request_size_b\"]\n",
    "                    if request_size_b < 0:\n",
    "                        raise ValueError('The request size value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    response_size_b = request_info[\"response_size_b\"]\n",
    "                    if response_size_b < 0:\n",
    "                        raise ValueError('The response size value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    request_operation_type = request_info[\"operation_type\"]\n",
    "                    \n",
    "                    req_proc_info = RequestProcessingInfo(request_type,\n",
    "                                                          entry_service,\n",
    "                                                          processing_times,\n",
    "                                                          timeout_ms,\n",
    "                                                          request_size_b,\n",
    "                                                          response_size_b,\n",
    "                                                          request_operation_type)\n",
    "                    self.reqs_processing_infos[request_type] = req_proc_info\n",
    "                \n",
    "                for service_config in config[\"services\"]:\n",
    "                    # Creating & adding the service:\n",
    "                    service_name = service_config[\"name\"]\n",
    "                    \n",
    "                    buffer_capacity_by_request_type = {}\n",
    "                    for buffer_capacity_config in service_config[\"buffer_capacity_by_request_type\"]:\n",
    "                        request_type = buffer_capacity_config[\"request_type\"]\n",
    "                        capacity = buffer_capacity_config[\"capacity\"]\n",
    "                        if capacity <= 0:\n",
    "                            raise ValueError('Buffer capacity is not positive for request type {} of service {}.'.format(request_type, service_name))\n",
    "                        buffer_capacity_by_request_type[request_type] = capacity\n",
    "                        \n",
    "                    threads_per_instance = service_config[\"threads_per_instance\"]\n",
    "                    if threads_per_instance <= 0:\n",
    "                        raise ValueError('Threads per instance is not positive for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    starting_instances_num = service_config[\"starting_instances_num\"]\n",
    "                    if starting_instances_num <= 0:\n",
    "                        raise ValueError('Number of service instances to start with is not positive for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    state_mb = service_config[\"state_mb\"]\n",
    "                    if state_mb < 0:\n",
    "                        raise ValueError('The state size is negative for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    service = Service(buffer_capacity_by_request_type = buffer_capacity_by_request_type,\n",
    "                                      threads_per_instance = threads_per_instance,\n",
    "                                      request_processing_infos = self.reqs_processing_infos,\n",
    "                                      instances = starting_instances_num,\n",
    "                                      state_mb = state_mb)\n",
    "                    \n",
    "                    self.services[service_name] = service\n",
    "                    \n",
    "                    # Adding the links of the given service to the structure.\n",
    "                    # TODO: think of whether the broken simmetry of the links\n",
    "                    # is appropriate.\n",
    "                    next_services = service_config[\"next\"]\n",
    "                    prev_services = service_config[\"prev\"]\n",
    "                    if len(next_services) == 0:\n",
    "                        next_services = None\n",
    "                    if len(prev_services) == 0:\n",
    "                        prev_services = None\n",
    "                    self.structure[service_name] = {'next': next_services, 'prev': prev_services}\n",
    "                    \n",
    "                \n",
    "        \n",
    "        # Dynamic state\n",
    "        self.new_requests = []\n",
    "        self.response_times_by_request = {}\n",
    "        \n",
    "    def step(self, simulation_step_ms):\n",
    "        if len(self.new_requests) > 0:\n",
    "            for req in self.new_requests:\n",
    "                entry_service = self.reqs_processing_infos[req.request_type].entry_service\n",
    "                req.processing_left_ms = self.reqs_processing_infos[req.request_type].processing_times[entry_service][1]\n",
    "                self.services[entry_service].add_request(req)\n",
    "                \n",
    "            self.new_requests = []\n",
    "        \n",
    "        # Proceed through the services // fan-in merge and fan-out copying\n",
    "        # is done in the app logic since it knows the structure and times\n",
    "        # IMPORTANT: the simulation step should be small for the following\n",
    "        # processing to work correctly ~5-10 ms.\n",
    "        for service_name, service in self.services.items():\n",
    "            service.step(simulation_step_ms)\n",
    "            \n",
    "            while len(service.out) > 0:\n",
    "                req = service.out.pop()\n",
    "                req_info = self.reqs_processing_infos[req.request_type]\n",
    "                \n",
    "                if req.upstream:\n",
    "                    next_services_names = self.structure[service_name]['next']\n",
    "                    if not next_services_names is None:\n",
    "                        for next_service_name in next_services_names:\n",
    "                            if next_service_name in req_info.processing_times:\n",
    "                                req_cpy = req\n",
    "                                req_cpy.processing_left_ms = req_info.processing_times[next_service_name][0]\n",
    "                                self.services[next_service_name].add_request(req_cpy)\n",
    "                    else:\n",
    "                        # Sending response\n",
    "                        req.upstream = False\n",
    "                        \n",
    "                if not req.upstream:\n",
    "                    prev_services_names = self.structure[service_name]['prev']\n",
    "                    \n",
    "                    if not prev_services_names is None:\n",
    "                        replies_expected = 0\n",
    "                        for prev_service_name in prev_services_names:\n",
    "                            if prev_service_name in req_info.processing_times:\n",
    "                                replies_expected += 1\n",
    "                        \n",
    "                        for prev_service_name in prev_services_names:\n",
    "                            if prev_service_name in req_info.processing_times:\n",
    "                                req_cpy = req\n",
    "                                req_cpy.processing_left_ms = req_info.processing_times[prev_service_name][1]\n",
    "                                req_cpy.replies_expected = replies_expected\n",
    "                                self.services[prev_service_name].add_request(req_cpy)\n",
    "                    else:\n",
    "                        # Response received by the user\n",
    "                        if req.request_type in self.response_times_by_request:\n",
    "                            self.response_times_by_request[req.request_type].append(req.cumulative_time_ms)\n",
    "                        else:\n",
    "                            self.response_times_by_request[req.request_type] = [req.cumulative_time_ms]\n",
    "                        del req\n",
    "                        \n",
    "    def enter_requests(self, new_requests):\n",
    "        self.new_requests = new_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation at each clock cycle:\n",
    "# 1) generates requests and tries to put into the system\n",
    "# 2) iterates over the reqs in the system and updates their stats\n",
    "# 3) according to the scaling policy may update the dyn config of the app\n",
    "class Simulation:\n",
    "    def __init__(self,\n",
    "                 workload_model,\n",
    "                 application_model,\n",
    "                 time_to_simulate_days = 0.0005,\n",
    "                 simulation_step_ms = 10):\n",
    "        \n",
    "        # add starting time\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        self.time_to_simulate_ms = int(time_to_simulate_days * 24 * 60 * 60 * 1000)\n",
    "        self.cur_simulation_time_ms = 0\n",
    "        self.workload_model = workload_model\n",
    "        self.application_model = application_model\n",
    "        self.sim_round = 0\n",
    "        \n",
    "    def _step(self):\n",
    "        # 1. Generate requests from the workload model, add them to the app model\n",
    "        # TODO: timestamps logic\n",
    "        new_requests = self.workload_model.generate_requests(100)\n",
    "        self.application_model.enter_requests(new_requests)\n",
    "        self.application_model.step(self.simulation_step_ms)\n",
    "        \n",
    "        # 3. Collect metrics (proc. times etc.)\n",
    "        \n",
    "    def start(self):\n",
    "        while(self.cur_simulation_time_ms <= self.time_to_simulate_ms):\n",
    "            self.cur_simulation_time_ms += self.simulation_step_ms\n",
    "            self._step()\n",
    "            self.sim_round += 1\n",
    "            if self.sim_round % 1000 == 0:\n",
    "                left_to_simulate_ms = self.time_to_simulate_ms - self.cur_simulation_time_ms\n",
    "                left_to_simulate_steps = left_to_simulate_ms // self.simulation_step_ms\n",
    "                print('[{}] Left to simulate: {} min or {} steps'.format(datetime.now(),\n",
    "                                                                         int(left_to_simulate_ms / (1000 * 60)),\n",
    "                                                                         left_to_simulate_steps))\n",
    "                # TODO: dynamically updating screen / display progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-12 20:52:22.107721] Left to simulate: 0 min or 3319 steps\n",
      "[2020-09-12 20:55:33.672910] Left to simulate: 0 min or 2319 steps\n",
      "[2020-09-12 21:00:54.025662] Left to simulate: 0 min or 1319 steps\n",
      "[2020-09-12 21:08:24.286317] Left to simulate: 0 min or 319 steps\n"
     ]
    }
   ],
   "source": [
    "wlm_test = WorkloadModel(filename = 'experiments/test/workload.json')\n",
    "appm_test = ApplicationModel('experiments/test/application.json')\n",
    "sim = Simulation(wlm_test, appm_test)\n",
    "sim.start()\n",
    "# TODO: check how to make the duration of the step ~constant, now it increases - 3, 5, 8 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([158.,   0.,   0.,   0.,   0., 127.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   6.,   0.,   0.,   0.,   8.]),\n",
       " array([190., 192., 194., 196., 198., 200., 202., 204., 206., 208., 210.,\n",
       "        212., 214., 216., 218., 220., 222., 224., 226., 228., 230., 232.,\n",
       "        234., 236., 238., 240., 242., 244., 246., 248., 250.]),\n",
       " <a list of 30 Patch objects>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n_bins = 30\n",
    "\n",
    "# Generate a normal distribution, center at x=0 and y=5\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs[0].hist(sim.application_model.response_times_by_request['auth'], bins=n_bins)\n",
    "axs[1].hist(sim.application_model.response_times_by_request['buy'], bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.median(sim.application_model.response_times_by_request['buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        \n",
    "rt = [Test(), Test(), Test()]\n",
    "for t in rt:\n",
    "    t.num += 4\n",
    "\n",
    "for t in rt:\n",
    "    print(t.num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt = [1, 2, 3]\n",
    "rt.remove(1)\n",
    "rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider scaling links?\n",
    "class Link:\n",
    "    def __init__(self,\n",
    "                 simulation_step_ms,\n",
    "                 latency,\n",
    "                 bandwidth = 1000):\n",
    "        # Static state\n",
    "        self.latency = latency\n",
    "        self.bandwidth = bandwidth\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.requests = [] \n",
    "        \n",
    "    def put_request(self, req):\n",
    "        if len(self.requests) < self.bandwidth:\n",
    "            req.processing_left_ms = self.latency\n",
    "            self.requests.append(req)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_requests(self):\n",
    "        # Called every simulation step thus updating reqs\n",
    "        reqs_to_give = []\n",
    "        for req in self.requests:\n",
    "            min_time_left = min(req.processing_left_ms, self.simulation_step_ms)\n",
    "            if req.processing_left_ms - min_time_left <= 0:\n",
    "                req.processing_left_ms = 0\n",
    "                req.cumulative_time_ms += min_time_left\n",
    "                reqs_to_give.append(req)\n",
    "            else:\n",
    "                req.processing_left_ms -= min_time_left\n",
    "                \n",
    "        return reqs_to_give\n",
    "\n",
    "class NetworkModel:\n",
    "    def __init__(self,\n",
    "                 links_dict_in = None):\n",
    "        self.links_dict_in = links_dict_in\n",
    "        self.links_dict_out = {}\n",
    "        \n",
    "        # TODO: from file\n",
    "        for link_start, outs in self.links_dict.items():\n",
    "            for out, link in outs.items():\n",
    "                if out in self.links_dict_out:\n",
    "                    self.links_dict_out[out].append(link)\n",
    "                else:\n",
    "                    self.links_dict_out[out] = [link]\n",
    "                    \n",
    "    def put_request(self,\n",
    "                    start_service_lbl,\n",
    "                    end_service_lbl,\n",
    "                    req):\n",
    "        \n",
    "        self.links_dict_in[start_service_lbl][end_service_lbl].put_request(req)\n",
    "    \n",
    "    def get_requests(self,\n",
    "                     end_service_lbl):\n",
    "        reqs = []\n",
    "        links = self.links_dict_out[end_service_lbl]\n",
    "        for link in links:\n",
    "            reqs.extend(link.get_requests())\n",
    "            \n",
    "        return reqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
