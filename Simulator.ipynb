{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The <Noname> simulator's goal is to provide the fast and cheap testing ground\n",
    "# for the autoscaling policies/mechanism used in interactive transaction-based\n",
    "# applications deployed in the cloud using the containers.\n",
    "# The following models constitute the simulator:\n",
    "# 1) application model - determines the logical structure of the application, i.e.\n",
    "# which services communicate, how much time is required to process particular requests,\n",
    "# how large are the buffers for storing the queries awaiting the processing. Overall,\n",
    "# the application is represented as a general networked queueing model.\n",
    "# The components of the model:\n",
    "# - static -> the connections forming the logic of the application + processing times\n",
    "# - dynamic -> the amount of instances of the application services (containers)\n",
    "# 2) workload/load model - determines the load generated by the users of the application,\n",
    "# i.e. requests times, distribution of the requests in time (e.g. diurnal, seasonal),\n",
    "# the composition of the workload mixture (i.e. distribution of requests in terms of\n",
    "# required processing times, e.g. 80% small reqs, 20% large ones)\n",
    "# 3) scaling model - determines the scaling behaviour of the application, i.e. how\n",
    "# much time might be required to take/conduct the scaling decision.\n",
    "# 4) service level model - determines the expected level(s) of the service provided by\n",
    "# the application as a whole, e.g. in terms of the response times or distirbutions thereof,\n",
    "# in terms of services availability.\n",
    "# 5) platform model (hardware/ virtual machines) - models the most relevant characteristics\n",
    "# of the platform in terms of performance, e.g. the number of hw threads/cores that might\n",
    "# be needed to known to accomodate the demands of the logical service instance in terms\n",
    "# of threads. Note: we are not solving the placement problem! This is something done\n",
    "# by the cloud services provider.\n",
    "# 6) cost model - models the cost of the platform resource used during the simulation.\n",
    "# 7) failure/availability model - determines the failure mode of the platform/app, s.t.\n",
    "# the scaling procedure should be able to compensate for the unpredictably failing nodes.\n",
    "# 8) performance interference/tenancy model - determines, how much CPU stealing can happen\n",
    "# on the platform shared between the simulated application and some other application\n",
    "# co-deployed in the cloud on the same infra.\n",
    "# 9) scaling policy - provides a scaling plan that is executed by the simulation,\n",
    "# follows a particular instance or the combination thereof of scaling policies.\n",
    "# 10) network model - determines link latencies and bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulator's assumptions:\n",
    "# 1) Requests chains separation -> requests made by the user do not depend on other\n",
    "# requests by the user, although they can generate multiple other requests.\n",
    "# 2) Simulation step is smaller or equal to the processing duration at the smallest\n",
    "# component.\n",
    "# 3) No retries are performed if requests fails due to some issue, e.g. overfull buffers,\n",
    "# not enough throughput on the links, timeout reached\n",
    "# 4) All the nodes in the same geographical region. This assumption is valid since\n",
    "# the autoscalers often act independently on each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from collections import deque\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicedRequestsNumDistribution(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for generating the random number of requests\n",
    "    based on the corresponding distribution registered with it.\n",
    "    The class registered with it should define own generate method.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def generate(self, num):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def set_avg_param(self, avg_param):\n",
    "        pass\n",
    "    \n",
    "class NormalDistribution:\n",
    "    \"\"\"\n",
    "    Generates the random number of requests in the time slice\n",
    "    according to the normal distribution. Wraps the corresponding\n",
    "    call to the np.random.normal.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def generate(self, num = 1):\n",
    "        return np.random.normal(self.mu, self.sigma, num)\n",
    "    \n",
    "    def set_avg_param(self, avg_param):\n",
    "        self.mu = avg_param\n",
    "\n",
    "# Registering the derived sliced request number generators\n",
    "SlicedRequestsNumDistribution.register(NormalDistribution)\n",
    "\n",
    "class WorkloadModel:\n",
    "    \"\"\"\n",
    "    Represents the workload generation model.\n",
    "    The parameters are taken from the corresponding JSON file passed to the ctor.\n",
    "    The overall process for the workload generation works as follows:\n",
    "    \n",
    "        If the seasonal pattern of the workload is defined in terms of per interval values (cur. only per\n",
    "        hour values are supported!) then each such value is uniformly split among seconds in the given\n",
    "        hour (taken from the timestamp of the generate_requests call) s.t. each seconds in an hout gets\n",
    "        its own quota in terms of requests to be produced during this second. These values are computed and\n",
    "        stored in current_means_split_across_hour_seconds only if they were not computed before for the\n",
    "        given hour current_hour.\n",
    "        \n",
    "        Following, these per-second values are used as parameters for the generative random distribution\n",
    "        (e.g. as mean value for the normal distribution) -- the generated random value is used as\n",
    "        an adjusted per second quota for each type of request separately, normalized by the ratio param.\n",
    "        \n",
    "        Next, the adjusted per request per second quota is uniformly distributed among the *step units*\n",
    "        of the second. The number of step units is the number of millisecond size intervals of\n",
    "        simulation_step_ms duration that fit into the second. The computation is only conducted if\n",
    "        it was not done before for the currently considered second in an hour, i.e. cur_second_in_hour.\n",
    "        The data structure with the buckets that correspond to step units is current_req_split_across_simulation_steps.\n",
    "        \n",
    "        Lastly, we select a bucket of the current_req_split_across_simulation_steps which\n",
    "        the <second * 1000 + milliseconds>th millisecond of the timestamp_ms falls into. The selected value\n",
    "        is the number of requests generated & returned for the given timestamp. \n",
    "    \n",
    "    Properties:\n",
    "    \n",
    "        simulation_step_ms (int):                          simulation step in milliseconds, used to compute\n",
    "                                                           the uniform distribution of the requests to generate\n",
    "                                                           in the second (ms buckets); passed by the Simulator.\n",
    "        \n",
    "        reqs_types_ratios (dict):                          ratio of requests (value) of the given request type (key)\n",
    "                                                           in the mixture; from config file.\n",
    "        \n",
    "        reqs_generators (dict):                            random sliced requests num generator (value) for the\n",
    "                                                           request type (key); from config file.\n",
    "        \n",
    "        monthly_vals (dict):                               contains records for each month (1-12) and for the\n",
    "                                                           wildcard month, i.e. any month (0); each record\n",
    "                                                           holds the average numbers of requests (all types\n",
    "                                                           together) on a per hour basis for each day of the week\n",
    "                                                           (mon - 0, ... sun - 6). Thus, the structure is:\n",
    "                                                           month -> weekday -> hour -> avg requests number;\n",
    "                                                           from config file.\n",
    "        \n",
    "        discretion_s (int):                                the discretion (resolution) at which the avg request\n",
    "                                                           numbers are stored in the monthly_vals structure;\n",
    "                                                           from config file. Currently supports only hourly resolution.\n",
    "        \n",
    "        ********************************************************************************************************\n",
    "        \n",
    "        current_means_split_across_hour_seconds (dict):    contains the uniform split of\n",
    "                                                           the avg requests number from monthly_vals (per hour)\n",
    "                                                           into the seconds of the current_hour.\n",
    "        \n",
    "        current_second_leftover_reqs (dict):               tracks, how many more requests can be distributed\n",
    "                                                           among milliseconds bins of the cur_second_in_hour\n",
    "                                                           for the given request type. The distribution is in\n",
    "                                                           current_req_split_across_simulation_steps.\n",
    "        \n",
    "        current_req_split_across_simulation_steps (dict):  holds the distribution of the requests number in\n",
    "                                                           the bins of cur_second_in_hour per each request type.\n",
    "        \n",
    "        current_hour (int):                                current hour of the day for the timestamp_ms of the\n",
    "                                                           generate_requests() call. Used to retrieve the\n",
    "                                                           avg requests number from monthly_vals.\n",
    "        \n",
    "        cur_second_in_hour (int):                          current second in hour for the timestamp_ms\n",
    "        \n",
    "        workload (dict):                                   array of the numbers of requests generated for the timestamp (value) for the given\n",
    "                                                           request type\n",
    "                     \n",
    "    Methods:\n",
    "        generate_requests (timestamp_ms):                  generates a mixture of requests (list) using the reqs_types_ratios\n",
    "                                                           and reqs_generators with the provided timestamp.\n",
    "    \n",
    "    Usage:\n",
    "        wkldmdl = WorkloadModel(10, filename = 'experiments/test/workload.json')\n",
    "        len(wkldmdl.generate_requests(100))\n",
    "        \n",
    "    TODO:\n",
    "        implement support for holidays etc.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 simulation_step_ms,\n",
    "                 reqs_types_ratios = None,\n",
    "                 filename = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        self.reqs_types_ratios = {}\n",
    "        self.reqs_generators = {}\n",
    "        self.monthly_vals = {}\n",
    "        self.discretion_s = 0\n",
    "        \n",
    "        if filename is None:\n",
    "            raise ValueError('Configuration file not provided for the WorkloadModel.')\n",
    "        else:\n",
    "            with open(filename) as f:\n",
    "                config = json.load(f)\n",
    "                if config[\"seasonal_pattern\"][\"type\"] == \"values\":\n",
    "                    self.discretion_s = config[\"seasonal_pattern\"][\"discretion_s\"]\n",
    "                    \n",
    "                    for pattern in config[\"seasonal_pattern\"][\"params\"]:\n",
    "                        \n",
    "                        month_id = 0\n",
    "                        if not pattern[\"month\"] == \"all\":\n",
    "                            month_id = int(pattern[\"month\"])\n",
    "                            \n",
    "                        if not month_id in self.monthly_vals:\n",
    "                            self.monthly_vals[month_id] = {}\n",
    "                        \n",
    "                        if pattern[\"day_of_week\"] == \"weekday\":\n",
    "                            for day_id in range(5):\n",
    "                                self.monthly_vals[month_id][day_id] = pattern[\"values\"]\n",
    "                        elif pattern[\"day_of_week\"] == \"weekend\":\n",
    "                            for day_id in range(5, 7):\n",
    "                                self.monthly_vals[month_id][day_id] = pattern[\"values\"]\n",
    "                        else:\n",
    "                            raise ValueError('day_of_week value {} undefined for the WorkloadModel.'.format(pattern[\"day_of_week\"]))\n",
    "                    \n",
    "                for conf in config[\"workloads_configs\"]:\n",
    "                    req_type = conf[\"request_type\"]\n",
    "                    req_ratio = conf[\"workload_config\"][\"ratio\"]\n",
    "                    if req_ratio < 0.0 or req_ratio > 1.0:\n",
    "                        raise ValueError('Unacceptable ratio value for the request of type {}.'.format(req_type))\n",
    "                    self.reqs_types_ratios[req_type] = req_ratio\n",
    "                     \n",
    "                    req_distribution_type = conf[\"workload_config\"][\"sliced_distribution\"][\"type\"]\n",
    "                    req_distribution_params = conf[\"workload_config\"][\"sliced_distribution\"][\"params\"]\n",
    "                    \n",
    "                    if req_distribution_type == \"normal\":\n",
    "                        mu = 0.0\n",
    "                        sigma = 0.1\n",
    "                        \n",
    "                        if len(req_distribution_params) > 0:\n",
    "                            mu = req_distribution_params[0]\n",
    "                        if len(req_distribution_params) > 1:\n",
    "                            sigma = req_distribution_params[1]\n",
    "                        \n",
    "                        self.reqs_generators[req_type] = NormalDistribution(mu, sigma)\n",
    "                    \n",
    "        # Dynamic state  \n",
    "        self.current_means_split_across_hour_seconds = {} \n",
    "        for s in range(self.discretion_s):\n",
    "            self.current_means_split_across_hour_seconds[s] = 0\n",
    "        self.current_second_leftover_reqs = {}\n",
    "        for req_type, _ in self.reqs_types_ratios.items():\n",
    "            self.current_second_leftover_reqs[req_type] = -1\n",
    "        self.current_req_split_across_simulation_steps = {} \n",
    "        for req_type, _ in self.reqs_types_ratios.items():\n",
    "            ms_division = {}\n",
    "            for ms_bucket_id in range(1000 // self.simulation_step_ms):\n",
    "                ms_division[ms_bucket_id] = 0   \n",
    "            self.current_req_split_across_simulation_steps[req_type] = ms_division\n",
    "        self.current_hour = -1\n",
    "        self.cur_second_in_hour = -1\n",
    "        self.workload = {}    \n",
    "        \n",
    "    def generate_requests(self,\n",
    "                          timestamp_ms):\n",
    "        gen_reqs = []\n",
    "        \n",
    "        # Check if the split of the seasonal workload across the seconds of the hour is available\n",
    "        query_dt = datetime.fromtimestamp(int(timestamp_ms / 1000))\n",
    "        if not query_dt.hour == self.current_hour:\n",
    "            # Generate the split if not available\n",
    "            self.current_hour = query_dt.hour\n",
    "            \n",
    "            if len(self.monthly_vals) > 0:\n",
    "                month_id = 0\n",
    "                if query_dt.month in self.monthly_vals:\n",
    "                    month_id = query_dt.month\n",
    "                    \n",
    "                # TODO: currently only supported per hour vals\n",
    "                avg_reqs_val = self.monthly_vals[month_id][query_dt.weekday()][query_dt.hour]\n",
    "                if not self.discretion_s == 3600:\n",
    "                    raise ValueError('Currently, only hourly discretion is supported for the requests generation.')\n",
    "                else:\n",
    "                    for s in range(self.discretion_s):\n",
    "                        self.current_means_split_across_hour_seconds[s] = 0\n",
    "                        \n",
    "                    for _ in range(avg_reqs_val):  \n",
    "                        hour_sec_picked = np.random.randint(self.discretion_s)\n",
    "                        self.current_means_split_across_hour_seconds[hour_sec_picked] += 1\n",
    "        \n",
    "        # Generating initial number of requests for the current second\n",
    "        cur_second_in_hour = query_dt.minute * 60 + query_dt.second\n",
    "        avg_param = self.current_means_split_across_hour_seconds[cur_second_in_hour]\n",
    "        \n",
    "        if not self.cur_second_in_hour == cur_second_in_hour: \n",
    "            for key, _ in self.current_second_leftover_reqs.items():\n",
    "                self.current_second_leftover_reqs[key] = -1\n",
    "            self.cur_second_in_hour = cur_second_in_hour\n",
    "        \n",
    "        for req_type, ratio in self.reqs_types_ratios.items():\n",
    "            if self.current_second_leftover_reqs[req_type] < 0:\n",
    "                self.reqs_generators[req_type].set_avg_param(avg_param)\n",
    "                num_reqs = self.reqs_generators[req_type].generate()\n",
    "                req_types_reqs_num = int(ratio * num_reqs)\n",
    "                if req_types_reqs_num < 0:\n",
    "                    req_types_reqs_num = 0\n",
    "                    \n",
    "                self.current_second_leftover_reqs[req_type] = req_types_reqs_num\n",
    "                \n",
    "                for key, _ in self.current_req_split_across_simulation_steps[req_type].items():\n",
    "                    self.current_req_split_across_simulation_steps[req_type][key] = 0\n",
    "        \n",
    "                for _ in range(self.current_second_leftover_reqs[req_type]):\n",
    "                    ms_bucket_picked = np.random.randint(len(self.current_req_split_across_simulation_steps[req_type]))\n",
    "                    self.current_req_split_across_simulation_steps[req_type][ms_bucket_picked] += 1\n",
    "        \n",
    "        # Generating requests for the current simulation step\n",
    "        for req_type, ratio in self.reqs_types_ratios.items():\n",
    "            ms_bucket_picked = (timestamp_ms - int(query_dt.timestamp() * 1000) ) // self.simulation_step_ms\n",
    "            req_types_reqs_num = self.current_req_split_across_simulation_steps[req_type][ms_bucket_picked]\n",
    "\n",
    "            for i in range(req_types_reqs_num):\n",
    "                req = Request(req_type)\n",
    "                gen_reqs.append(req)\n",
    "                self.current_req_split_across_simulation_steps[req_type][ms_bucket_picked] -= 1\n",
    "                \n",
    "            if req_type in self.workload:\n",
    "                self.workload[req_type].append((timestamp_ms, req_types_reqs_num))\n",
    "            else:\n",
    "                self.workload[req_type] = [(timestamp_ms, req_types_reqs_num)]\n",
    "                \n",
    "        return gen_reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    def __init__(self,\n",
    "                 request_type,\n",
    "                 request_id = None):\n",
    "        # Static state\n",
    "        self.request_type = request_type\n",
    "        if request_id is None:\n",
    "            self.request_id = uuid.uuid1()\n",
    "        else:\n",
    "            self.request_id = request_id\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.processing_left_ms = 0\n",
    "        self.cumulative_time_ms = 0\n",
    "        self.upstream = True\n",
    "        self.replies_expected = 1 # to implement the fan-in on the level of service\n",
    "        \n",
    "    def set_downstream(self):\n",
    "        self.upstream = False\n",
    "    \n",
    "class RequestProcessingInfo:\n",
    "    def __init__(self,\n",
    "                 request_type,\n",
    "                 entry_service,\n",
    "                 processing_times,\n",
    "                 timeout_ms,\n",
    "                 request_size_b,\n",
    "                 response_size_b,\n",
    "                 request_operation_type):\n",
    "        \n",
    "        self.request_type = request_type\n",
    "        self.entry_service = entry_service\n",
    "        self.processing_times = processing_times\n",
    "        self.timeout_ms = timeout_ms\n",
    "        self.request_size_b = request_size_b\n",
    "        self.response_size_b = response_size_b\n",
    "        self.request_operation_type = request_operation_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkBuffer:\n",
    "    \"\"\"\n",
    "    Combines link and the buffer. The requests in the link advance when the step method is called.\n",
    "    If the processing time left (i.e. waiting on the link) is over, the request proceeds to the\n",
    "    buffer, where it can be extracted from for the further processing in a service.\n",
    "    If the used throughput reached the throughput limit, the request is lost. The same happens\n",
    "    if upon the transition to the buffer, the buffer is full for the given request type.\n",
    "    \n",
    "    Properties:\n",
    "    \n",
    "        As buffer:\n",
    "            capacity_by_request_type (dict):  holds capacity of the buffer by request type, in terms of\n",
    "                                              requests currently placed in the buffer\n",
    "                                              \n",
    "            [STUB] policy:                    policy used for moving requests in the buffer, e.g. FIFO/LIFO\n",
    "            \n",
    "            ********************************************************************************************************\n",
    "            \n",
    "            requests (collections.deque):     holds current requests in the buffer\n",
    "            \n",
    "            reqs_cnt (dict):                  holds current request count by the request type, used to rapidly check\n",
    "                                              if more requests of the given type can be accomodated in the buffer\n",
    "            \n",
    "        As link:\n",
    "            latency_ms (int):                 latency of the link in milliseconds, taken from the config\n",
    "            \n",
    "            throughput_mbps (int):            throughput of the link in Megabytes per sec, taken from the config\n",
    "            \n",
    "            request_processing_infos (dict):  holds requests processing information to compute the used\n",
    "                                              throughput etc.\n",
    "            \n",
    "            ********************************************************************************************************\n",
    "            \n",
    "            requests_in_transfer (list):      holds requests that are currently \"transferred\" by this link\n",
    "            \n",
    "            used_throughput_mbps (int):       throughput currently used on this link by the \"transferred\" reqs\n",
    "        \n",
    "    Methods:\n",
    "    \n",
    "        As buffer:\n",
    "            append_left (req):                puts the request req at the beginning of the buffer to give other\n",
    "                                              requests opportunity to be processed if the current request waits\n",
    "                                              for other replies to get processed (fan-in)\n",
    "            \n",
    "            pop:                              takes the last added request out of the buffer for processing (LIFO)\n",
    "            \n",
    "            pop_left:                         takes the first added request out of the buffer for processing (FIFO)\n",
    "            \n",
    "            size:                             returns size of the buffer\n",
    "            \n",
    "            add_cumulative_time (delta):      adds time delta to every request in the buffer\n",
    "            \n",
    "            remove_by_id (request_id):        removes all the requests with request id request_id from the buffer\n",
    "            \n",
    "        As link:\n",
    "            put (req):                        puts a new request req on a link, i.e. in requests_in_transfer,\n",
    "                                              if there is enough spare throughput; otherwise drops the request\n",
    "            \n",
    "            step (simulation_step_ms):        makes a discrete simulation time step of the length simulation_step_ms\n",
    "                                              to advance the requests held on the link, i.e. in requests_in_transfer,\n",
    "                                              and to put them into the buffer if possible (capacity left). In case of\n",
    "                                              no spare capacity in the buffer, the request is also dropped.\n",
    "        \n",
    "            _req_occupied_mbps (req):         private method that computes the throughput used by the request req\n",
    "    \n",
    "    TODO:\n",
    "        implementing scaling of the links? e.g. according to the added instances of services.\n",
    "        implement wrapping of the lower-level details into policies like LIFO, FIFO, etc. hide append, pop etc.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 capacity_by_request_type,\n",
    "                 request_processing_infos,\n",
    "                 latency_ms = 10, # TODO: remove val\n",
    "                 throughput_mbps = 1000, # TODO: remove val\n",
    "                 policy = \"FIFO\"): \n",
    "        \n",
    "        # Static state\n",
    "        # Buffer:\n",
    "        self.capacity_by_request_type = capacity_by_request_type\n",
    "        self.policy = policy\n",
    "        \n",
    "        # Link:\n",
    "        self.latency_ms = latency_ms\n",
    "        self.throughput_mbps = throughput_mbps\n",
    "        self.request_processing_infos = request_processing_infos\n",
    "        \n",
    "        # Dynamic state\n",
    "        # Buffer:\n",
    "        self.requests = deque([])\n",
    "        self.reqs_cnt = {}\n",
    "        for request_type in capacity_by_request_type.keys():\n",
    "            self.reqs_cnt[request_type] = 0\n",
    "            \n",
    "        # Link:\n",
    "        self.requests_in_transfer = []\n",
    "        self.used_throughput_mbps = 0\n",
    "    \n",
    "    def step(self, simulation_step_ms):\n",
    "        \"\"\" Processing requests to bring them from the link into the buffer \"\"\"\n",
    "        for req in self.requests_in_transfer:\n",
    "            min_time_to_subtract_ms = min(req.processing_left_ms, simulation_step_ms)\n",
    "            req.processing_left_ms -= min_time_to_subtract_ms\n",
    "            if req.processing_left_ms <= 0:\n",
    "                capacity = self.capacity_by_request_type[req.request_type]\n",
    "                self.used_throughput_mbps -= self._req_occupied_mbps(req)\n",
    "                \n",
    "                if self.reqs_cnt[req.request_type] == capacity:\n",
    "                    del req # dropping the request if no spare capacity\n",
    "                else:\n",
    "                    req.cumulative_time_ms += min_time_to_subtract_ms\n",
    "                    if req.cumulative_time_ms >= self.request_processing_infos[req.request_type].timeout_ms:\n",
    "                        del req\n",
    "                    else:\n",
    "                        self.requests.append(req)\n",
    "                        self.reqs_cnt[req.request_type] += 1 \n",
    "    \n",
    "    def put(self, req):\n",
    "        req_size_b_mbps = self._req_occupied_mbps(req)\n",
    "\n",
    "        if self.throughput_mbps - self.used_throughput_mbps >= req_size_b_mbps:\n",
    "            self.used_throughput_mbps += req_size_b_mbps\n",
    "            req.processing_left_ms = self.latency_ms\n",
    "            self.requests_in_transfer.append(req)\n",
    "        else:\n",
    "            del req\n",
    "    \n",
    "    def append_left(self, req):\n",
    "        self.requests.appendLeft(req)\n",
    "    \n",
    "    def pop(self):\n",
    "        req = None\n",
    "        if len(self.requests) > 0:\n",
    "            req = self.requests.pop()\n",
    "            self.reqs_cnt[req.request_type] -= 1\n",
    "            \n",
    "        return req\n",
    "    \n",
    "    def pop_left(self):\n",
    "        req = None\n",
    "        if len(self.requests) > 0:\n",
    "            req = self.requests.popLeft()\n",
    "            self.reqs_cnt[req.request_type] -= 1\n",
    "        \n",
    "        return req\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.requests)\n",
    "    \n",
    "    def add_cumulative_time(self, delta):\n",
    "        for req in self.requests:\n",
    "            req.cumulative_time_ms += delta\n",
    "    \n",
    "    def remove_by_id(self, request_id):\n",
    "        for req in reversed(self.requests):\n",
    "            if req.request_id == request_id:\n",
    "                self.requests.remove(req)\n",
    "                \n",
    "    def _req_occupied_mbps(self, req):\n",
    "        req_size_b = 0\n",
    "        if req.upstream:\n",
    "            req_size_b = self.request_processing_infos[req.request_type].request_size_b\n",
    "        else:\n",
    "            req_size_b = self.request_processing_infos[req.request_type].response_size_b\n",
    "        req_size_b_mb = req_size_b / (1024 * 1024)\n",
    "        req_size_b_mbps = req_size_b_mb * (self.latency_ms / 1000) # taking channel for that long\n",
    "        return req_size_b_mbps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Service:\n",
    "    def __init__(self,\n",
    "                 buffer_capacity_by_request_type,\n",
    "                 threads_per_instance,\n",
    "                 request_processing_infos,\n",
    "                 instances = 1,\n",
    "                 state_mb = 0):\n",
    "        \n",
    "        # Static state\n",
    "        # TODO: should replace with the real platform info below!\n",
    "        self.hw_threads_per_instance = 4\n",
    "        self.threads_per_instance = threads_per_instance\n",
    "        # If state_mb is 0, then the service is stateless\n",
    "        self.state_mb = state_mb\n",
    "        \n",
    "        # Dynamic state\n",
    "        # Upstream and downstream links/buffers of the service            \n",
    "        self.upstream_buf = LinkBuffer(buffer_capacity_by_request_type, request_processing_infos)\n",
    "        self.downstream_buf = LinkBuffer(buffer_capacity_by_request_type, request_processing_infos)\n",
    "        # Scaling-related\n",
    "        self.instances = instances\n",
    "        # requests that are currently in simultaneous processing\n",
    "        self.in_processing_simultaneous = []\n",
    "        # requests that are processed in this step, they can proceed\n",
    "        self.out = []\n",
    "        \n",
    "    def add_request(self, req):\n",
    "        # decide where to put the request\n",
    "        if req.upstream:\n",
    "            self.upstream_buf.put(req)\n",
    "        else:\n",
    "            self.downstream_buf.put(req)\n",
    "        \n",
    "    def step(self, simulation_step_ms):\n",
    "        processing_time_left_at_step = simulation_step_ms\n",
    "        \n",
    "        # Propagating requests in the link\n",
    "        self.downstream_buf.step(simulation_step_ms)  \n",
    "        self.upstream_buf.step(simulation_step_ms)\n",
    "        \n",
    "        while(processing_time_left_at_step > 0):\n",
    "           \n",
    "            if (self.downstream_buf.size() == 0) and (self.upstream_buf.size() == 0):\n",
    "                processing_time_left_at_step = 0\n",
    "                continue\n",
    "                \n",
    "            if len(self.in_processing_simultaneous) > 0:\n",
    "                # Find minimal leftover duration, subtract it,\n",
    "                # and propagate the request\n",
    "                min_leftover_time = min([req.processing_left_ms for req in self.in_processing_simultaneous])\n",
    "                min_time_to_subtract = min(min_leftover_time, processing_time_left_at_step)\n",
    "                new_in_processing_simultaneous = []\n",
    "                \n",
    "                for req in self.in_processing_simultaneous:\n",
    "                    new_time_left = req.processing_left_ms - min_time_to_subtract\n",
    "                    req.cumulative_time_ms += min_time_to_subtract\n",
    "                    if new_time_left > 0:\n",
    "                        req.processing_left_ms = new_time_left\n",
    "                        new_in_processing_simultaneous.append(req)\n",
    "                    else:\n",
    "                        # Request is put into the out buffer to be\n",
    "                        # processed further according to the app structure\n",
    "                        req.processing_left_ms = 0  \n",
    "                        self.out.append(req)\n",
    "                \n",
    "                processing_time_left_at_step -= min_time_to_subtract\n",
    "                self.in_processing_simultaneous = new_in_processing_simultaneous\n",
    "                \n",
    "            spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            # Assumption: first we try to process the downstream reqs to\n",
    "            # provide the response faster, but overall it is application-dependent\n",
    "            while ((self.downstream_buf.size() > 0) or (self.upstream_buf.size() > 0)) and (spare_capacity > 0):\n",
    "                if self.downstream_buf.size() > 0:\n",
    "                    req = self.downstream_buf.requests[-1]\n",
    "                    # Processing fan-in case\n",
    "                    if req.replies_expected > 1:\n",
    "                        req_id_ref = req.request_id\n",
    "                        reqs_present = 1\n",
    "                        for req_lookup in self.downstream_buf.requests[:-1]:\n",
    "                            if req_lookup.request_id == req_id_ref:\n",
    "                                reqs_present += 1\n",
    "                                \n",
    "                        if reqs_present == req.replies_expected:\n",
    "                            req = self.downstream_buf.pop()\n",
    "                            # Removing all the related requests\n",
    "                            self.downstream_buf.remove_by_id(req_id_ref)\n",
    "                        else:\n",
    "                            # pushing to the beginning of the deque to enable\n",
    "                            # progress in processing the downstream reqs\n",
    "                            req = self.downstream_buf.pop()\n",
    "                            self.downstream_buf.append_left(req)\n",
    "                            req = None\n",
    "                            \n",
    "                    else: \n",
    "                        req = self.downstream_buf.pop()\n",
    "                    \n",
    "                    if not req is None:\n",
    "                        req.replies_expected = 1\n",
    "                        self.in_processing_simultaneous.append(req)\n",
    "                        spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            \n",
    "                if self.upstream_buf.size() > 0:\n",
    "                    req = self.upstream_buf.pop()\n",
    "                    self.in_processing_simultaneous.append(req)\n",
    "                    spare_capacity = int(self.instances * self.hw_threads_per_instance - len(self.in_processing_simultaneous) * self.threads_per_instance)\n",
    "            \n",
    "        # Increase the cumulative time for all the reqs left in the buffers waiting\n",
    "        self.upstream_buf.add_cumulative_time(simulation_step_ms)\n",
    "        self.downstream_buf.add_cumulative_time(simulation_step_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application model incorps params of the requests since it depends on the\n",
    "# structure and the processing logic of the app, whereas the workload model\n",
    "# quantifies the amount and the distribution of the requests in time/volume\n",
    "# so, propagation chain of the request goes into the app model\n",
    "\n",
    "# Services: {'frontend': service1, 'appserver': service2, 'db': service3}\n",
    "# Structure: {'frontend': ['appserver'], 'appserver': ['db'], 'db': None}\n",
    "# Starting points: ['frontend']\n",
    "# reqs_processing_infos: {'auth': req_proc_info1} -> also store the reverse path!\n",
    "class ApplicationModel:\n",
    "    def __init__(self,\n",
    "                 filename = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.name = None\n",
    "        self.services = {}\n",
    "        self.structure = {}\n",
    "        self.reqs_processing_infos = {}\n",
    "        \n",
    "        if filename is None:\n",
    "            raise ValueError('Configuration file not provided for the ApplicationModel.')\n",
    "        else:\n",
    "            with open(filename) as f:\n",
    "                config = json.load(f)\n",
    "                \n",
    "                self.name = config[\"app_name\"]\n",
    "                \n",
    "                for request_info in config[\"requests\"]:\n",
    "                    request_type = request_info[\"request_type\"]\n",
    "                    entry_service = request_info[\"entry_service\"]\n",
    "                    \n",
    "                    processing_times = {}\n",
    "                    for processing_time_service_entry in request_info[\"processing_times_ms\"]:\n",
    "                        service_name = processing_time_service_entry[\"service\"]\n",
    "                        upstream_time = processing_time_service_entry[\"upstream\"]\n",
    "                        if upstream_time < 0:\n",
    "                            raise ValueError('The upstream time for the request {} when passing through the service {} is negative.'.format(request_type, service_name))\n",
    "                        downstream_time = processing_time_service_entry[\"downstream\"]\n",
    "                        if downstream_time < 0:\n",
    "                            raise ValueError('The downstream time for the request {} when passing through the service {} is negative.'.format(request_type, service_name))\n",
    "                        \n",
    "                        processing_times[service_name] = [upstream_time, downstream_time]\n",
    "                    \n",
    "                    timeout_ms = request_info[\"timeout_ms\"]\n",
    "                    if timeout_ms < 0:\n",
    "                        raise ValueError('The timeout value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    request_size_b = request_info[\"request_size_b\"]\n",
    "                    if request_size_b < 0:\n",
    "                        raise ValueError('The request size value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    response_size_b = request_info[\"response_size_b\"]\n",
    "                    if response_size_b < 0:\n",
    "                        raise ValueError('The response size value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    request_operation_type = request_info[\"operation_type\"]\n",
    "                    \n",
    "                    req_proc_info = RequestProcessingInfo(request_type,\n",
    "                                                          entry_service,\n",
    "                                                          processing_times,\n",
    "                                                          timeout_ms,\n",
    "                                                          request_size_b,\n",
    "                                                          response_size_b,\n",
    "                                                          request_operation_type)\n",
    "                    self.reqs_processing_infos[request_type] = req_proc_info\n",
    "                \n",
    "                for service_config in config[\"services\"]:\n",
    "                    # Creating & adding the service:\n",
    "                    service_name = service_config[\"name\"]\n",
    "                    \n",
    "                    buffer_capacity_by_request_type = {}\n",
    "                    for buffer_capacity_config in service_config[\"buffer_capacity_by_request_type\"]:\n",
    "                        request_type = buffer_capacity_config[\"request_type\"]\n",
    "                        capacity = buffer_capacity_config[\"capacity\"]\n",
    "                        if capacity <= 0:\n",
    "                            raise ValueError('Buffer capacity is not positive for request type {} of service {}.'.format(request_type, service_name))\n",
    "                        buffer_capacity_by_request_type[request_type] = capacity\n",
    "                        \n",
    "                    threads_per_instance = service_config[\"threads_per_instance\"]\n",
    "                    if threads_per_instance <= 0:\n",
    "                        raise ValueError('Threads per instance is not positive for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    starting_instances_num = service_config[\"starting_instances_num\"]\n",
    "                    if starting_instances_num <= 0:\n",
    "                        raise ValueError('Number of service instances to start with is not positive for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    state_mb = service_config[\"state_mb\"]\n",
    "                    if state_mb < 0:\n",
    "                        raise ValueError('The state size is negative for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    service = Service(buffer_capacity_by_request_type = buffer_capacity_by_request_type,\n",
    "                                      threads_per_instance = threads_per_instance,\n",
    "                                      request_processing_infos = self.reqs_processing_infos,\n",
    "                                      instances = starting_instances_num,\n",
    "                                      state_mb = state_mb)\n",
    "                    \n",
    "                    self.services[service_name] = service\n",
    "                    \n",
    "                    # Adding the links of the given service to the structure.\n",
    "                    # TODO: think of whether the broken simmetry of the links\n",
    "                    # is appropriate.\n",
    "                    next_services = service_config[\"next\"]\n",
    "                    prev_services = service_config[\"prev\"]\n",
    "                    if len(next_services) == 0:\n",
    "                        next_services = None\n",
    "                    if len(prev_services) == 0:\n",
    "                        prev_services = None\n",
    "                    self.structure[service_name] = {'next': next_services, 'prev': prev_services}\n",
    "                    \n",
    "                \n",
    "        \n",
    "        # Dynamic state\n",
    "        self.new_requests = []\n",
    "        self.response_times_by_request = {}\n",
    "        \n",
    "    def step(self, simulation_step_ms):\n",
    "        if len(self.new_requests) > 0:\n",
    "            for req in self.new_requests:\n",
    "                entry_service = self.reqs_processing_infos[req.request_type].entry_service\n",
    "                req.processing_left_ms = self.reqs_processing_infos[req.request_type].processing_times[entry_service][1]\n",
    "                self.services[entry_service].add_request(req)\n",
    "                \n",
    "            self.new_requests = []\n",
    "        \n",
    "        # Proceed through the services // fan-in merge and fan-out copying\n",
    "        # is done in the app logic since it knows the structure and times\n",
    "        # IMPORTANT: the simulation step should be small for the following\n",
    "        # processing to work correctly ~5-10 ms.\n",
    "        for service_name, service in self.services.items():\n",
    "            service.step(simulation_step_ms)\n",
    "            \n",
    "            while len(service.out) > 0:\n",
    "                req = service.out.pop()\n",
    "                req_info = self.reqs_processing_infos[req.request_type]\n",
    "                \n",
    "                if req.upstream:\n",
    "                    next_services_names = self.structure[service_name]['next']\n",
    "                    if not next_services_names is None:\n",
    "                        for next_service_name in next_services_names:\n",
    "                            if next_service_name in req_info.processing_times:\n",
    "                                req_cpy = req\n",
    "                                req_cpy.processing_left_ms = req_info.processing_times[next_service_name][0]\n",
    "                                self.services[next_service_name].add_request(req_cpy)\n",
    "                    else:\n",
    "                        # Sending response\n",
    "                        req.upstream = False\n",
    "                        \n",
    "                if not req.upstream:\n",
    "                    prev_services_names = self.structure[service_name]['prev']\n",
    "                    \n",
    "                    if not prev_services_names is None:\n",
    "                        replies_expected = 0\n",
    "                        for prev_service_name in prev_services_names:\n",
    "                            if prev_service_name in req_info.processing_times:\n",
    "                                replies_expected += 1\n",
    "                        \n",
    "                        for prev_service_name in prev_services_names:\n",
    "                            if prev_service_name in req_info.processing_times:\n",
    "                                req_cpy = req\n",
    "                                req_cpy.processing_left_ms = req_info.processing_times[prev_service_name][1]\n",
    "                                req_cpy.replies_expected = replies_expected\n",
    "                                self.services[prev_service_name].add_request(req_cpy)\n",
    "                    else:\n",
    "                        # Response received by the user\n",
    "                        if req.request_type in self.response_times_by_request:\n",
    "                            self.response_times_by_request[req.request_type].append(req.cumulative_time_ms)\n",
    "                        else:\n",
    "                            self.response_times_by_request[req.request_type] = [req.cumulative_time_ms]\n",
    "                        del req\n",
    "                        \n",
    "    def enter_requests(self, new_requests):\n",
    "        self.new_requests = new_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    \"\"\"\n",
    "    Manages the high-level simulation process and stores all the simulation-relevant variables.\n",
    "    Before the time time_to_simulate_ms is reached, each simulation_step_ms milliseconds a new\n",
    "    simulation step is taken by calling _step method. At each simulations step:\n",
    "    1) new requests entering the simulation are generated by the workload_model,\n",
    "    2) the generated requests are added to the application_model,\n",
    "    3) a simulation step of the application model is taken, which implies taking the corresponding\n",
    "       simulation steps on its services and links/buffers.\n",
    "    If the results_dir is provided then the resulting response times for the requests and the\n",
    "    workload generated per timestamp is stored in a pickle file marked with the application name\n",
    "    taken from the config file and the date and time of the simulation.\n",
    "    The progress of the simulation is tracked and presented by the progress bar.\n",
    "    \n",
    "    Properties:\n",
    "    \n",
    "        workload_model:                   the model that is used to generate the workload\n",
    "        \n",
    "        application_model:                the application model that is simulated by calling its step() method\n",
    "        \n",
    "        time_to_simulate_ms (int):        the interval of time that should be simulated, in milliseconds\n",
    "        \n",
    "        simulation_step_ms (int):         the discretion of the simulation. IMPORTANT: in order to apporach the\n",
    "                                          asynchronous behaviour of the real applications as close as possible\n",
    "                                          this parameter should be as small as possible. Meaningful numbers for\n",
    "                                          this parameters are around 10-20 ms, smaller ones may result in\n",
    "                                          very large execution time of the simulation. Ideally, this step should\n",
    "                                          be not larger than the smallest time required to process any request\n",
    "                                          on link/in service.\n",
    "                                          \n",
    "        stat_updates_every_round (int):   every stat_updates_every_round<th> round a stat summary is printed.\n",
    "                                          If equals 0 then only the progress bar is available, which makes\n",
    "                                          impossible to observe the time spent at each stat_updates_every_round\n",
    "                                          rounds.\n",
    "                                          \n",
    "        results_dir (string):             a directory used to store the results of the simulation, i.e.\n",
    "                                          reponse times of individual requests and the time series of the\n",
    "                                          generated workload. If dir does not exist, it is created.\n",
    "        \n",
    "        ********************************************************************************************************\n",
    "        \n",
    "        cur_simulation_time_ms (int):     current simulation time in milliseconds; on the initialization of the\n",
    "                                          simulation it is set up to equal simulation_start_datetime converted\n",
    "                                          to ms. Increments by simulation_step_ms on each simulation step. \n",
    "                                          \n",
    "        sim_round (int):                  current simulation round.\n",
    "    \n",
    "    Methods:\n",
    "    \n",
    "        start:                            main simulation loop which continues until the simulation runs\n",
    "                                          out of time, i.e. cur_simulation_time_ms > time_to_simulate_ms.\n",
    "                                          \n",
    "        _step:                            a private simulation step method that should only be called\n",
    "                                          inside the start method.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 workload_model,\n",
    "                 application_model,\n",
    "                 simulation_start_datetime,\n",
    "                 time_to_simulate_days = 0.0005,\n",
    "                 simulation_step_ms = 10,\n",
    "                 stat_updates_every_round = 0,\n",
    "                 results_dir = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.workload_model = workload_model\n",
    "        self.application_model = application_model\n",
    "        self.time_to_simulate_ms = int(simulation_start_datetime.timestamp() * 1000) + int(time_to_simulate_days * 24 * 60 * 60 * 1000)\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        self.stat_updates_every_round = stat_updates_every_round\n",
    "        self.results_dir = results_dir\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.cur_simulation_time_ms = int(simulation_start_datetime.timestamp() * 1000)\n",
    "        self.sim_round = 0\n",
    "        \n",
    "    def start(self):\n",
    "        \n",
    "        left_to_simulate_ms = self.time_to_simulate_ms - self.cur_simulation_time_ms\n",
    "        left_to_simulate_steps = left_to_simulate_ms // self.simulation_step_ms\n",
    "        \n",
    "        with tqdm(total = left_to_simulate_steps) as progress_bar:\n",
    "            while(self.cur_simulation_time_ms <= self.time_to_simulate_ms):\n",
    "                self.cur_simulation_time_ms += self.simulation_step_ms\n",
    "                self._step()\n",
    "                self.sim_round += 1\n",
    "                if self.stat_updates_every_round > 0:\n",
    "                    if self.sim_round % self.stat_updates_every_round == 0:\n",
    "                        left_to_simulate_ms = self.time_to_simulate_ms - self.cur_simulation_time_ms\n",
    "                        left_to_simulate_steps = left_to_simulate_ms // self.simulation_step_ms\n",
    "                        print('[{}] Left to simulate: {} min or {} steps'.format(datetime.now(),\n",
    "                                                                                 int(left_to_simulate_ms / (1000 * 60)),\n",
    "                                                                                 left_to_simulate_steps))\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        # Storing the simulation results on a disk\n",
    "        if not self.results_dir is None:\n",
    "            filename = self.application_model.name + \"DT\" + datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\") + \".pkl\"\n",
    "            if not os.path.exists(self.results_dir):\n",
    "                os.mkdir(self.results_dir)\n",
    "                full_filename = os.path.join(results_dir, filename)\n",
    "                results_to_store = {\"response_times_by_request\": self.application_model.response_times_by_request,\n",
    "                                    \"workload\": self.workload_model.workload}\n",
    "                \n",
    "                with open(full_filename, 'wb') as f:\n",
    "                    pickle.dump(results_to_store, f)\n",
    "    \n",
    "    def _step(self):\n",
    "        new_requests = self.workload_model.generate_requests(self.cur_simulation_time_ms)\n",
    "        self.application_model.enter_requests(new_requests)\n",
    "        self.application_model.step(self.simulation_step_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 1059/4319 [00:01<00:05, 563.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-13 14:00:03.188536] Left to simulate: 0 min or 3319 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 2022/4319 [00:04<00:11, 196.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-13 14:00:06.804525] Left to simulate: 0 min or 2319 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 3012/4319 [00:12<00:11, 117.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-13 14:00:14.445547] Left to simulate: 0 min or 1319 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 4012/4319 [00:23<00:03, 81.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-13 14:00:24.984125] Left to simulate: 0 min or 319 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4320it [00:27, 158.73it/s]                         \n"
     ]
    }
   ],
   "source": [
    "wlm_test = WorkloadModel(simulation_step_ms = 10, filename = 'experiments/test/workload.json')\n",
    "appm_test = ApplicationModel('experiments/test/application.json')\n",
    "sim = Simulation(wlm_test, appm_test, datetime.now(), stat_updates_every_round = 1000)\n",
    "sim.start()\n",
    "# TODO: check how to make the duration of the step ~constant, now it increases - 3, 5, 8 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.,  9., 12., 11., 10., 12., 22., 11., 11., 11., 11., 11., 22.,\n",
       "        10., 13.]),\n",
       " array([ 50.        ,  73.33333333,  96.66666667, 120.        ,\n",
       "        143.33333333, 166.66666667, 190.        , 213.33333333,\n",
       "        236.66666667, 260.        , 283.33333333, 306.66666667,\n",
       "        330.        , 353.33333333, 376.66666667, 400.        ]),\n",
       " <a list of 15 Patch objects>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n_bins = 15\n",
    "\n",
    "# Generate a normal distribution, center at x=0 and y=5\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs[0].hist(sim.application_model.response_times_by_request['auth'], bins=n_bins)\n",
    "axs[1].hist(sim.application_model.response_times_by_request['buy'], bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.median(sim.application_model.response_times_by_request['auth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        \n",
    "rt = [Test(), Test(), Test()]\n",
    "for t in rt:\n",
    "    t.num += 4\n",
    "\n",
    "for t in rt:\n",
    "    print(t.num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt = [1, 2, 3]\n",
    "rt.remove(1)\n",
    "rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider scaling links?\n",
    "class Link:\n",
    "    def __init__(self,\n",
    "                 simulation_step_ms,\n",
    "                 latency,\n",
    "                 bandwidth = 1000):\n",
    "        # Static state\n",
    "        self.latency = latency\n",
    "        self.bandwidth = bandwidth\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.requests = [] \n",
    "        \n",
    "    def put_request(self, req):\n",
    "        if len(self.requests) < self.bandwidth:\n",
    "            req.processing_left_ms = self.latency\n",
    "            self.requests.append(req)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_requests(self):\n",
    "        # Called every simulation step thus updating reqs\n",
    "        reqs_to_give = []\n",
    "        for req in self.requests:\n",
    "            min_time_left = min(req.processing_left_ms, self.simulation_step_ms)\n",
    "            if req.processing_left_ms - min_time_left <= 0:\n",
    "                req.processing_left_ms = 0\n",
    "                req.cumulative_time_ms += min_time_left\n",
    "                reqs_to_give.append(req)\n",
    "            else:\n",
    "                req.processing_left_ms -= min_time_left\n",
    "                \n",
    "        return reqs_to_give\n",
    "\n",
    "class NetworkModel:\n",
    "    def __init__(self,\n",
    "                 links_dict_in = None):\n",
    "        self.links_dict_in = links_dict_in\n",
    "        self.links_dict_out = {}\n",
    "        \n",
    "        # TODO: from file\n",
    "        for link_start, outs in self.links_dict.items():\n",
    "            for out, link in outs.items():\n",
    "                if out in self.links_dict_out:\n",
    "                    self.links_dict_out[out].append(link)\n",
    "                else:\n",
    "                    self.links_dict_out[out] = [link]\n",
    "                    \n",
    "    def put_request(self,\n",
    "                    start_service_lbl,\n",
    "                    end_service_lbl,\n",
    "                    req):\n",
    "        \n",
    "        self.links_dict_in[start_service_lbl][end_service_lbl].put_request(req)\n",
    "    \n",
    "    def get_requests(self,\n",
    "                     end_service_lbl):\n",
    "        reqs = []\n",
    "        links = self.links_dict_out[end_service_lbl]\n",
    "        for link in links:\n",
    "            reqs.extend(link.get_requests())\n",
    "            \n",
    "        return reqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
