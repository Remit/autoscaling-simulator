{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buratino: A Golden Key to Understanding the Millisecond-Scale Autoscaling\n",
    "\n",
    "# The Buratino simulator's goal is to provide the fast and cheap testing ground\n",
    "# for the autoscaling policies/mechanism used in interactive transaction-based\n",
    "# applications deployed in the cloud using the containers.\n",
    "# The following models constitute the simulator:\n",
    "# 1) application model - determines the logical structure of the application, i.e.\n",
    "# which services communicate, how much time is required to process particular requests,\n",
    "# how large are the buffers for storing the queries awaiting the processing. Overall,\n",
    "# the application is represented as a general networked queueing model.\n",
    "# The components of the model:\n",
    "# - static -> the connections forming the logic of the application + processing times\n",
    "# - dynamic -> the amount of instances of the application services (containers)\n",
    "# 2) workload/load model - determines the load generated by the users of the application,\n",
    "# i.e. requests times, distribution of the requests in time (e.g. diurnal, seasonal),\n",
    "# the composition of the workload mixture (i.e. distribution of requests in terms of\n",
    "# required processing times, e.g. 80% small reqs, 20% large ones)\n",
    "# 3) scaling model - determines the scaling behaviour of the application, i.e. how\n",
    "# much time might be required to take/conduct the scaling decision.\n",
    "# 4) service level model - determines the expected level(s) of the service provided by\n",
    "# the application as a whole, e.g. in terms of the response times or distirbutions thereof,\n",
    "# in terms of services availability.\n",
    "# 5) platform model (hardware/ virtual machines) - models the most relevant characteristics\n",
    "# of the platform in terms of performance, e.g. the number of hw threads/cores that might\n",
    "# be needed to known to accomodate the demands of the logical service instance in terms\n",
    "# of threads. Note: we are not solving the placement problem! This is something done\n",
    "# by the cloud services provider.\n",
    "# 6) cost model - models the cost of the platform resource used during the simulation.\n",
    "# 7) failure/availability model - determines the failure mode of the platform/app, s.t.\n",
    "# the scaling procedure should be able to compensate for the unpredictably failing nodes.\n",
    "# 8) performance interference/tenancy model - determines, how much CPU stealing can happen\n",
    "# on the platform shared between the simulated application and some other application\n",
    "# co-deployed in the cloud on the same infra.\n",
    "# 9) scaling policy - provides a scaling plan that is executed by the simulation,\n",
    "# follows a particular instance or the combination thereof of scaling policies.\n",
    "# 10) network model - determines link latencies and bandwidth\n",
    "# 11) deployment model - determines initial state of the app deployment, integrated into app config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypotheses to check:\n",
    "# 1) utilization-based autoscaling may fail to give the application owner better \n",
    "# service level -- hence, not the same as the workload-based autoscaling\n",
    "# 2) reactive autoscaling policy (any! both util-based and w/l-based) suffers to scale correctly for the incoming workload\n",
    "# on the millisecond scale (flash crowd) even if the scaling constants are to the \n",
    "# advantage thereof (instant scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulator's assumptions:\n",
    "# 1) Requests chains separation -> requests made by the user do not depend on other\n",
    "# requests by the user, although they can generate multiple other requests.\n",
    "# 2) Simulation step is smaller or equal to the processing duration at the smallest\n",
    "# component.\n",
    "# 3) No retries are performed if requests fails due to some issue, e.g. overfull buffers,\n",
    "# not enough throughput on the links, timeout reached\n",
    "# 4) All the nodes in the same geographical region. This assumption is valid since\n",
    "# the autoscalers often act independently on each region.\n",
    "# 5) Request is processed at once by the service w/o interruptions. Future: implement OS sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from collections import deque\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiments/test/t1.json'"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('experimentjs')\n",
    "os.path.join('experiments/test','t1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicedRequestsNumDistribution(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for generating the random number of requests\n",
    "    based on the corresponding distribution registered with it.\n",
    "    The class registered with it should define own generate method.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def generate(self, num):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def set_avg_param(self, avg_param):\n",
    "        pass\n",
    "    \n",
    "class NormalDistribution:\n",
    "    \"\"\"\n",
    "    Generates the random number of requests in the time slice\n",
    "    according to the normal distribution. Wraps the corresponding\n",
    "    call to the np.random.normal.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def generate(self, num = 1):\n",
    "        return np.random.normal(self.mu, self.sigma, num)\n",
    "    \n",
    "    def set_avg_param(self, avg_param):\n",
    "        self.mu = avg_param\n",
    "\n",
    "# Registering the derived sliced request number generators\n",
    "SlicedRequestsNumDistribution.register(NormalDistribution)\n",
    "\n",
    "class WorkloadModel:\n",
    "    \"\"\"\n",
    "    Represents the workload generation model.\n",
    "    The parameters are taken from the corresponding JSON file passed to the ctor.\n",
    "    The overall process for the workload generation works as follows:\n",
    "    \n",
    "        If the seasonal pattern of the workload is defined in terms of per interval values (cur. only per\n",
    "        hour values are supported!) then each such value is uniformly split among seconds in the given\n",
    "        hour (taken from the timestamp of the generate_requests call) s.t. each seconds in an hout gets\n",
    "        its own quota in terms of requests to be produced during this second. These values are computed and\n",
    "        stored in current_means_split_across_hour_seconds only if they were not computed before for the\n",
    "        given hour current_hour.\n",
    "        \n",
    "        Following, these per-second values are used as parameters for the generative random distribution\n",
    "        (e.g. as mean value for the normal distribution) -- the generated random value is used as\n",
    "        an adjusted per second quota for each type of request separately, normalized by the ratio param.\n",
    "        \n",
    "        Next, the adjusted per request per second quota is uniformly distributed among the *step units*\n",
    "        of the second. The number of step units is the number of millisecond size intervals of\n",
    "        simulation_step_ms duration that fit into the second. The computation is only conducted if\n",
    "        it was not done before for the currently considered second in an hour, i.e. cur_second_in_hour.\n",
    "        The data structure with the buckets that correspond to step units is current_req_split_across_simulation_steps.\n",
    "        \n",
    "        Lastly, we select a bucket of the current_req_split_across_simulation_steps which\n",
    "        the <second * 1000 + milliseconds>th millisecond of the timestamp_ms falls into. The selected value\n",
    "        is the number of requests generated & returned for the given timestamp. \n",
    "    \n",
    "    Properties:\n",
    "    \n",
    "        simulation_step_ms (int):                          simulation step in milliseconds, used to compute\n",
    "                                                           the uniform distribution of the requests to generate\n",
    "                                                           in the second (ms buckets); passed by the Simulator.\n",
    "        \n",
    "        reqs_types_ratios (dict):                          ratio of requests (value) of the given request type (key)\n",
    "                                                           in the mixture; from config file.\n",
    "        \n",
    "        reqs_generators (dict):                            random sliced requests num generator (value) for the\n",
    "                                                           request type (key); from config file.\n",
    "        \n",
    "        monthly_vals (dict):                               contains records for each month (1-12) and for the\n",
    "                                                           wildcard month, i.e. any month (0); each record\n",
    "                                                           holds the average numbers of requests (all types\n",
    "                                                           together) on a per hour basis for each day of the week\n",
    "                                                           (mon - 0, ... sun - 6). Thus, the structure is:\n",
    "                                                           month -> weekday -> hour -> avg requests number;\n",
    "                                                           from config file.\n",
    "        \n",
    "        discretion_s (int):                                the discretion (resolution) at which the avg request\n",
    "                                                           numbers are stored in the monthly_vals structure;\n",
    "                                                           from config file. Currently supports only hourly resolution.\n",
    "        \n",
    "        ********************************************************************************************************\n",
    "        \n",
    "        current_means_split_across_hour_seconds (dict):    contains the uniform split of\n",
    "                                                           the avg requests number from monthly_vals (per hour)\n",
    "                                                           into the seconds of the current_hour.\n",
    "        \n",
    "        current_second_leftover_reqs (dict):               tracks, how many more requests can be distributed\n",
    "                                                           among milliseconds bins of the cur_second_in_hour\n",
    "                                                           for the given request type. The distribution is in\n",
    "                                                           current_req_split_across_simulation_steps.\n",
    "        \n",
    "        current_req_split_across_simulation_steps (dict):  holds the distribution of the requests number in\n",
    "                                                           the bins of cur_second_in_hour per each request type.\n",
    "        \n",
    "        current_hour (int):                                current hour of the day for the timestamp_ms of the\n",
    "                                                           generate_requests() call. Used to retrieve the\n",
    "                                                           avg requests number from monthly_vals.\n",
    "        \n",
    "        cur_second_in_hour (int):                          current second in hour for the timestamp_ms\n",
    "        \n",
    "        workload (dict):                                   array of the numbers of requests generated for the timestamp (value) for the given\n",
    "                                                           request type\n",
    "                     \n",
    "    Methods:\n",
    "        generate_requests (timestamp_ms):                  generates a mixture of requests (list) using the reqs_types_ratios\n",
    "                                                           and reqs_generators with the provided timestamp.\n",
    "    \n",
    "    Usage:\n",
    "        wkldmdl = WorkloadModel(10, filename = 'experiments/test/workload.json')\n",
    "        len(wkldmdl.generate_requests(100))\n",
    "        \n",
    "    TODO:\n",
    "        implement support for holidays etc.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 simulation_step_ms,\n",
    "                 reqs_types_ratios = None,\n",
    "                 filename = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        self.reqs_types_ratios = {}\n",
    "        self.reqs_generators = {}\n",
    "        self.monthly_vals = {}\n",
    "        self.discretion_s = 0\n",
    "        \n",
    "        if filename is None:\n",
    "            raise ValueError('Configuration file not provided for the WorkloadModel.')\n",
    "        else:\n",
    "            with open(filename) as f:\n",
    "                config = json.load(f)\n",
    "                if config[\"seasonal_pattern\"][\"type\"] == \"values\":\n",
    "                    self.discretion_s = config[\"seasonal_pattern\"][\"discretion_s\"]\n",
    "                    \n",
    "                    for pattern in config[\"seasonal_pattern\"][\"params\"]:\n",
    "                        \n",
    "                        month_id = 0\n",
    "                        if not pattern[\"month\"] == \"all\":\n",
    "                            month_id = int(pattern[\"month\"])\n",
    "                            \n",
    "                        if not month_id in self.monthly_vals:\n",
    "                            self.monthly_vals[month_id] = {}\n",
    "                        \n",
    "                        if pattern[\"day_of_week\"] == \"weekday\":\n",
    "                            for day_id in range(5):\n",
    "                                self.monthly_vals[month_id][day_id] = pattern[\"values\"]\n",
    "                        elif pattern[\"day_of_week\"] == \"weekend\":\n",
    "                            for day_id in range(5, 7):\n",
    "                                self.monthly_vals[month_id][day_id] = pattern[\"values\"]\n",
    "                        else:\n",
    "                            raise ValueError('day_of_week value {} undefined for the WorkloadModel.'.format(pattern[\"day_of_week\"]))\n",
    "                    \n",
    "                for conf in config[\"workloads_configs\"]:\n",
    "                    req_type = conf[\"request_type\"]\n",
    "                    req_ratio = conf[\"workload_config\"][\"ratio\"]\n",
    "                    if req_ratio < 0.0 or req_ratio > 1.0:\n",
    "                        raise ValueError('Unacceptable ratio value for the request of type {}.'.format(req_type))\n",
    "                    self.reqs_types_ratios[req_type] = req_ratio\n",
    "                     \n",
    "                    req_distribution_type = conf[\"workload_config\"][\"sliced_distribution\"][\"type\"]\n",
    "                    req_distribution_params = conf[\"workload_config\"][\"sliced_distribution\"][\"params\"]\n",
    "                    \n",
    "                    if req_distribution_type == \"normal\":\n",
    "                        mu = 0.0\n",
    "                        sigma = 0.1\n",
    "                        \n",
    "                        if len(req_distribution_params) > 0:\n",
    "                            mu = req_distribution_params[0]\n",
    "                        if len(req_distribution_params) > 1:\n",
    "                            sigma = req_distribution_params[1]\n",
    "                        \n",
    "                        self.reqs_generators[req_type] = NormalDistribution(mu, sigma)\n",
    "                    \n",
    "        # Dynamic state  \n",
    "        self.current_means_split_across_hour_seconds = {} \n",
    "        for s in range(self.discretion_s):\n",
    "            self.current_means_split_across_hour_seconds[s] = 0\n",
    "        self.current_second_leftover_reqs = {}\n",
    "        for req_type, _ in self.reqs_types_ratios.items():\n",
    "            self.current_second_leftover_reqs[req_type] = -1\n",
    "        self.current_req_split_across_simulation_steps = {} \n",
    "        for req_type, _ in self.reqs_types_ratios.items():\n",
    "            ms_division = {}\n",
    "            for ms_bucket_id in range(1000 // self.simulation_step_ms):\n",
    "                ms_division[ms_bucket_id] = 0   \n",
    "            self.current_req_split_across_simulation_steps[req_type] = ms_division\n",
    "        self.current_hour = -1\n",
    "        self.cur_second_in_hour = -1\n",
    "        self.workload = {}    \n",
    "        \n",
    "    def generate_requests(self,\n",
    "                          timestamp_ms):\n",
    "        gen_reqs = []\n",
    "        \n",
    "        # Check if the split of the seasonal workload across the seconds of the hour is available\n",
    "        query_dt = datetime.fromtimestamp(int(timestamp_ms / 1000))\n",
    "        if not query_dt.hour == self.current_hour:\n",
    "            # Generate the split if not available\n",
    "            self.current_hour = query_dt.hour\n",
    "            \n",
    "            if len(self.monthly_vals) > 0:\n",
    "                month_id = 0\n",
    "                if query_dt.month in self.monthly_vals:\n",
    "                    month_id = query_dt.month\n",
    "                    \n",
    "                # TODO: currently only supported per hour vals\n",
    "                avg_reqs_val = self.monthly_vals[month_id][query_dt.weekday()][query_dt.hour]\n",
    "                if not self.discretion_s == 3600:\n",
    "                    raise ValueError('Currently, only hourly discretion is supported for the requests generation.')\n",
    "                else:\n",
    "                    for s in range(self.discretion_s):\n",
    "                        self.current_means_split_across_hour_seconds[s] = 0\n",
    "                        \n",
    "                    for _ in range(avg_reqs_val):  \n",
    "                        hour_sec_picked = np.random.randint(self.discretion_s)\n",
    "                        self.current_means_split_across_hour_seconds[hour_sec_picked] += 1\n",
    "        \n",
    "        # Generating initial number of requests for the current second\n",
    "        cur_second_in_hour = query_dt.minute * 60 + query_dt.second\n",
    "        avg_param = self.current_means_split_across_hour_seconds[cur_second_in_hour]\n",
    "        \n",
    "        if not self.cur_second_in_hour == cur_second_in_hour: \n",
    "            for key, _ in self.current_second_leftover_reqs.items():\n",
    "                self.current_second_leftover_reqs[key] = -1\n",
    "            self.cur_second_in_hour = cur_second_in_hour\n",
    "        \n",
    "        for req_type, ratio in self.reqs_types_ratios.items():\n",
    "            if self.current_second_leftover_reqs[req_type] < 0:\n",
    "                self.reqs_generators[req_type].set_avg_param(avg_param)\n",
    "                num_reqs = self.reqs_generators[req_type].generate()\n",
    "                req_types_reqs_num = int(ratio * num_reqs)\n",
    "                if req_types_reqs_num < 0:\n",
    "                    req_types_reqs_num = 0\n",
    "                    \n",
    "                self.current_second_leftover_reqs[req_type] = req_types_reqs_num\n",
    "                \n",
    "                for key, _ in self.current_req_split_across_simulation_steps[req_type].items():\n",
    "                    self.current_req_split_across_simulation_steps[req_type][key] = 0\n",
    "        \n",
    "                for _ in range(self.current_second_leftover_reqs[req_type]):\n",
    "                    ms_bucket_picked = np.random.randint(len(self.current_req_split_across_simulation_steps[req_type]))\n",
    "                    self.current_req_split_across_simulation_steps[req_type][ms_bucket_picked] += 1\n",
    "        \n",
    "        # Generating requests for the current simulation step\n",
    "        for req_type, ratio in self.reqs_types_ratios.items():\n",
    "            ms_bucket_picked = (timestamp_ms - int(query_dt.timestamp() * 1000) ) // self.simulation_step_ms\n",
    "            req_types_reqs_num = self.current_req_split_across_simulation_steps[req_type][ms_bucket_picked]\n",
    "\n",
    "            for i in range(req_types_reqs_num):\n",
    "                req = Request(req_type)\n",
    "                gen_reqs.append(req)\n",
    "                self.current_req_split_across_simulation_steps[req_type][ms_bucket_picked] -= 1\n",
    "                \n",
    "            if req_type in self.workload:\n",
    "                self.workload[req_type].append((timestamp_ms, req_types_reqs_num))\n",
    "            else:\n",
    "                self.workload[req_type] = [(timestamp_ms, req_types_reqs_num)]\n",
    "                \n",
    "        return gen_reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    def __init__(self,\n",
    "                 request_type,\n",
    "                 request_id = None):\n",
    "        # Static state\n",
    "        self.request_type = request_type\n",
    "        if request_id is None:\n",
    "            self.request_id = uuid.uuid1()\n",
    "        else:\n",
    "            self.request_id = request_id\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.processing_left_ms = 0\n",
    "        self.cumulative_time_ms = 0\n",
    "        self.upstream = True\n",
    "        self.replies_expected = 1 # to implement the fan-in on the level of service\n",
    "        \n",
    "    def set_downstream(self):\n",
    "        self.upstream = False\n",
    "    \n",
    "class RequestProcessingInfo:\n",
    "    def __init__(self,\n",
    "                 request_type,\n",
    "                 entry_service,\n",
    "                 processing_times,\n",
    "                 timeout_ms,\n",
    "                 request_size_b,\n",
    "                 response_size_b,\n",
    "                 request_operation_type):\n",
    "        \n",
    "        self.request_type = request_type\n",
    "        self.entry_service = entry_service\n",
    "        self.processing_times = processing_times\n",
    "        self.timeout_ms = timeout_ms\n",
    "        self.request_size_b = request_size_b\n",
    "        self.response_size_b = response_size_b\n",
    "        self.request_operation_type = request_operation_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkBuffer:\n",
    "    \"\"\"\n",
    "    Combines link and the buffer. The requests in the link advance when the step method is called.\n",
    "    If the processing time left (i.e. waiting on the link) is over, the request proceeds to the\n",
    "    buffer, where it can be extracted from for the further processing in a service.\n",
    "    If the used throughput reached the throughput limit, the request is lost. The same happens\n",
    "    if upon the transition to the buffer, the buffer is full for the given request type.\n",
    "    \n",
    "    Properties:\n",
    "    \n",
    "        As buffer:\n",
    "            capacity_by_request_type (dict):  holds capacity of the buffer by request type, in terms of\n",
    "                                              requests currently placed in the buffer\n",
    "                                              \n",
    "            [STUB] policy:                    policy used for moving requests in the buffer, e.g. FIFO/LIFO\n",
    "            \n",
    "            ********************************************************************************************************\n",
    "            \n",
    "            requests (collections.deque):     holds current requests in the buffer\n",
    "            \n",
    "            reqs_cnt (dict):                  holds current request count by the request type, used to rapidly check\n",
    "                                              if more requests of the given type can be accomodated in the buffer\n",
    "            \n",
    "        As link:\n",
    "            latency_ms (int):                 latency of the link in milliseconds, taken from the config\n",
    "            \n",
    "            throughput_mbps (int):            throughput of the link in Megabytes per sec, taken from the config\n",
    "            \n",
    "            request_processing_infos (dict):  holds requests processing information to compute the used\n",
    "                                              throughput etc.\n",
    "            \n",
    "            ********************************************************************************************************\n",
    "            \n",
    "            requests_in_transfer (list):      holds requests that are currently \"transferred\" by this link\n",
    "            \n",
    "            used_throughput_MBps (int):       throughput currently used on this link by the \"transferred\" reqs\n",
    "        \n",
    "    Methods:\n",
    "    \n",
    "        As buffer:\n",
    "            append_left (req):                puts the request req at the beginning of the buffer to give other\n",
    "                                              requests opportunity to be processed if the current request waits\n",
    "                                              for other replies to get processed (fan-in)\n",
    "            \n",
    "            pop:                              takes the last added request out of the buffer for processing (LIFO)\n",
    "            \n",
    "            pop_left:                         takes the first added request out of the buffer for processing (FIFO)\n",
    "            \n",
    "            size:                             returns size of the buffer\n",
    "            \n",
    "            add_cumulative_time (delta):      adds time delta to every request in the buffer\n",
    "            \n",
    "            remove_by_id (request_id):        removes all the requests with request id request_id from the buffer\n",
    "            \n",
    "        As link:\n",
    "            put (req):                        puts a new request req on a link, i.e. in requests_in_transfer,\n",
    "                                              if there is enough spare throughput; otherwise drops the request\n",
    "            \n",
    "            step (simulation_step_ms):        makes a discrete simulation time step of the length simulation_step_ms\n",
    "                                              to advance the requests held on the link, i.e. in requests_in_transfer,\n",
    "                                              and to put them into the buffer if possible (capacity left). In case of\n",
    "                                              no spare capacity in the buffer, the request is also dropped.\n",
    "        \n",
    "            _req_occupied_MBps (req):         private method that computes the throughput used by the request req\n",
    "    \n",
    "    TODO:\n",
    "        implementing scaling of the links? e.g. according to the added instances of services.\n",
    "        implement wrapping of the lower-level details into policies like LIFO, FIFO, etc. hide append, pop etc.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 capacity_by_request_type,\n",
    "                 request_processing_infos,\n",
    "                 latency_ms,\n",
    "                 throughput_MBps,\n",
    "                 policy = \"FIFO\"): \n",
    "        \n",
    "        # Static state\n",
    "        # Buffer:\n",
    "        self.capacity_by_request_type = capacity_by_request_type\n",
    "        self.policy = policy\n",
    "        \n",
    "        # Link:\n",
    "        self.latency_ms = latency_ms\n",
    "        self.throughput_MBps = throughput_MBps\n",
    "        self.request_processing_infos = request_processing_infos\n",
    "        \n",
    "        # Dynamic state\n",
    "        # Buffer:\n",
    "        self.requests = deque([])\n",
    "        self.reqs_cnt = {}\n",
    "        for request_type in capacity_by_request_type.keys():\n",
    "            self.reqs_cnt[request_type] = 0\n",
    "            \n",
    "        # Link:\n",
    "        self.requests_in_transfer = []\n",
    "        self.used_throughput_MBps = 0\n",
    "    \n",
    "    def step(self, simulation_step_ms):\n",
    "        \"\"\" Processing requests to bring them from the link into the buffer \"\"\"\n",
    "        for req in self.requests_in_transfer:\n",
    "            min_time_to_subtract_ms = min(req.processing_left_ms, simulation_step_ms)\n",
    "            req.processing_left_ms -= min_time_to_subtract_ms\n",
    "            if req.processing_left_ms <= 0:\n",
    "                capacity = self.capacity_by_request_type[req.request_type]\n",
    "                self.used_throughput_MBps -= self._req_occupied_MBps(req)\n",
    "                \n",
    "                if self.reqs_cnt[req.request_type] == capacity:\n",
    "                    del req # dropping the request if no spare capacity\n",
    "                else:\n",
    "                    req.cumulative_time_ms += min_time_to_subtract_ms\n",
    "                    if req.cumulative_time_ms >= self.request_processing_infos[req.request_type].timeout_ms:\n",
    "                        del req\n",
    "                    else:\n",
    "                        self.requests.append(req)\n",
    "                        self.reqs_cnt[req.request_type] += 1 \n",
    "    \n",
    "    def put(self, req):\n",
    "        req_size_b_MBps = self._req_occupied_MBps(req)\n",
    "\n",
    "        if self.throughput_MBps - self.used_throughput_MBps >= req_size_b_MBps:\n",
    "            self.used_throughput_MBps += req_size_b_MBps\n",
    "            req.processing_left_ms = self.latency_ms\n",
    "            self.requests_in_transfer.append(req)\n",
    "        else:\n",
    "            del req\n",
    "    \n",
    "    def append_left(self, req):\n",
    "        self.requests.appendLeft(req)\n",
    "    \n",
    "    def pop(self):\n",
    "        req = None\n",
    "        if len(self.requests) > 0:\n",
    "            req = self.requests.pop()\n",
    "            self.reqs_cnt[req.request_type] -= 1\n",
    "            \n",
    "        return req\n",
    "    \n",
    "    def pop_left(self):\n",
    "        req = None\n",
    "        if len(self.requests) > 0:\n",
    "            req = self.requests.popLeft()\n",
    "            self.reqs_cnt[req.request_type] -= 1\n",
    "        \n",
    "        return req\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.requests)\n",
    "    \n",
    "    def add_cumulative_time(self, delta):\n",
    "        for req in self.requests:\n",
    "            req.cumulative_time_ms += delta\n",
    "    \n",
    "    def remove_by_id(self, request_id):\n",
    "        for req in reversed(self.requests):\n",
    "            if req.request_id == request_id:\n",
    "                self.requests.remove(req)\n",
    "                \n",
    "    def _req_occupied_MBps(self, req):\n",
    "        req_size_b = 0\n",
    "        if req.upstream:\n",
    "            req_size_b = self.request_processing_infos[req.request_type].request_size_b\n",
    "        else:\n",
    "            req_size_b = self.request_processing_infos[req.request_type].response_size_b\n",
    "        req_size_b_mb = req_size_b / (1024 * 1024)\n",
    "        req_size_b_MBps = req_size_b_mb * (self.latency_ms / 1000) # taking channel for that long\n",
    "        return req_size_b_MBps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeInfo:\n",
    "    \"\"\"\n",
    "    Holds the static information about the node used to deploy the application, e.g. virtual machine.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_type,\n",
    "                 vCPU,\n",
    "                 memory,\n",
    "                 network_bandwidth_MBps,\n",
    "                 price_p_h = 0.0,\n",
    "                 cpu_credits_h = 0,\n",
    "                 latency_ms = 0):\n",
    "        \n",
    "        self.node_type = node_type\n",
    "        self.vCPU = vCPU\n",
    "        self.memory = memory\n",
    "        self.network_bandwidth_MBps = network_bandwidth_MBps\n",
    "        self.price_p_h = price_p_h\n",
    "        self.cpu_credits_h = cpu_credits_h\n",
    "        self.latency_ms = latency_ms\n",
    "    \n",
    "class PlatformModel:\n",
    "    \"\"\"\n",
    "    Defines the hardware/virtual platform underlying the application. Acts as a centralized storage\n",
    "    of the platform configuration, both static and dynamic. In terms of the dynamic state, it\n",
    "    tracks how many instances of each node are added/removed to later use these numbers to\n",
    "    reconstruct the overall utilization during the simulation time.\n",
    "    \n",
    "    Properties:\n",
    "    \n",
    "    Methods:\n",
    "    \n",
    "    TODO:\n",
    "        consider comparing against the quota/budget before issuing new nodes in get_new_nodes\n",
    "        consider introducing failure model here?\n",
    "        consider cross-cloud support / federated cloud\n",
    "        consider adding the desired vs actual distinction in the vms counts\n",
    "        consider introducing the randomization for the scaling times\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 starting_time_ms,\n",
    "                 platform_scaling_model,\n",
    "                 config_filename = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.node_types = {}\n",
    "        self.platform_scaling_model = platform_scaling_model\n",
    "        \n",
    "        if config_filename is None:\n",
    "            raise ValueError('Configuration file not provided for the PlatformModel.')\n",
    "        else:\n",
    "            with open(config_filename) as f:\n",
    "                config = json.load(f)\n",
    "                \n",
    "                for vm_type in config[\"vm_types\"]:\n",
    "                    type_name = vm_type[\"type\"]\n",
    "                    \n",
    "                    self.node_types[type_name] = NodeInfo(type_name,\n",
    "                                                          vm_type[\"vCPU\"],\n",
    "                                                          vm_type[\"memory\"],\n",
    "                                                          vm_type[\"network_bandwidth_MBps\"],\n",
    "                                                          vm_type[\"price_p_h\"],\n",
    "                                                          vm_type[\"cpu_credits_h\"],\n",
    "                                                          vm_type[\"latency_ms\"])\n",
    "                    \n",
    "        # Dynamic state\n",
    "        # timeline that won't change\n",
    "        self.nodes_state = {}\n",
    "        for node_type, node_info in self.node_types.items():\n",
    "            self.nodes_state[node_type] = {}\n",
    "            self.nodes_state[node_type][starting_time_ms] = 0\n",
    "            # format of val: [node_type][<timestamp>] = <+/-><delta_num>\n",
    "        # schedule timeline that is subject to invalidation\n",
    "        # format of val: [service_name][node_type][<timestamp>] = <+/-><delta_num>\n",
    "        self.scheduled_nodes_state_per_service = {}\n",
    "        # desired state timelines\n",
    "        self.desired_nodes_state = {}\n",
    "        for node_type, node_info in self.node_types.items():\n",
    "            self.desired_nodes_state[node_type] = {}\n",
    "            self.desired_nodes_state[node_type][starting_time_ms] = 0\n",
    "        self.scheduled_desired_nodes_state_per_service = {}\n",
    "    \n",
    "    def get_new_nodes(self,\n",
    "                      simulation_timestamp_ms, \n",
    "                      service_name, \n",
    "                      desired_timestamp_ms,\n",
    "                      provider,\n",
    "                      node_type,\n",
    "                      count):\n",
    "\n",
    "        num_added = count\n",
    "        \n",
    "        adjustment_ms = self.platform_scaling_model.get_boot_up_ms(provider,\n",
    "                                                                   node_type)\n",
    "        return self._update_scaling_events(simulation_timestamp_ms,\n",
    "                                           service_name,\n",
    "                                           desired_timestamp_ms,\n",
    "                                           provider,\n",
    "                                           node_type,\n",
    "                                           num_added,\n",
    "                                           adjustment_ms)\n",
    "                \n",
    "    def remove_nodes(self,\n",
    "                     simulation_timestamp_ms, \n",
    "                     service_name, \n",
    "                     desired_timestamp_ms,\n",
    "                     provider,\n",
    "                     node_type,\n",
    "                     count):\n",
    "        \n",
    "        num_removed = -count\n",
    "        \n",
    "        adjustment_ms = self.platform_scaling_model.get_tear_down_ms(provider,\n",
    "                                                                     node_type)\n",
    "        return self._update_scaling_events(simulation_timestamp_ms,\n",
    "                                           service_name,\n",
    "                                           desired_timestamp_ms,\n",
    "                                           provider,\n",
    "                                           node_type,\n",
    "                                           num_removed,\n",
    "                                           adjustment_ms)\n",
    "    \n",
    "    def compute_usage(self,\n",
    "                      simulation_step_ms,\n",
    "                      simulation_end_ms):\n",
    "        # Converting the up/down changes in the number of vms into cur number per sim step\n",
    "        nodes_usage = {}\n",
    "        \n",
    "        for node_type, delta_line in self.nodes_state.items():\n",
    "            if len(delta_line) > 1: # filtering only those node types that were used\n",
    "                nodes_usage[node_type] = {\"timestamps\": [], \"count\": []}\n",
    "                \n",
    "        for node_type in nodes_usage.keys():\n",
    "            next_event_id = 1\n",
    "            next_timestamp = list(self.nodes_state[node_type].keys())[0]\n",
    "            latest_count = self.nodes_state[node_type][next_timestamp]\n",
    "            timestamp_cur = next_timestamp\n",
    "            \n",
    "            while timestamp_cur <= simulation_end_ms:\n",
    "                \n",
    "                event_cnt = self.nodes_state[node_type].get(timestamp_cur)\n",
    "                if not event_cnt is None:\n",
    "                    latest_count += event_cnt\n",
    "                    if latest_count < 0:\n",
    "                        latest_count = 0\n",
    "                \n",
    "                nodes_usage[node_type][\"timestamps\"].append(timestamp_cur)\n",
    "                nodes_usage[node_type][\"count\"].append(latest_count)\n",
    "                \n",
    "                timestamp_cur += simulation_step_ms\n",
    "                \n",
    "        return nodes_usage\n",
    "    \n",
    "    def _update_scaling_events(self,\n",
    "                               simulation_timestamp_ms,\n",
    "                               service_name,\n",
    "                               desired_timestamp_ms,\n",
    "                               provider,\n",
    "                               node_type,\n",
    "                               delta,\n",
    "                               adjustment_ms):\n",
    "        # In case of reactive autoscaling:\n",
    "        # simulation_timestamp_ms = desired_timestamp_ms\n",
    "        ts_adjusted = desired_timestamp_ms + adjustment_ms\n",
    "        \n",
    "        # 1. Maintaining schedules\n",
    "        # Updating the scheduled state to be enacted (not yet in force)\n",
    "        self._update_schedule(service_name,\n",
    "                              node_type,\n",
    "                              self.scheduled_nodes_state_per_service,\n",
    "                              delta,\n",
    "                              ts_adjusted)\n",
    "        \n",
    "        # Updating the desired scheduled state (not yet in force)\n",
    "        self._update_schedule(service_name,\n",
    "                              node_type,\n",
    "                              self.scheduled_desired_nodes_state_per_service,\n",
    "                              delta,\n",
    "                              desired_timestamp_ms)\n",
    "        \n",
    "        # 2. Maintaining histories\n",
    "        # Updating the enacted history (in force)\n",
    "        self._update_scaling_history(simulation_timestamp_ms,\n",
    "                                     service_name,\n",
    "                                     node_type,\n",
    "                                     self.scheduled_nodes_state_per_service,\n",
    "                                     self.nodes_state)\n",
    "        \n",
    "        # Updating the desired history (in force)\n",
    "        self._update_scaling_history(simulation_timestamp_ms,\n",
    "                                     service_name,\n",
    "                                     node_type,\n",
    "                                     self.scheduled_desired_nodes_state_per_service,\n",
    "                                     self.desired_nodes_state)\n",
    "        \n",
    "        return (ts_adjusted, self.node_types[node_type], delta)\n",
    "    \n",
    "    def _update_schedule(self,\n",
    "                         service_name,\n",
    "                         node_type,\n",
    "                         schedule,\n",
    "                         delta,\n",
    "                         cutoff_ts):\n",
    "        \n",
    "        if service_name in schedule:\n",
    "            for scheduled_ts_ms in reversed(list(schedule[service_name][node_type].keys())):\n",
    "                # invalidating scheduled scaling events that occur later or\n",
    "                # at the same time\n",
    "                if cutoff_ts <= scheduled_ts_ms:\n",
    "                    del schedule[service_name][node_type][scheduled_ts_ms]     \n",
    "        else:\n",
    "            schedule[service_name] = {}\n",
    "            schedule[service_name][node_type] = {}\n",
    "        \n",
    "        schedule[service_name][node_type][cutoff_ts] = delta\n",
    "    \n",
    "    def _update_scaling_history(self,\n",
    "                                simulation_timestamp_ms,\n",
    "                                service_name,\n",
    "                                node_type,\n",
    "                                schedule,\n",
    "                                history):\n",
    "        \n",
    "        for scheduled_ts_ms in reversed(list(schedule[service_name][node_type].keys())):\n",
    "            if scheduled_ts_ms <= simulation_timestamp_ms:\n",
    "                scheduled_correction = schedule[service_name][node_type][scheduled_ts_ms]\n",
    "        \n",
    "                if not scheduled_ts_ms in history[node_type]:\n",
    "                    history[node_type][scheduled_ts_ms] = scheduled_correction\n",
    "                else:\n",
    "                    history[node_type][scheduled_ts_ms] += scheduled_correction\n",
    "        \n",
    "                del schedule[service_name][node_type][scheduled_ts_ms]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Service:\n",
    "    \"\"\"\n",
    "    \n",
    "    TODO:\n",
    "        implement simulation of the different OS scheduling disciplines like CFS, currently assuming\n",
    "        that the request takes the thread and does not let it go until its processing is finished\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 service_name,\n",
    "                 threads_per_service_instance,\n",
    "                 buffer_capacity_by_request_type,\n",
    "                 deployment_model,\n",
    "                 request_processing_infos,\n",
    "                 service_instances,\n",
    "                 platform_model_access_point,\n",
    "                 joint_scaling_policy,\n",
    "                 platform_scaling_policy,\n",
    "                 service_instances_scaling_policy,\n",
    "                 application_scaling_model,\n",
    "                 state_mb = 0,\n",
    "                 res_util_metrics_avg_interval_ms = 500):\n",
    "        \n",
    "        # Static state\n",
    "        self.service_name = service_name\n",
    "        self.threads_per_node_instance = deployment_model.node_info.vCPU\n",
    "        self.threads_per_service_instance = threads_per_service_instance\n",
    "        self.res_util_metrics_avg_interval_ms = res_util_metrics_avg_interval_ms\n",
    "        # If state_mb is 0, then the service is stateless\n",
    "        self.state_mb = state_mb\n",
    "        \n",
    "        # Dynamic state\n",
    "        # Upstream and downstream links/buffers of the service    \n",
    "        self.upstream_buf = LinkBuffer(buffer_capacity_by_request_type,\n",
    "                                       request_processing_infos,\n",
    "                                       deployment_model.node_info.latency_ms,\n",
    "                                       deployment_model.node_info.network_bandwidth_MBps)\n",
    "        self.downstream_buf = LinkBuffer(buffer_capacity_by_request_type,\n",
    "                                         request_processing_infos,\n",
    "                                         deployment_model.node_info.latency_ms,\n",
    "                                         deployment_model.node_info.network_bandwidth_MBps)\n",
    "        # Scaling-related\n",
    "        self.promised_next_platform_state = {\"next_ts\": 0,\n",
    "                                             \"next_count\": 0}\n",
    "        self.promised_next_service_state = {\"next_ts\": 0,\n",
    "                                            \"next_count\": 0}\n",
    "        self.service_instances = service_instances\n",
    "        self.node_count = deployment_model.node_count\n",
    "        self.res_util_tmp_buffer = []\n",
    "        self.res_util_avg = {}\n",
    "        self.res_util_avg[\"cpu\"] = []\n",
    "        \n",
    "        pl_scaling_pol = platform_scaling_policy(platform_model_access_point,\n",
    "                                                 self.service_name,\n",
    "                                                 deployment_model.provider,\n",
    "                                                 deployment_model.node_info,\n",
    "                                                 node_capacity_in_metric_units = 1,\n",
    "                                                 utilization_target_ratio = 0.4,\n",
    "                                                 node_instances_scaling_step = 1,\n",
    "                                                 cooldown_period_ms = 0,\n",
    "                                                 past_observations_considered = 10)\n",
    "        \n",
    "        boot_up_ms = application_scaling_model.get_service_scaling_params(self.service_name).boot_up_ms\n",
    "        # TODO: consider adding service_instances_scaling_step = 1,\n",
    "        service_inst_scaling_policy = service_instances_scaling_policy(boot_up_ms,\n",
    "                                                                       threads_per_service_instance)\n",
    "\n",
    "        self.service_scaling_policy = joint_scaling_policy(pl_scaling_pol,\n",
    "                                                           service_inst_scaling_policy)\n",
    "        \n",
    "        # requests that are currently in simultaneous processing\n",
    "        self.in_processing_simultaneous = []\n",
    "        # requests that are processed in this step, they can proceed\n",
    "        self.out = []\n",
    "        \n",
    "    def add_request(self, req):\n",
    "        # decide where to put the request\n",
    "        if req.upstream:\n",
    "            self.upstream_buf.put(req)\n",
    "        else:\n",
    "            self.downstream_buf.put(req)\n",
    "        \n",
    "    def step(self,\n",
    "             simulation_time_ms,\n",
    "             simulation_step_ms):\n",
    "        \n",
    "        # Adjusting the service and platform capacity based on the \n",
    "        # autoscaler's scaling results.\n",
    "        if self.promised_next_platform_state[\"next_ts\"] == simulation_time_ms:\n",
    "            self.node_count = self.promised_next_platform_state[\"next_count\"]\n",
    "            \n",
    "        if self.promised_next_service_state[\"next_ts\"] == simulation_time_ms:\n",
    "            self.service_instances = self.promised_next_service_state[\"next_count\"]\n",
    "        \n",
    "        processing_time_left_at_step = simulation_step_ms\n",
    "        \n",
    "        # Propagating requests in the link\n",
    "        self.downstream_buf.step(simulation_step_ms)  \n",
    "        self.upstream_buf.step(simulation_step_ms)\n",
    "        \n",
    "        while(processing_time_left_at_step > 0):\n",
    "           \n",
    "            if (self.downstream_buf.size() == 0) and (self.upstream_buf.size() == 0):\n",
    "                processing_time_left_at_step = 0\n",
    "                continue\n",
    "                \n",
    "            if len(self.in_processing_simultaneous) > 0:\n",
    "                # Find minimal leftover duration, subtract it,\n",
    "                # and propagate the request\n",
    "                min_leftover_time = min([req.processing_left_ms for req in self.in_processing_simultaneous])\n",
    "                min_time_to_subtract = min(min_leftover_time, processing_time_left_at_step)\n",
    "                new_in_processing_simultaneous = []\n",
    "                \n",
    "                for req in self.in_processing_simultaneous:\n",
    "                    new_time_left = req.processing_left_ms - min_time_to_subtract\n",
    "                    req.cumulative_time_ms += min_time_to_subtract\n",
    "                    if new_time_left > 0:\n",
    "                        req.processing_left_ms = new_time_left\n",
    "                        new_in_processing_simultaneous.append(req)\n",
    "                    else:\n",
    "                        # Request is put into the out buffer to be\n",
    "                        # processed further according to the app structure\n",
    "                        req.processing_left_ms = 0  \n",
    "                        self.out.append(req)\n",
    "                \n",
    "                processing_time_left_at_step -= min_time_to_subtract\n",
    "                self.in_processing_simultaneous = new_in_processing_simultaneous\n",
    "                \n",
    "            spare_capacity = self._compute_current_capacity_in_threads()\n",
    "            \n",
    "            # Assumption: first we try to process the downstream reqs to\n",
    "            # provide the response faster, but overall it is application-dependent\n",
    "            while ((self.downstream_buf.size() > 0) or (self.upstream_buf.size() > 0)) and (spare_capacity > 0):\n",
    "                if self.downstream_buf.size() > 0:\n",
    "                    req = self.downstream_buf.requests[-1]\n",
    "                    # Processing fan-in case\n",
    "                    if req.replies_expected > 1:\n",
    "                        req_id_ref = req.request_id\n",
    "                        reqs_present = 1\n",
    "                        for req_lookup in self.downstream_buf.requests[:-1]:\n",
    "                            if req_lookup.request_id == req_id_ref:\n",
    "                                reqs_present += 1\n",
    "                                \n",
    "                        if reqs_present == req.replies_expected:\n",
    "                            req = self.downstream_buf.pop()\n",
    "                            # Removing all the related requests\n",
    "                            self.downstream_buf.remove_by_id(req_id_ref)\n",
    "                        else:\n",
    "                            # pushing to the beginning of the deque to enable\n",
    "                            # progress in processing the downstream reqs\n",
    "                            req = self.downstream_buf.pop()\n",
    "                            self.downstream_buf.append_left(req)\n",
    "                            req = None\n",
    "                            \n",
    "                    else: \n",
    "                        req = self.downstream_buf.pop()\n",
    "                    \n",
    "                    if not req is None:\n",
    "                        req.replies_expected = 1\n",
    "                        self.in_processing_simultaneous.append(req)\n",
    "                        spare_capacity = self._compute_current_capacity_in_threads()\n",
    "            \n",
    "                if self.upstream_buf.size() > 0:\n",
    "                    req = self.upstream_buf.pop()\n",
    "                    self.in_processing_simultaneous.append(req)\n",
    "                    spare_capacity = self._compute_current_capacity_in_threads()\n",
    "            \n",
    "        # Increase the cumulative time for all the reqs left in the buffers waiting\n",
    "        self.upstream_buf.add_cumulative_time(simulation_step_ms)\n",
    "        self.downstream_buf.add_cumulative_time(simulation_step_ms)\n",
    "        \n",
    "        self._compute_res_util_cpu(simulation_step_ms)\n",
    "        \n",
    "        at_least_metric_vals = self.res_util_metrics_avg_interval_ms // simulation_step_ms\n",
    "        # Reconciling the promised state based on what autoscaler says\n",
    "        if len(self.res_util_avg[\"cpu\"]) >= at_least_metric_vals:\n",
    "            cur_service_state = ServiceState(self.service_name,\n",
    "                                             self.service_instances,\n",
    "                                             self.node_count,\n",
    "                                             self.res_util_avg)\n",
    "\n",
    "            next_service_state = self.service_scaling_policy.reconcile_service_state(simulation_time_ms,\n",
    "                                                                                     cur_service_state)\n",
    "            if not next_service_state is None:\n",
    "                self.promised_next_platform_state = next_service_state[\"node_instances\"]\n",
    "                self.promised_next_service_state = next_service_state[\"service_instances\"]\n",
    "\n",
    "    def _compute_current_capacity_in_threads(self):\n",
    "        return int(self.node_count * self.threads_per_node_instance - len(self.in_processing_simultaneous) * self.threads_per_service_instance)\n",
    "    \n",
    "    def _compute_res_util_cpu(self,\n",
    "                              simulation_step_ms):\n",
    "        \n",
    "        if len(self.res_util_tmp_buffer) == int(self.res_util_metrics_avg_interval_ms / simulation_step_ms):\n",
    "            self.res_util_avg[\"cpu\"].append(np.mean(self.res_util_tmp_buffer))\n",
    "            self.res_util_tmp_buffer = []\n",
    "        \n",
    "        thread_util_percentage = (len(self.in_processing_simultaneous) * self.threads_per_service_instance) / (self.node_count * self.threads_per_node_instance)\n",
    "        self.res_util_tmp_buffer.append(thread_util_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentModel:\n",
    "    \"\"\"\n",
    "    Summarizes parameters that are relevant for the initial deployment.\n",
    "    \n",
    "    TODO:\n",
    "        consider deployment that does not start straight away; may require adjustment of the\n",
    "        application model to check the schedule of the deployment for particular services.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 provider,\n",
    "                 node_info,\n",
    "                 node_count):\n",
    "        \n",
    "        self.provider = provider\n",
    "        self.node_info = node_info\n",
    "        self.node_count = node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeScalingInfo:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_type,\n",
    "                 boot_up_ms,\n",
    "                 tear_down_ms):\n",
    "        \n",
    "        self.node_type = node_type\n",
    "        self.boot_up_ms = boot_up_ms\n",
    "        self.tear_down_ms = tear_down_ms\n",
    "\n",
    "class PlatformScalingInfo:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 provider,\n",
    "                 decision_making_time_ms,\n",
    "                 link_added_throughput_coef_per_vm,\n",
    "                 node_scaling_infos_raw):\n",
    "        \n",
    "        self.provider = provider\n",
    "        self.decision_making_time_ms = decision_making_time_ms\n",
    "        self.link_added_throughput_coef_per_vm = link_added_throughput_coef_per_vm\n",
    "        self.node_scaling_infos = {}\n",
    "        \n",
    "        for node_scaling_info_raw in node_scaling_infos_raw:\n",
    "            nsi = NodeScalingInfo(node_scaling_info_raw[\"type\"],\n",
    "                                  node_scaling_info_raw[\"boot_up_ms\"],\n",
    "                                  node_scaling_info_raw[\"tear_down_ms\"])\n",
    "            self.node_scaling_infos[node_scaling_info_raw[\"type\"]] = nsi\n",
    "\n",
    "class PlatformScalingModel:\n",
    "    \"\"\"\n",
    "    Wraps the logic of the platform's nodes scaling by providing the relevant\n",
    "    scaling parameters when queried s.a. booting and tear down times. The\n",
    "    scaling parameters provided are governed by the configuration file, but\n",
    "    are subject to the randomness, i.e. they are drawn from the normal\n",
    "    distribution with the mu coefficient corresponding to the parameter\n",
    "    defined in the scaling configuration file.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 simulation_step_ms,\n",
    "                 sigma_ms = 10):\n",
    "        \n",
    "        self.platform_scaling_infos = {}\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        self.sigma_ms = 10\n",
    "    \n",
    "    def add_provider(self,\n",
    "                     provider = \"on-premise\",\n",
    "                     decision_making_time_ms = 0,\n",
    "                     link_added_throughput_coef_per_vm = 1,\n",
    "                     node_scaling_infos_raw = []):\n",
    "        \n",
    "        psi = PlatformScalingInfo(provider,\n",
    "                                  decision_making_time_ms,\n",
    "                                  link_added_throughput_coef_per_vm,\n",
    "                                  node_scaling_infos_raw)\n",
    "        self.platform_scaling_infos[provider] = psi\n",
    "    \n",
    "    def get_boot_up_ms(self,\n",
    "                       provider,\n",
    "                       node_type):\n",
    "        \n",
    "        mu = self.platform_scaling_infos[provider].node_scaling_infos[node_type].boot_up_ms\n",
    "        boot_up_ms = self.simulation_step_ms\n",
    "        raw_boot_up_ms = np.random.normal(mu, self.sigma_ms, 1)\n",
    "        if raw_boot_up_ms > boot_up_ms:\n",
    "            boot_up_ms = int(math.ceil(raw_boot_up_ms / self.simulation_step_ms)) * self.simulation_step_ms\n",
    "            \n",
    "        return boot_up_ms\n",
    "        \n",
    "    def get_tear_down_ms(self,\n",
    "                         provider,\n",
    "                         node_type):\n",
    "        \n",
    "        mu = self.platform_scaling_infos[provider].node_scaling_infos[node_type].tear_down_ms\n",
    "        tear_down_ms = self.simulation_step_ms\n",
    "        raw_tear_down_ms = np.random.normal(mu, self.sigma_ms, 1)\n",
    "        if raw_tear_down_ms > tear_down_ms:\n",
    "            tear_down_ms = int(math.ceil(raw_tear_down_ms / self.simulation_step_ms)) * self.simulation_step_ms\n",
    "            \n",
    "        return tear_down_ms\n",
    "            \n",
    "class ServiceScalingInfo:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 boot_up_ms):\n",
    "        \n",
    "        self.boot_up_ms = boot_up_ms\n",
    "        \n",
    "class ApplicationScalingModel:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 decision_making_time_ms = 0,\n",
    "                 service_scaling_infos_raw = []):\n",
    "        \n",
    "        self.decision_making_time_ms = decision_making_time_ms\n",
    "        self.service_scaling_infos = {}\n",
    "        \n",
    "        for service_scaling_info_raw in service_scaling_infos_raw:\n",
    "            ssi = ServiceScalingInfo(service_scaling_info_raw[\"boot_up_ms\"])\n",
    "            \n",
    "            self.service_scaling_infos[service_scaling_info_raw[\"name\"]] = ssi\n",
    "            \n",
    "    def get_service_scaling_params(self,\n",
    "                                   service_name):\n",
    "        ssi = None\n",
    "        if service_name in self.service_scaling_infos:\n",
    "            ssi = self.service_scaling_infos[service_name]\n",
    "            \n",
    "        return ssi\n",
    "\n",
    "class ScalingModel:\n",
    "    \"\"\"\n",
    "    Defines the scaling behaviour that does not depend upon the scaling policy, i.e. represents \n",
    "    unmanaged scaling characteristics such as booting times for VMs or start-up times for service\n",
    "    instances. Encompasses two parts, one related to the Platform Model, the other related to the\n",
    "    Services in the Application Model.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 simulation_step_ms,\n",
    "                 config_filename = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.platform_scaling_model = PlatformScalingModel(simulation_step_ms)\n",
    "        self.application_scaling_model = None\n",
    "        \n",
    "        if config_filename is None:\n",
    "            raise ValueError('Configuration file not provided for the ApplicationModel.')\n",
    "        else:\n",
    "            with open(config_filename) as f:\n",
    "                config = json.load(f)\n",
    "                \n",
    "                # 1. Filling into the platform scaling information\n",
    "                # Defaults\n",
    "                provider = \"on-premise\"\n",
    "                decision_making_time_ms = 0\n",
    "                link_added_throughput_coef_per_vm = 1\n",
    "                nodes_scaling_infos_raw = []\n",
    "                \n",
    "                for platform_i in config[\"platform\"]:\n",
    "                    \n",
    "                    if \"provider\" in platform_i.keys():\n",
    "                        provider = platform_i[\"provider\"]\n",
    "                    if \"decision_making_time_ms\" in platform_i.keys():\n",
    "                        decision_making_time_ms = platform_i[\"decision_making_time_ms\"]\n",
    "                    if \"link_added_throughput_coef_per_vm\" in platform_i.keys():\n",
    "                        link_added_throughput_coef_per_vm = platform_i[\"link_added_throughput_coef_per_vm\"]\n",
    "                    if \"nodes\" in platform_i.keys():\n",
    "                        nodes_scaling_infos_raw = platform_i[\"nodes\"]\n",
    "                    \n",
    "                    self.platform_scaling_model.add_provider(provider,\n",
    "                                                             decision_making_time_ms,\n",
    "                                                             link_added_throughput_coef_per_vm,\n",
    "                                                             nodes_scaling_infos_raw)\n",
    "                    \n",
    "                # 2. Filling into the application scaling information\n",
    "                # Defaults\n",
    "                decision_making_time_ms = 0\n",
    "                service_scaling_infos_raw = []\n",
    "                \n",
    "                if \"decision_making_time_ms\" in config[\"application\"].keys():\n",
    "                    decision_making_time_ms = config[\"application\"][\"decision_making_time_ms\"]\n",
    "                if \"services\" in config[\"application\"].keys():\n",
    "                    service_scaling_infos_raw = config[\"application\"][\"services\"]\n",
    "                    \n",
    "                self.application_scaling_model = ApplicationScalingModel(decision_making_time_ms,\n",
    "                                                                         service_scaling_infos_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServiceState:\n",
    "    \"\"\"\n",
    "    Contains information relevant to conduct the scaling. The state should be\n",
    "    updated at each simulation step and provided to the ServiceScalingPolicyHierarchy\n",
    "    s.t. the scaling decision could be taken. The information stored in the\n",
    "    ServiceState is diverse and satisfies any type of scaling policy that\n",
    "    could be used, be it utilization-based or workload-based policy, reactive\n",
    "    or predictive, etc.\n",
    "    \n",
    "    TODO:\n",
    "        add properties for workload-based scaling + predictive\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 service_name,\n",
    "                 cur_service_instances,\n",
    "                 cur_node_instances,\n",
    "                 tracked_metrics_util_vals):\n",
    "        \n",
    "        self.service_name = service_name\n",
    "        self.cur_service_instances = cur_service_instances\n",
    "        self.cur_node_instances = cur_node_instances\n",
    "        self.tracked_metrics_util_vals = tracked_metrics_util_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.UtilizationCentricServiceScalingPolicyHierarchy"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ServiceScalingPolicyHierarchy(ABC):\n",
    "    \"\"\"\n",
    "    Wraps scaling policies for the service, both on the level of the service\n",
    "    instances and on the level of the infrastructure underlying the service.\n",
    "    The procedure for the hierarchical scaling (service and infrastructure)\n",
    "    depends on whether the scaling is workload-centric or utilization-centric.\n",
    "    \n",
    "    In the former case, one starts with determining the desired amount of \n",
    "    service instances, and follows up with the infrastructure trying to \n",
    "    accommodate them. This kind of scaling is tightly related to the\n",
    "    application-level goals, i.e. user-facing SLOs like response time.\n",
    "    \n",
    "    In the latter case, one aims to meet the resource utilization goal(s),\n",
    "    and hence starts with determining the desired amount of nodes to push\n",
    "    the utilization towards the desired level, following up by filling the\n",
    "    whole available capacity with the service instances.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 infrastructure_scaling_policy,\n",
    "                 service_scaling_policy):\n",
    "        \n",
    "        self.infrastructure_scaling_policy = infrastructure_scaling_policy\n",
    "        self.service_scaling_policy = service_scaling_policy\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reconcile_service_state(self,\n",
    "                                cur_simulation_time_ms,\n",
    "                                service_state):\n",
    "        pass\n",
    "    \n",
    "class WorkloadCentricServiceScalingPolicyHierarchy(ServiceScalingPolicyHierarchy):\n",
    "    \"\"\"\n",
    "    Workload-centric service scaling aims to meet SLOs for the service and\n",
    "    emphasizes the *workload* and its characteristics during the scaling.\n",
    "    The process is as follows:\n",
    "    1) the desired number of service instances is determined (either reactive\n",
    "    or predictive);\n",
    "    2) for the desired number of service instances from (1), the desired number of\n",
    "    nodes is determined in a purely reactive manner.\n",
    "    \n",
    "    TODO:\n",
    "        implement\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 infrastructure_scaling_policy,\n",
    "                 service_scaling_policy):\n",
    "        \n",
    "        super().__init__(infrastructure_scaling_policy,\n",
    "                         service_scaling_policy)\n",
    "        \n",
    "    def reconcile_service_state(self,\n",
    "                                cur_simulation_time_ms,\n",
    "                                service_state):\n",
    "        pass\n",
    "    \n",
    "class UtilizationCentricServiceScalingPolicyHierarchy(ServiceScalingPolicyHierarchy):\n",
    "    \"\"\"\n",
    "    Utilization-centric service scaling aims to meet the resource utilization goals\n",
    "    for the infrastructure underlying the service and emphasizes the *resource\n",
    "    utilization* during the scaling.\n",
    "    The process is as follows:\n",
    "    1) the desired number of nodes to meet the infrastructure utilization goal\n",
    "    is determined and scheduled (either reactive or predictive);\n",
    "    2) the actually deployed number of nodes is filled with the service instances\n",
    "    to maximize the use of the provided infra capacity since it is anyway paid for.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 infrastructure_scaling_policy,\n",
    "                 service_scaling_policy):\n",
    "        \n",
    "        super().__init__(infrastructure_scaling_policy,\n",
    "                         service_scaling_policy)\n",
    "        \n",
    "    def reconcile_service_state(self,\n",
    "                                cur_simulation_time_ms,\n",
    "                                service_state):\n",
    "        \n",
    "        next_platform_state_ts_ms, node_info, future_node_instances = self.infrastructure_scaling_policy.reconcile_platform_state(cur_simulation_time_ms,\n",
    "                                                                                                                                  service_state.cur_node_instances,\n",
    "                                                                                                                                  service_state.tracked_metrics_util_vals)\n",
    "\n",
    "        # If a scaling action should be perfomed on the infrastructure\n",
    "        next_service_state = None\n",
    "        if not next_platform_state_ts_ms is None:\n",
    "            next_service_state_ts_ms, future_service_instances = self.service_scaling_policy.reconcile_service_instances_state(next_platform_state_ts_ms,\n",
    "                                                                                                                               node_info,\n",
    "                                                                                                                               future_node_instances)\n",
    "        \n",
    "            next_service_state = {}\n",
    "            next_service_state[\"node_instances\"] = {\"next_ts\": next_platform_state_ts_ms,\n",
    "                                                    \"next_count\": future_node_instances}\n",
    "            next_service_state[\"service_instances\"] = {\"next_ts\": next_service_state_ts_ms,\n",
    "                                                       \"next_count\": future_service_instances}\n",
    "        \n",
    "        return next_service_state\n",
    "    \n",
    "ServiceScalingPolicyHierarchy.register(WorkloadCentricServiceScalingPolicyHierarchy)\n",
    "ServiceScalingPolicyHierarchy.register(UtilizationCentricServiceScalingPolicyHierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReactiveServiceScalingPolicy:\n",
    "    \"\"\"\n",
    "    Since the running nodes\n",
    "    are anyway paid for, the mission of this policy is simply to populate them\n",
    "    with the service instances to take all the spare place.\n",
    "    \n",
    "    TODO:\n",
    "        consider more complex logic -- might be useful in workload-driven case\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 boot_up_ms,\n",
    "                 threads_per_service_instance):\n",
    "        \n",
    "        self.boot_up_ms = boot_up_ms\n",
    "        self.threads_per_service_instance = threads_per_service_instance\n",
    "        \n",
    "    def reconcile_service_instances_state(self,\n",
    "                                          next_platform_state_ts_ms,\n",
    "                                          node_info,\n",
    "                                          future_node_instances):\n",
    "        # TODO: consider tracking deployed service instances separately\n",
    "        # to account for ongoing requests\n",
    "        next_service_state_ts_ms = next_platform_state_ts_ms + self.boot_up_ms\n",
    "        future_service_instances = 0\n",
    "        for _ in range(future_node_instances):\n",
    "            future_service_instances_per_node = 1\n",
    "            if self.threads_per_service_instance < node_info.vCPU:\n",
    "                future_service_instances_per_node = math.ceil(node_info.vCPU / self.threads_per_service_instance)\n",
    "            \n",
    "            future_service_instances += future_service_instances_per_node\n",
    "        \n",
    "        return (next_service_state_ts_ms, future_service_instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CPUUtilizationBasedPlatformScalingPolicy"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# util-based (reactive/predictive):\n",
    "# - utilization metric (CPU util, mem util, net util, combined?)\n",
    "# - target in terms of util metric (80% util of CPU)\n",
    "# - capacity of nodes in terms of metric\n",
    "# - scaling step (service inst, node inst)\n",
    "# - cooldown\n",
    "#\n",
    "# workload-based (reactive/predictive):\n",
    "# - workload quantification metric (qps, throughput, combined?)\n",
    "# - target in terms of SLO (resp time limit)\n",
    "# - SLI (resp time)\n",
    "# - capacity of nodes in terms of workload (CHALLENGING!)\n",
    "# - scaling step\n",
    "# - cooldown?\n",
    "#\n",
    "    \n",
    "\n",
    "# The below classes are supposed to be used on a *per service* basis\n",
    "class PlatformScalingPolicy(ABC):\n",
    "    \"\"\"\n",
    "    The top class in the hierarchy of the scaling policies. Incorporates\n",
    "    the most general properties and methods of the scaling policy.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 platform_model,\n",
    "                 service_name,\n",
    "                 provider,\n",
    "                 node_info,\n",
    "                 node_instances_scaling_step,\n",
    "                 cooldown_period_ms):\n",
    "        \n",
    "        self.platform_model = platform_model\n",
    "        self.service_name = service_name\n",
    "        self.provider = provider\n",
    "        self.node_info = node_info\n",
    "        self.node_instances_scaling_step = node_instances_scaling_step\n",
    "        self.cooldown_period_ms = cooldown_period_ms\n",
    "\n",
    "class UtilizationMetric:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 metric_name,\n",
    "                 node_capacity_in_metric_units,\n",
    "                 utilization_target_ratio):\n",
    "        \n",
    "        self.metric_name = metric_name\n",
    "        self.node_capacity_in_metric_units = node_capacity_in_metric_units\n",
    "        self.utilization_target_ratio = utilization_target_ratio\n",
    "        \n",
    "class UtilizationBasedPlatformScalingPolicy(PlatformScalingPolicy):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 platform_model,\n",
    "                 service_name,\n",
    "                 provider,\n",
    "                 node_info,\n",
    "                 node_instances_scaling_step,\n",
    "                 cooldown_period_ms,\n",
    "                 past_observations_considered):\n",
    "        \n",
    "        # Static state\n",
    "        self.metric = \"\"\n",
    "        super().__init__(platform_model,\n",
    "                         service_name,\n",
    "                         provider,\n",
    "                         node_info,\n",
    "                         node_instances_scaling_step,\n",
    "                         cooldown_period_ms)\n",
    "        \n",
    "        self.past_observations_considered = past_observations_considered\n",
    "        self.utilization_metrics = {}\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reconcile_platform_state(self,\n",
    "                                 cur_simulation_time_ms,\n",
    "                                 cur_node_instances,\n",
    "                                 tracked_metrics_util_vals):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class CPUUtilizationBasedPlatformScalingPolicy(UtilizationBasedPlatformScalingPolicy):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "        consider splitting into reactive and predictive with own compute_instances methods\n",
    "        current version is reactive\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 platform_model,\n",
    "                 service_name,\n",
    "                 provider,\n",
    "                 node_info,\n",
    "                 node_capacity_in_metric_units = 1,\n",
    "                 utilization_target_ratio = 0.8,\n",
    "                 node_instances_scaling_step = 1,\n",
    "                 cooldown_period_ms = 0,\n",
    "                 past_observations_considered = 10):\n",
    "        \n",
    "        super().__init__(platform_model,\n",
    "                         service_name,\n",
    "                         provider,\n",
    "                         node_info,\n",
    "                         node_instances_scaling_step,\n",
    "                         cooldown_period_ms,\n",
    "                         past_observations_considered)\n",
    "        \n",
    "        self.metric = \"cpu\"\n",
    "        util_metric = UtilizationMetric(self.metric,\n",
    "                                        node_capacity_in_metric_units,\n",
    "                                        utilization_target_ratio)\n",
    "        \n",
    "        self.utilization_metrics[self.metric] = util_metric\n",
    "        \n",
    "    def reconcile_platform_state(self,\n",
    "                                 cur_simulation_time_ms,\n",
    "                                 cur_node_instances,\n",
    "                                 tracked_metrics_util_vals):\n",
    "        \n",
    "        if not self.metric in tracked_metrics_util_vals:\n",
    "            raise ValueError('Not possible to compute the desired state since no metric {} is provided for compute_instances()'.format(self.metric))\n",
    "        \n",
    "        # Reference for the formula:\n",
    "        # https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details\n",
    "        past_cpu_util = tracked_metrics_util_vals[self.metric][-self.past_observations_considered:]\n",
    "        stabilized_cpu_util = np.mean(past_cpu_util)\n",
    "        delta_nodes = math.ceil(cur_node_instances * (stabilized_cpu_util / self.utilization_metrics[self.metric].utilization_target_ratio)) - cur_node_instances\n",
    "        rounding_fn = None\n",
    "        if delta_nodes < 0:\n",
    "            rounding_fn = math.floor\n",
    "        else:\n",
    "            rounding_fn = math.ceil\n",
    "        \n",
    "        delta_nodes_adj = rounding_fn(delta_nodes / self.node_instances_scaling_step) * self.node_instances_scaling_step\n",
    "        # TODO: consider distinguishing between reactive/predictive\n",
    "        # currently only reactive policy\n",
    "        desired_scaling_timestamp_ms = cur_simulation_time_ms\n",
    "        # TODO: consider rescheduling of the requests currently being processed\n",
    "        # like waiting for the processing to finish and blocking new scheduling\n",
    "        timestamp_ms = None\n",
    "        node_info = None\n",
    "        future_node_instances = cur_node_instances\n",
    "        \n",
    "        # getting promises of nodes available to use future_node_instances\n",
    "        # at the timestamp timestamp_ms\n",
    "        real_delta = 0\n",
    "        if delta_nodes_adj > 0:\n",
    "\n",
    "            timestamp_ms, node_info, real_delta = self.platform_model.get_new_nodes(cur_simulation_time_ms,\n",
    "                                                                                    self.service_name,\n",
    "                                                                                    desired_scaling_timestamp_ms,\n",
    "                                                                                    self.provider,\n",
    "                                                                                    self.node_info.node_type,\n",
    "                                                                                    delta_nodes_adj)\n",
    "        elif delta_nodes_adj < 0:\n",
    "            \n",
    "            timestamp_ms, node_info, real_delta = self.platform_model.remove_nodes(cur_simulation_time_ms,\n",
    "                                                                                   self.service_name,\n",
    "                                                                                   desired_scaling_timestamp_ms + self.cooldown_period_ms,\n",
    "                                                                                   self.provider,\n",
    "                                                                                   self.node_info.node_type,\n",
    "                                                                                   -delta_nodes_adj)\n",
    "        future_node_instances += real_delta\n",
    "        \n",
    "        return (timestamp_ms, node_info, future_node_instances)\n",
    "\n",
    "PlatformScalingPolicy.register(UtilizationBasedPlatformScalingPolicy)\n",
    "UtilizationBasedPlatformScalingPolicy.register(CPUUtilizationBasedPlatformScalingPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "class ApplicationScalingPolicy:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application model incorps params of the requests since it depends on the\n",
    "# structure and the processing logic of the app, whereas the workload model\n",
    "# quantifies the amount and the distribution of the requests in time/volume\n",
    "# so, propagation chain of the request goes into the app model\n",
    "class ApplicationModel:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 starting_time_ms,\n",
    "                 platform_model,\n",
    "                 application_scaling_model,\n",
    "                 joint_scaling_policy = UtilizationCentricServiceScalingPolicyHierarchy,\n",
    "                 platform_scaling_policy = CPUUtilizationBasedPlatformScalingPolicy, \n",
    "                 service_instances_scaling_policy = ReactiveServiceScalingPolicy,\n",
    "                 filename = None):\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.new_requests = []\n",
    "        self.response_times_by_request = {}\n",
    "        self.platform_model = platform_model\n",
    "        \n",
    "        # Static state\n",
    "        self.name = None\n",
    "        self.application_scaling_model = application_scaling_model\n",
    "        self.services = {}\n",
    "        self.structure = {}\n",
    "        self.reqs_processing_infos = {}\n",
    "        \n",
    "        if filename is None:\n",
    "            raise ValueError('Configuration file not provided for the ApplicationModel.')\n",
    "        else:\n",
    "            with open(filename) as f:\n",
    "                config = json.load(f)\n",
    "                \n",
    "                self.name = config[\"app_name\"]\n",
    "                \n",
    "                for request_info in config[\"requests\"]:\n",
    "                    request_type = request_info[\"request_type\"]\n",
    "                    entry_service = request_info[\"entry_service\"]\n",
    "                    \n",
    "                    processing_times = {}\n",
    "                    for processing_time_service_entry in request_info[\"processing_times_ms\"]:\n",
    "                        service_name = processing_time_service_entry[\"service\"]\n",
    "                        upstream_time = processing_time_service_entry[\"upstream\"]\n",
    "                        if upstream_time < 0:\n",
    "                            raise ValueError('The upstream time for the request {} when passing through the service {} is negative.'.format(request_type, service_name))\n",
    "                        downstream_time = processing_time_service_entry[\"downstream\"]\n",
    "                        if downstream_time < 0:\n",
    "                            raise ValueError('The downstream time for the request {} when passing through the service {} is negative.'.format(request_type, service_name))\n",
    "                        \n",
    "                        processing_times[service_name] = [upstream_time, downstream_time]\n",
    "                    \n",
    "                    timeout_ms = request_info[\"timeout_ms\"]\n",
    "                    if timeout_ms < 0:\n",
    "                        raise ValueError('The timeout value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    request_size_b = request_info[\"request_size_b\"]\n",
    "                    if request_size_b < 0:\n",
    "                        raise ValueError('The request size value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    response_size_b = request_info[\"response_size_b\"]\n",
    "                    if response_size_b < 0:\n",
    "                        raise ValueError('The response size value for the request {} is negative.'.format(request_type))\n",
    "                    \n",
    "                    request_operation_type = request_info[\"operation_type\"]\n",
    "                    \n",
    "                    req_proc_info = RequestProcessingInfo(request_type,\n",
    "                                                          entry_service,\n",
    "                                                          processing_times,\n",
    "                                                          timeout_ms,\n",
    "                                                          request_size_b,\n",
    "                                                          response_size_b,\n",
    "                                                          request_operation_type)\n",
    "                    self.reqs_processing_infos[request_type] = req_proc_info\n",
    "                \n",
    "                for service_config in config[\"services\"]:\n",
    "                    # Creating & adding the service:\n",
    "                    service_name = service_config[\"name\"]\n",
    "                    \n",
    "                    buffer_capacity_by_request_type = {}\n",
    "                    for buffer_capacity_config in service_config[\"buffer_capacity_by_request_type\"]:\n",
    "                        request_type = buffer_capacity_config[\"request_type\"]\n",
    "                        capacity = buffer_capacity_config[\"capacity\"]\n",
    "                        if capacity <= 0:\n",
    "                            raise ValueError('Buffer capacity is not positive for request type {} of service {}.'.format(request_type, service_name))\n",
    "                        buffer_capacity_by_request_type[request_type] = capacity\n",
    "                        \n",
    "                    threads_per_instance = service_config[\"threads_per_instance\"]\n",
    "                    if threads_per_instance <= 0:\n",
    "                        raise ValueError('Threads per instance is not positive for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    starting_instances_num = service_config[\"starting_instances_num\"]\n",
    "                    if starting_instances_num <= 0:\n",
    "                        raise ValueError('Number of service instances to start with is not positive for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    state_mb = service_config[\"state_mb\"]\n",
    "                    if state_mb < 0:\n",
    "                        raise ValueError('The state size is negative for the service {}.'.format(service_name))\n",
    "                    \n",
    "                    # Grabbing deployment model for the service\n",
    "                    provider = service_config[\"deployment\"][\"provider\"]\n",
    "                    node_info = self.platform_model.node_types[service_config[\"deployment\"][\"vm_type\"]]\n",
    "                    node_count = service_config[\"deployment\"][\"count\"]\n",
    "                    deployment_model = DeploymentModel(provider, node_info, node_count)\n",
    "                    \n",
    "                    service = Service(service_name,\n",
    "                                      threads_per_instance,\n",
    "                                      buffer_capacity_by_request_type,\n",
    "                                      deployment_model,\n",
    "                                      self.reqs_processing_infos,\n",
    "                                      starting_instances_num,\n",
    "                                      self.platform_model, \n",
    "                                      joint_scaling_policy,\n",
    "                                      platform_scaling_policy, \n",
    "                                      service_instances_scaling_policy,\n",
    "                                      self.application_scaling_model,\n",
    "                                      state_mb)\n",
    "                    \n",
    "                    add_ts_ms, node_info, num_added = self.platform_model.get_new_nodes(starting_time_ms,\n",
    "                                                                                        service_name,\n",
    "                                                                                        starting_time_ms,\n",
    "                                                                                        provider,\n",
    "                                                                                        service_config[\"deployment\"][\"vm_type\"],\n",
    "                                                                                        node_count)\n",
    "                    if num_added < node_count:\n",
    "                        raise ValueError('Failed to deploy the service {}, insufficient number of nodes.'.format(service_name))\n",
    "                    \n",
    "                    self.services[service_name] = service\n",
    "                    \n",
    "                    # Adding the links of the given service to the structure.\n",
    "                    # TODO: think of whether the broken symmetry of the links\n",
    "                    # is appropriate.\n",
    "                    next_services = service_config[\"next\"]\n",
    "                    prev_services = service_config[\"prev\"]\n",
    "                    if len(next_services) == 0:\n",
    "                        next_services = None\n",
    "                    if len(prev_services) == 0:\n",
    "                        prev_services = None\n",
    "                    self.structure[service_name] = {'next': next_services, 'prev': prev_services}\n",
    "        \n",
    "    def step(self,\n",
    "             cur_simulation_time_ms,\n",
    "             simulation_step_ms):\n",
    "        \n",
    "        if len(self.new_requests) > 0:\n",
    "            for req in self.new_requests:\n",
    "                entry_service = self.reqs_processing_infos[req.request_type].entry_service\n",
    "                req.processing_left_ms = self.reqs_processing_infos[req.request_type].processing_times[entry_service][1]\n",
    "                self.services[entry_service].add_request(req)\n",
    "                \n",
    "            self.new_requests = []\n",
    "        \n",
    "        # Proceed through the services // fan-in merge and fan-out copying\n",
    "        # is done in the app logic since it knows the structure and times\n",
    "        # IMPORTANT: the simulation step should be small for the following\n",
    "        # processing to work correctly ~5-10 ms.\n",
    "        for service_name, service in self.services.items():\n",
    "            # Simulation step in service\n",
    "            service.step(cur_simulation_time_ms,\n",
    "                         simulation_step_ms)\n",
    "            \n",
    "            while len(service.out) > 0:\n",
    "                req = service.out.pop()\n",
    "                req_info = self.reqs_processing_infos[req.request_type]\n",
    "                \n",
    "                if req.upstream:\n",
    "                    next_services_names = self.structure[service_name]['next']\n",
    "                    if not next_services_names is None:\n",
    "                        for next_service_name in next_services_names:\n",
    "                            if next_service_name in req_info.processing_times:\n",
    "                                req_cpy = req\n",
    "                                req_cpy.processing_left_ms = req_info.processing_times[next_service_name][0]\n",
    "                                self.services[next_service_name].add_request(req_cpy)\n",
    "                    else:\n",
    "                        # Sending response\n",
    "                        req.upstream = False\n",
    "                        \n",
    "                if not req.upstream:\n",
    "                    prev_services_names = self.structure[service_name]['prev']\n",
    "                    \n",
    "                    if not prev_services_names is None:\n",
    "                        replies_expected = 0\n",
    "                        for prev_service_name in prev_services_names:\n",
    "                            if prev_service_name in req_info.processing_times:\n",
    "                                replies_expected += 1\n",
    "                        \n",
    "                        for prev_service_name in prev_services_names:\n",
    "                            if prev_service_name in req_info.processing_times:\n",
    "                                req_cpy = req\n",
    "                                req_cpy.processing_left_ms = req_info.processing_times[prev_service_name][1]\n",
    "                                req_cpy.replies_expected = replies_expected\n",
    "                                self.services[prev_service_name].add_request(req_cpy)\n",
    "                    else:\n",
    "                        # Response received by the user\n",
    "                        if req.request_type in self.response_times_by_request:\n",
    "                            self.response_times_by_request[req.request_type].append(req.cumulative_time_ms)\n",
    "                        else:\n",
    "                            self.response_times_by_request[req.request_type] = [req.cumulative_time_ms]\n",
    "                        del req\n",
    "                        \n",
    "    def enter_requests(self, new_requests):\n",
    "        self.new_requests = new_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    \"\"\"\n",
    "    Manages the high-level simulation process and stores all the simulation-relevant variables.\n",
    "    Before the time time_to_simulate_ms is reached, each simulation_step_ms milliseconds a new\n",
    "    simulation step is taken by calling _step method. At each simulations step:\n",
    "    1) new requests entering the simulation are generated by the workload_model,\n",
    "    2) the generated requests are added to the application_model,\n",
    "    3) a simulation step of the application model is taken, which implies taking the corresponding\n",
    "       simulation steps on its services and links/buffers.\n",
    "    If the results_dir is provided then the resulting response times for the requests and the\n",
    "    workload generated per timestamp is stored in a pickle file marked with the application name\n",
    "    taken from the config file and the date and time of the simulation.\n",
    "    The progress of the simulation is tracked and presented by the progress bar.\n",
    "    \n",
    "    Properties:\n",
    "    \n",
    "        workload_model:                   the model that is used to generate the workload\n",
    "        \n",
    "        application_model:                the application model that is simulated by calling its step() method\n",
    "        \n",
    "        time_to_simulate_ms (int):        the interval of time that should be simulated, in milliseconds\n",
    "        \n",
    "        simulation_step_ms (int):         the discretion of the simulation. IMPORTANT: in order to apporach the\n",
    "                                          asynchronous behaviour of the real applications as close as possible\n",
    "                                          this parameter should be as small as possible. Meaningful numbers for\n",
    "                                          this parameters are around 10-20 ms, smaller ones may result in\n",
    "                                          very large execution time of the simulation. Ideally, this step should\n",
    "                                          be not larger than the smallest time required to process any request\n",
    "                                          on link/in service.\n",
    "                                          \n",
    "        stat_updates_every_round (int):   every stat_updates_every_round<th> round a stat summary is printed.\n",
    "                                          If equals 0 then only the progress bar is available, which makes\n",
    "                                          impossible to observe the time spent at each stat_updates_every_round\n",
    "                                          rounds.\n",
    "                                          \n",
    "        results_dir (string):             a directory used to store the results of the simulation, i.e.\n",
    "                                          reponse times of individual requests and the time series of the\n",
    "                                          generated workload. If dir does not exist, it is created.\n",
    "        \n",
    "        ********************************************************************************************************\n",
    "        \n",
    "        cur_simulation_time_ms (int):     current simulation time in milliseconds; on the initialization of the\n",
    "                                          simulation it is set up to equal simulation_start_datetime converted\n",
    "                                          to ms. Increments by simulation_step_ms on each simulation step. \n",
    "                                          \n",
    "        sim_round (int):                  current simulation round.\n",
    "    \n",
    "    Methods:\n",
    "    \n",
    "        start:                            main simulation loop which continues until the simulation runs\n",
    "                                          out of time, i.e. cur_simulation_time_ms > time_to_simulate_ms.\n",
    "                                          \n",
    "        _step:                            a private simulation step method that should only be called\n",
    "                                          inside the start method.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 workload_model,\n",
    "                 application_model,\n",
    "                 simulation_start_datetime,\n",
    "                 time_to_simulate_days = 0.0005,\n",
    "                 simulation_step_ms = 10,\n",
    "                 stat_updates_every_round = 0,\n",
    "                 results_dir = None):\n",
    "        \n",
    "        # Static state\n",
    "        self.workload_model = workload_model\n",
    "        self.application_model = application_model\n",
    "        self.time_to_simulate_ms = int(simulation_start_datetime.timestamp() * 1000) + int(time_to_simulate_days * 24 * 60 * 60 * 1000)\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        self.stat_updates_every_round = stat_updates_every_round\n",
    "        self.results_dir = results_dir\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.cur_simulation_time_ms = int(simulation_start_datetime.timestamp() * 1000)\n",
    "        self.sim_round = 0\n",
    "        \n",
    "    def start(self):\n",
    "        \n",
    "        left_to_simulate_ms = self.time_to_simulate_ms - self.cur_simulation_time_ms\n",
    "        left_to_simulate_steps = left_to_simulate_ms // self.simulation_step_ms\n",
    "        \n",
    "        with tqdm(total = left_to_simulate_steps) as progress_bar:\n",
    "            while(self.cur_simulation_time_ms <= self.time_to_simulate_ms):\n",
    "                self.cur_simulation_time_ms += self.simulation_step_ms\n",
    "                self._step()\n",
    "                self.sim_round += 1\n",
    "                if self.stat_updates_every_round > 0:\n",
    "                    if self.sim_round % self.stat_updates_every_round == 0:\n",
    "                        left_to_simulate_ms = self.time_to_simulate_ms - self.cur_simulation_time_ms\n",
    "                        left_to_simulate_steps = left_to_simulate_ms // self.simulation_step_ms\n",
    "                        print('[{}] Left to simulate: {} min or {} steps'.format(datetime.now(),\n",
    "                                                                                 int(left_to_simulate_ms / (1000 * 60)),\n",
    "                                                                                 left_to_simulate_steps))\n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        # Storing the simulation results on a disk\n",
    "        if not self.results_dir is None:\n",
    "            filename = self.application_model.name + \"DT\" + datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\") + \".pkl\"\n",
    "            if not os.path.exists(self.results_dir):\n",
    "                os.mkdir(self.results_dir)\n",
    "                full_filename = os.path.join(results_dir, filename)\n",
    "                results_to_store = {\"response_times_by_request\": self.application_model.response_times_by_request,\n",
    "                                    \"workload\": self.workload_model.workload}\n",
    "                \n",
    "                with open(full_filename, 'wb') as f:\n",
    "                    pickle.dump(results_to_store, f)\n",
    "    \n",
    "    def _step(self):\n",
    "        new_requests = self.workload_model.generate_requests(self.cur_simulation_time_ms)\n",
    "        self.application_model.enter_requests(new_requests)\n",
    "        self.application_model.step(self.cur_simulation_time_ms,\n",
    "                                    self.simulation_step_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 1094/4319 [00:01<00:06, 479.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-15 20:53:51.250574] Left to simulate: 0 min or 3319 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 2030/4319 [00:05<00:15, 149.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-15 20:53:55.762703] Left to simulate: 0 min or 2319 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 3020/4319 [00:14<00:12, 102.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-15 20:54:04.047515] Left to simulate: 0 min or 1319 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 4017/4319 [00:24<00:03, 96.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-15 20:54:14.521171] Left to simulate: 0 min or 319 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4320it [00:27, 154.74it/s]                         \n"
     ]
    }
   ],
   "source": [
    "starting_time = datetime.now()\n",
    "starting_time_ms = int(starting_time.timestamp() * 1000)\n",
    "simulation_step_ms = 10\n",
    "wlm_test = WorkloadModel(simulation_step_ms, filename = 'experiments/test/workload.json')\n",
    "sclm_test = ScalingModel(simulation_step_ms, 'experiments/test/scaling.json')\n",
    "plm_test = PlatformModel(starting_time_ms,\n",
    "                         sclm_test.platform_scaling_model,\n",
    "                         'experiments/test/platform.json')\n",
    "appm_test = ApplicationModel(starting_time_ms,\n",
    "                             plm_test,\n",
    "                             sclm_test.application_scaling_model,\n",
    "                             UtilizationCentricServiceScalingPolicyHierarchy,\n",
    "                             CPUUtilizationBasedPlatformScalingPolicy, \n",
    "                             ReactiveServiceScalingPolicy,\n",
    "                             'experiments/test/application.json')\n",
    "sim = Simulation(wlm_test, appm_test, starting_time, stat_updates_every_round = 1000)\n",
    "sim.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t3.xlarge': {'timestamps': [1600196030026,\n",
       "   1600196030036,\n",
       "   1600196030046,\n",
       "   1600196030056,\n",
       "   1600196030066,\n",
       "   1600196030076,\n",
       "   1600196030086,\n",
       "   1600196030096,\n",
       "   1600196030106,\n",
       "   1600196030116,\n",
       "   1600196030126,\n",
       "   1600196030136,\n",
       "   1600196030146,\n",
       "   1600196030156,\n",
       "   1600196030166,\n",
       "   1600196030176,\n",
       "   1600196030186,\n",
       "   1600196030196,\n",
       "   1600196030206,\n",
       "   1600196030216,\n",
       "   1600196030226,\n",
       "   1600196030236,\n",
       "   1600196030246,\n",
       "   1600196030256,\n",
       "   1600196030266,\n",
       "   1600196030276,\n",
       "   1600196030286,\n",
       "   1600196030296,\n",
       "   1600196030306,\n",
       "   1600196030316,\n",
       "   1600196030326,\n",
       "   1600196030336,\n",
       "   1600196030346,\n",
       "   1600196030356,\n",
       "   1600196030366,\n",
       "   1600196030376,\n",
       "   1600196030386,\n",
       "   1600196030396,\n",
       "   1600196030406,\n",
       "   1600196030416,\n",
       "   1600196030426,\n",
       "   1600196030436,\n",
       "   1600196030446,\n",
       "   1600196030456,\n",
       "   1600196030466,\n",
       "   1600196030476,\n",
       "   1600196030486,\n",
       "   1600196030496,\n",
       "   1600196030506,\n",
       "   1600196030516,\n",
       "   1600196030526,\n",
       "   1600196030536,\n",
       "   1600196030546,\n",
       "   1600196030556,\n",
       "   1600196030566,\n",
       "   1600196030576,\n",
       "   1600196030586,\n",
       "   1600196030596,\n",
       "   1600196030606,\n",
       "   1600196030616,\n",
       "   1600196030626,\n",
       "   1600196030636,\n",
       "   1600196030646,\n",
       "   1600196030656,\n",
       "   1600196030666,\n",
       "   1600196030676,\n",
       "   1600196030686,\n",
       "   1600196030696,\n",
       "   1600196030706,\n",
       "   1600196030716,\n",
       "   1600196030726,\n",
       "   1600196030736,\n",
       "   1600196030746,\n",
       "   1600196030756,\n",
       "   1600196030766,\n",
       "   1600196030776,\n",
       "   1600196030786,\n",
       "   1600196030796,\n",
       "   1600196030806,\n",
       "   1600196030816,\n",
       "   1600196030826,\n",
       "   1600196030836,\n",
       "   1600196030846,\n",
       "   1600196030856,\n",
       "   1600196030866,\n",
       "   1600196030876,\n",
       "   1600196030886,\n",
       "   1600196030896,\n",
       "   1600196030906,\n",
       "   1600196030916,\n",
       "   1600196030926,\n",
       "   1600196030936,\n",
       "   1600196030946,\n",
       "   1600196030956,\n",
       "   1600196030966,\n",
       "   1600196030976,\n",
       "   1600196030986,\n",
       "   1600196030996,\n",
       "   1600196031006,\n",
       "   1600196031016,\n",
       "   1600196031026,\n",
       "   1600196031036,\n",
       "   1600196031046,\n",
       "   1600196031056,\n",
       "   1600196031066,\n",
       "   1600196031076,\n",
       "   1600196031086,\n",
       "   1600196031096,\n",
       "   1600196031106,\n",
       "   1600196031116,\n",
       "   1600196031126,\n",
       "   1600196031136,\n",
       "   1600196031146,\n",
       "   1600196031156,\n",
       "   1600196031166,\n",
       "   1600196031176,\n",
       "   1600196031186,\n",
       "   1600196031196,\n",
       "   1600196031206,\n",
       "   1600196031216,\n",
       "   1600196031226,\n",
       "   1600196031236,\n",
       "   1600196031246,\n",
       "   1600196031256,\n",
       "   1600196031266,\n",
       "   1600196031276,\n",
       "   1600196031286,\n",
       "   1600196031296,\n",
       "   1600196031306,\n",
       "   1600196031316,\n",
       "   1600196031326,\n",
       "   1600196031336,\n",
       "   1600196031346,\n",
       "   1600196031356,\n",
       "   1600196031366,\n",
       "   1600196031376,\n",
       "   1600196031386,\n",
       "   1600196031396,\n",
       "   1600196031406,\n",
       "   1600196031416,\n",
       "   1600196031426,\n",
       "   1600196031436,\n",
       "   1600196031446,\n",
       "   1600196031456,\n",
       "   1600196031466,\n",
       "   1600196031476,\n",
       "   1600196031486,\n",
       "   1600196031496,\n",
       "   1600196031506,\n",
       "   1600196031516,\n",
       "   1600196031526,\n",
       "   1600196031536,\n",
       "   1600196031546,\n",
       "   1600196031556,\n",
       "   1600196031566,\n",
       "   1600196031576,\n",
       "   1600196031586,\n",
       "   1600196031596,\n",
       "   1600196031606,\n",
       "   1600196031616,\n",
       "   1600196031626,\n",
       "   1600196031636,\n",
       "   1600196031646,\n",
       "   1600196031656,\n",
       "   1600196031666,\n",
       "   1600196031676,\n",
       "   1600196031686,\n",
       "   1600196031696,\n",
       "   1600196031706,\n",
       "   1600196031716,\n",
       "   1600196031726,\n",
       "   1600196031736,\n",
       "   1600196031746,\n",
       "   1600196031756,\n",
       "   1600196031766,\n",
       "   1600196031776,\n",
       "   1600196031786,\n",
       "   1600196031796,\n",
       "   1600196031806,\n",
       "   1600196031816,\n",
       "   1600196031826,\n",
       "   1600196031836,\n",
       "   1600196031846,\n",
       "   1600196031856,\n",
       "   1600196031866,\n",
       "   1600196031876,\n",
       "   1600196031886,\n",
       "   1600196031896,\n",
       "   1600196031906,\n",
       "   1600196031916,\n",
       "   1600196031926,\n",
       "   1600196031936,\n",
       "   1600196031946,\n",
       "   1600196031956,\n",
       "   1600196031966,\n",
       "   1600196031976,\n",
       "   1600196031986,\n",
       "   1600196031996,\n",
       "   1600196032006,\n",
       "   1600196032016,\n",
       "   1600196032026,\n",
       "   1600196032036,\n",
       "   1600196032046,\n",
       "   1600196032056,\n",
       "   1600196032066,\n",
       "   1600196032076,\n",
       "   1600196032086,\n",
       "   1600196032096,\n",
       "   1600196032106,\n",
       "   1600196032116,\n",
       "   1600196032126,\n",
       "   1600196032136,\n",
       "   1600196032146,\n",
       "   1600196032156,\n",
       "   1600196032166,\n",
       "   1600196032176,\n",
       "   1600196032186,\n",
       "   1600196032196,\n",
       "   1600196032206,\n",
       "   1600196032216,\n",
       "   1600196032226,\n",
       "   1600196032236,\n",
       "   1600196032246,\n",
       "   1600196032256,\n",
       "   1600196032266,\n",
       "   1600196032276,\n",
       "   1600196032286,\n",
       "   1600196032296,\n",
       "   1600196032306,\n",
       "   1600196032316,\n",
       "   1600196032326,\n",
       "   1600196032336,\n",
       "   1600196032346,\n",
       "   1600196032356,\n",
       "   1600196032366,\n",
       "   1600196032376,\n",
       "   1600196032386,\n",
       "   1600196032396,\n",
       "   1600196032406,\n",
       "   1600196032416,\n",
       "   1600196032426,\n",
       "   1600196032436,\n",
       "   1600196032446,\n",
       "   1600196032456,\n",
       "   1600196032466,\n",
       "   1600196032476,\n",
       "   1600196032486,\n",
       "   1600196032496,\n",
       "   1600196032506,\n",
       "   1600196032516,\n",
       "   1600196032526,\n",
       "   1600196032536,\n",
       "   1600196032546,\n",
       "   1600196032556,\n",
       "   1600196032566,\n",
       "   1600196032576,\n",
       "   1600196032586,\n",
       "   1600196032596,\n",
       "   1600196032606,\n",
       "   1600196032616,\n",
       "   1600196032626,\n",
       "   1600196032636,\n",
       "   1600196032646,\n",
       "   1600196032656,\n",
       "   1600196032666,\n",
       "   1600196032676,\n",
       "   1600196032686,\n",
       "   1600196032696,\n",
       "   1600196032706,\n",
       "   1600196032716,\n",
       "   1600196032726,\n",
       "   1600196032736,\n",
       "   1600196032746,\n",
       "   1600196032756,\n",
       "   1600196032766,\n",
       "   1600196032776,\n",
       "   1600196032786,\n",
       "   1600196032796,\n",
       "   1600196032806,\n",
       "   1600196032816,\n",
       "   1600196032826,\n",
       "   1600196032836,\n",
       "   1600196032846,\n",
       "   1600196032856,\n",
       "   1600196032866,\n",
       "   1600196032876,\n",
       "   1600196032886,\n",
       "   1600196032896,\n",
       "   1600196032906,\n",
       "   1600196032916,\n",
       "   1600196032926,\n",
       "   1600196032936,\n",
       "   1600196032946,\n",
       "   1600196032956,\n",
       "   1600196032966,\n",
       "   1600196032976,\n",
       "   1600196032986,\n",
       "   1600196032996,\n",
       "   1600196033006,\n",
       "   1600196033016,\n",
       "   1600196033026,\n",
       "   1600196033036,\n",
       "   1600196033046,\n",
       "   1600196033056,\n",
       "   1600196033066,\n",
       "   1600196033076,\n",
       "   1600196033086,\n",
       "   1600196033096,\n",
       "   1600196033106,\n",
       "   1600196033116,\n",
       "   1600196033126,\n",
       "   1600196033136,\n",
       "   1600196033146,\n",
       "   1600196033156,\n",
       "   1600196033166,\n",
       "   1600196033176,\n",
       "   1600196033186,\n",
       "   1600196033196,\n",
       "   1600196033206,\n",
       "   1600196033216,\n",
       "   1600196033226,\n",
       "   1600196033236,\n",
       "   1600196033246,\n",
       "   1600196033256,\n",
       "   1600196033266,\n",
       "   1600196033276,\n",
       "   1600196033286,\n",
       "   1600196033296,\n",
       "   1600196033306,\n",
       "   1600196033316,\n",
       "   1600196033326,\n",
       "   1600196033336,\n",
       "   1600196033346,\n",
       "   1600196033356,\n",
       "   1600196033366,\n",
       "   1600196033376,\n",
       "   1600196033386,\n",
       "   1600196033396,\n",
       "   1600196033406,\n",
       "   1600196033416,\n",
       "   1600196033426,\n",
       "   1600196033436,\n",
       "   1600196033446,\n",
       "   1600196033456,\n",
       "   1600196033466,\n",
       "   1600196033476,\n",
       "   1600196033486,\n",
       "   1600196033496,\n",
       "   1600196033506,\n",
       "   1600196033516,\n",
       "   1600196033526,\n",
       "   1600196033536,\n",
       "   1600196033546,\n",
       "   1600196033556,\n",
       "   1600196033566,\n",
       "   1600196033576,\n",
       "   1600196033586,\n",
       "   1600196033596,\n",
       "   1600196033606,\n",
       "   1600196033616,\n",
       "   1600196033626,\n",
       "   1600196033636,\n",
       "   1600196033646,\n",
       "   1600196033656,\n",
       "   1600196033666,\n",
       "   1600196033676,\n",
       "   1600196033686,\n",
       "   1600196033696,\n",
       "   1600196033706,\n",
       "   1600196033716,\n",
       "   1600196033726,\n",
       "   1600196033736,\n",
       "   1600196033746,\n",
       "   1600196033756,\n",
       "   1600196033766,\n",
       "   1600196033776,\n",
       "   1600196033786,\n",
       "   1600196033796,\n",
       "   1600196033806,\n",
       "   1600196033816,\n",
       "   1600196033826,\n",
       "   1600196033836,\n",
       "   1600196033846,\n",
       "   1600196033856,\n",
       "   1600196033866,\n",
       "   1600196033876,\n",
       "   1600196033886,\n",
       "   1600196033896,\n",
       "   1600196033906,\n",
       "   1600196033916,\n",
       "   1600196033926,\n",
       "   1600196033936,\n",
       "   1600196033946,\n",
       "   1600196033956,\n",
       "   1600196033966,\n",
       "   1600196033976,\n",
       "   1600196033986,\n",
       "   1600196033996,\n",
       "   1600196034006,\n",
       "   1600196034016,\n",
       "   1600196034026,\n",
       "   1600196034036,\n",
       "   1600196034046,\n",
       "   1600196034056,\n",
       "   1600196034066,\n",
       "   1600196034076,\n",
       "   1600196034086,\n",
       "   1600196034096,\n",
       "   1600196034106,\n",
       "   1600196034116,\n",
       "   1600196034126,\n",
       "   1600196034136,\n",
       "   1600196034146,\n",
       "   1600196034156,\n",
       "   1600196034166,\n",
       "   1600196034176,\n",
       "   1600196034186,\n",
       "   1600196034196,\n",
       "   1600196034206,\n",
       "   1600196034216,\n",
       "   1600196034226,\n",
       "   1600196034236,\n",
       "   1600196034246,\n",
       "   1600196034256,\n",
       "   1600196034266,\n",
       "   1600196034276,\n",
       "   1600196034286,\n",
       "   1600196034296,\n",
       "   1600196034306,\n",
       "   1600196034316,\n",
       "   1600196034326,\n",
       "   1600196034336,\n",
       "   1600196034346,\n",
       "   1600196034356,\n",
       "   1600196034366,\n",
       "   1600196034376,\n",
       "   1600196034386,\n",
       "   1600196034396,\n",
       "   1600196034406,\n",
       "   1600196034416,\n",
       "   1600196034426,\n",
       "   1600196034436,\n",
       "   1600196034446,\n",
       "   1600196034456,\n",
       "   1600196034466,\n",
       "   1600196034476,\n",
       "   1600196034486,\n",
       "   1600196034496,\n",
       "   1600196034506,\n",
       "   1600196034516,\n",
       "   1600196034526,\n",
       "   1600196034536,\n",
       "   1600196034546,\n",
       "   1600196034556,\n",
       "   1600196034566,\n",
       "   1600196034576,\n",
       "   1600196034586,\n",
       "   1600196034596,\n",
       "   1600196034606,\n",
       "   1600196034616,\n",
       "   1600196034626,\n",
       "   1600196034636,\n",
       "   1600196034646,\n",
       "   1600196034656,\n",
       "   1600196034666,\n",
       "   1600196034676,\n",
       "   1600196034686,\n",
       "   1600196034696,\n",
       "   1600196034706,\n",
       "   1600196034716,\n",
       "   1600196034726,\n",
       "   1600196034736,\n",
       "   1600196034746,\n",
       "   1600196034756,\n",
       "   1600196034766,\n",
       "   1600196034776,\n",
       "   1600196034786,\n",
       "   1600196034796,\n",
       "   1600196034806,\n",
       "   1600196034816,\n",
       "   1600196034826,\n",
       "   1600196034836,\n",
       "   1600196034846,\n",
       "   1600196034856,\n",
       "   1600196034866,\n",
       "   1600196034876,\n",
       "   1600196034886,\n",
       "   1600196034896,\n",
       "   1600196034906,\n",
       "   1600196034916,\n",
       "   1600196034926,\n",
       "   1600196034936,\n",
       "   1600196034946,\n",
       "   1600196034956,\n",
       "   1600196034966,\n",
       "   1600196034976,\n",
       "   1600196034986,\n",
       "   1600196034996,\n",
       "   1600196035006,\n",
       "   1600196035016,\n",
       "   1600196035026,\n",
       "   1600196035036,\n",
       "   1600196035046,\n",
       "   1600196035056,\n",
       "   1600196035066,\n",
       "   1600196035076,\n",
       "   1600196035086,\n",
       "   1600196035096,\n",
       "   1600196035106,\n",
       "   1600196035116,\n",
       "   1600196035126,\n",
       "   1600196035136,\n",
       "   1600196035146,\n",
       "   1600196035156,\n",
       "   1600196035166,\n",
       "   1600196035176,\n",
       "   1600196035186,\n",
       "   1600196035196,\n",
       "   1600196035206,\n",
       "   1600196035216,\n",
       "   1600196035226,\n",
       "   1600196035236,\n",
       "   1600196035246,\n",
       "   1600196035256,\n",
       "   1600196035266,\n",
       "   1600196035276,\n",
       "   1600196035286,\n",
       "   1600196035296,\n",
       "   1600196035306,\n",
       "   1600196035316,\n",
       "   1600196035326,\n",
       "   1600196035336,\n",
       "   1600196035346,\n",
       "   1600196035356,\n",
       "   1600196035366,\n",
       "   1600196035376,\n",
       "   1600196035386,\n",
       "   1600196035396,\n",
       "   1600196035406,\n",
       "   1600196035416,\n",
       "   1600196035426,\n",
       "   1600196035436,\n",
       "   1600196035446,\n",
       "   1600196035456,\n",
       "   1600196035466,\n",
       "   1600196035476,\n",
       "   1600196035486,\n",
       "   1600196035496,\n",
       "   1600196035506,\n",
       "   1600196035516,\n",
       "   1600196035526,\n",
       "   1600196035536,\n",
       "   1600196035546,\n",
       "   1600196035556,\n",
       "   1600196035566,\n",
       "   1600196035576,\n",
       "   1600196035586,\n",
       "   1600196035596,\n",
       "   1600196035606,\n",
       "   1600196035616,\n",
       "   1600196035626,\n",
       "   1600196035636,\n",
       "   1600196035646,\n",
       "   1600196035656,\n",
       "   1600196035666,\n",
       "   1600196035676,\n",
       "   1600196035686,\n",
       "   1600196035696,\n",
       "   1600196035706,\n",
       "   1600196035716,\n",
       "   1600196035726,\n",
       "   1600196035736,\n",
       "   1600196035746,\n",
       "   1600196035756,\n",
       "   1600196035766,\n",
       "   1600196035776,\n",
       "   1600196035786,\n",
       "   1600196035796,\n",
       "   1600196035806,\n",
       "   1600196035816,\n",
       "   1600196035826,\n",
       "   1600196035836,\n",
       "   1600196035846,\n",
       "   1600196035856,\n",
       "   1600196035866,\n",
       "   1600196035876,\n",
       "   1600196035886,\n",
       "   1600196035896,\n",
       "   1600196035906,\n",
       "   1600196035916,\n",
       "   1600196035926,\n",
       "   1600196035936,\n",
       "   1600196035946,\n",
       "   1600196035956,\n",
       "   1600196035966,\n",
       "   1600196035976,\n",
       "   1600196035986,\n",
       "   1600196035996,\n",
       "   1600196036006,\n",
       "   1600196036016,\n",
       "   1600196036026,\n",
       "   1600196036036,\n",
       "   1600196036046,\n",
       "   1600196036056,\n",
       "   1600196036066,\n",
       "   1600196036076,\n",
       "   1600196036086,\n",
       "   1600196036096,\n",
       "   1600196036106,\n",
       "   1600196036116,\n",
       "   1600196036126,\n",
       "   1600196036136,\n",
       "   1600196036146,\n",
       "   1600196036156,\n",
       "   1600196036166,\n",
       "   1600196036176,\n",
       "   1600196036186,\n",
       "   1600196036196,\n",
       "   1600196036206,\n",
       "   1600196036216,\n",
       "   1600196036226,\n",
       "   1600196036236,\n",
       "   1600196036246,\n",
       "   1600196036256,\n",
       "   1600196036266,\n",
       "   1600196036276,\n",
       "   1600196036286,\n",
       "   1600196036296,\n",
       "   1600196036306,\n",
       "   1600196036316,\n",
       "   1600196036326,\n",
       "   1600196036336,\n",
       "   1600196036346,\n",
       "   1600196036356,\n",
       "   1600196036366,\n",
       "   1600196036376,\n",
       "   1600196036386,\n",
       "   1600196036396,\n",
       "   1600196036406,\n",
       "   1600196036416,\n",
       "   1600196036426,\n",
       "   1600196036436,\n",
       "   1600196036446,\n",
       "   1600196036456,\n",
       "   1600196036466,\n",
       "   1600196036476,\n",
       "   1600196036486,\n",
       "   1600196036496,\n",
       "   1600196036506,\n",
       "   1600196036516,\n",
       "   1600196036526,\n",
       "   1600196036536,\n",
       "   1600196036546,\n",
       "   1600196036556,\n",
       "   1600196036566,\n",
       "   1600196036576,\n",
       "   1600196036586,\n",
       "   1600196036596,\n",
       "   1600196036606,\n",
       "   1600196036616,\n",
       "   1600196036626,\n",
       "   1600196036636,\n",
       "   1600196036646,\n",
       "   1600196036656,\n",
       "   1600196036666,\n",
       "   1600196036676,\n",
       "   1600196036686,\n",
       "   1600196036696,\n",
       "   1600196036706,\n",
       "   1600196036716,\n",
       "   1600196036726,\n",
       "   1600196036736,\n",
       "   1600196036746,\n",
       "   1600196036756,\n",
       "   1600196036766,\n",
       "   1600196036776,\n",
       "   1600196036786,\n",
       "   1600196036796,\n",
       "   1600196036806,\n",
       "   1600196036816,\n",
       "   1600196036826,\n",
       "   1600196036836,\n",
       "   1600196036846,\n",
       "   1600196036856,\n",
       "   1600196036866,\n",
       "   1600196036876,\n",
       "   1600196036886,\n",
       "   1600196036896,\n",
       "   1600196036906,\n",
       "   1600196036916,\n",
       "   1600196036926,\n",
       "   1600196036936,\n",
       "   1600196036946,\n",
       "   1600196036956,\n",
       "   1600196036966,\n",
       "   1600196036976,\n",
       "   1600196036986,\n",
       "   1600196036996,\n",
       "   1600196037006,\n",
       "   1600196037016,\n",
       "   1600196037026,\n",
       "   1600196037036,\n",
       "   1600196037046,\n",
       "   1600196037056,\n",
       "   1600196037066,\n",
       "   1600196037076,\n",
       "   1600196037086,\n",
       "   1600196037096,\n",
       "   1600196037106,\n",
       "   1600196037116,\n",
       "   1600196037126,\n",
       "   1600196037136,\n",
       "   1600196037146,\n",
       "   1600196037156,\n",
       "   1600196037166,\n",
       "   1600196037176,\n",
       "   1600196037186,\n",
       "   1600196037196,\n",
       "   1600196037206,\n",
       "   1600196037216,\n",
       "   1600196037226,\n",
       "   1600196037236,\n",
       "   1600196037246,\n",
       "   1600196037256,\n",
       "   1600196037266,\n",
       "   1600196037276,\n",
       "   1600196037286,\n",
       "   1600196037296,\n",
       "   1600196037306,\n",
       "   1600196037316,\n",
       "   1600196037326,\n",
       "   1600196037336,\n",
       "   1600196037346,\n",
       "   1600196037356,\n",
       "   1600196037366,\n",
       "   1600196037376,\n",
       "   1600196037386,\n",
       "   1600196037396,\n",
       "   1600196037406,\n",
       "   1600196037416,\n",
       "   1600196037426,\n",
       "   1600196037436,\n",
       "   1600196037446,\n",
       "   1600196037456,\n",
       "   1600196037466,\n",
       "   1600196037476,\n",
       "   1600196037486,\n",
       "   1600196037496,\n",
       "   1600196037506,\n",
       "   1600196037516,\n",
       "   1600196037526,\n",
       "   1600196037536,\n",
       "   1600196037546,\n",
       "   1600196037556,\n",
       "   1600196037566,\n",
       "   1600196037576,\n",
       "   1600196037586,\n",
       "   1600196037596,\n",
       "   1600196037606,\n",
       "   1600196037616,\n",
       "   1600196037626,\n",
       "   1600196037636,\n",
       "   1600196037646,\n",
       "   1600196037656,\n",
       "   1600196037666,\n",
       "   1600196037676,\n",
       "   1600196037686,\n",
       "   1600196037696,\n",
       "   1600196037706,\n",
       "   1600196037716,\n",
       "   1600196037726,\n",
       "   1600196037736,\n",
       "   1600196037746,\n",
       "   1600196037756,\n",
       "   1600196037766,\n",
       "   1600196037776,\n",
       "   1600196037786,\n",
       "   1600196037796,\n",
       "   1600196037806,\n",
       "   1600196037816,\n",
       "   1600196037826,\n",
       "   1600196037836,\n",
       "   1600196037846,\n",
       "   1600196037856,\n",
       "   1600196037866,\n",
       "   1600196037876,\n",
       "   1600196037886,\n",
       "   1600196037896,\n",
       "   1600196037906,\n",
       "   1600196037916,\n",
       "   1600196037926,\n",
       "   1600196037936,\n",
       "   1600196037946,\n",
       "   1600196037956,\n",
       "   1600196037966,\n",
       "   1600196037976,\n",
       "   1600196037986,\n",
       "   1600196037996,\n",
       "   1600196038006,\n",
       "   1600196038016,\n",
       "   1600196038026,\n",
       "   1600196038036,\n",
       "   1600196038046,\n",
       "   1600196038056,\n",
       "   1600196038066,\n",
       "   1600196038076,\n",
       "   1600196038086,\n",
       "   1600196038096,\n",
       "   1600196038106,\n",
       "   1600196038116,\n",
       "   1600196038126,\n",
       "   1600196038136,\n",
       "   1600196038146,\n",
       "   1600196038156,\n",
       "   1600196038166,\n",
       "   1600196038176,\n",
       "   1600196038186,\n",
       "   1600196038196,\n",
       "   1600196038206,\n",
       "   1600196038216,\n",
       "   1600196038226,\n",
       "   1600196038236,\n",
       "   1600196038246,\n",
       "   1600196038256,\n",
       "   1600196038266,\n",
       "   1600196038276,\n",
       "   1600196038286,\n",
       "   1600196038296,\n",
       "   1600196038306,\n",
       "   1600196038316,\n",
       "   1600196038326,\n",
       "   1600196038336,\n",
       "   1600196038346,\n",
       "   1600196038356,\n",
       "   1600196038366,\n",
       "   1600196038376,\n",
       "   1600196038386,\n",
       "   1600196038396,\n",
       "   1600196038406,\n",
       "   1600196038416,\n",
       "   1600196038426,\n",
       "   1600196038436,\n",
       "   1600196038446,\n",
       "   1600196038456,\n",
       "   1600196038466,\n",
       "   1600196038476,\n",
       "   1600196038486,\n",
       "   1600196038496,\n",
       "   1600196038506,\n",
       "   1600196038516,\n",
       "   1600196038526,\n",
       "   1600196038536,\n",
       "   1600196038546,\n",
       "   1600196038556,\n",
       "   1600196038566,\n",
       "   1600196038576,\n",
       "   1600196038586,\n",
       "   1600196038596,\n",
       "   1600196038606,\n",
       "   1600196038616,\n",
       "   1600196038626,\n",
       "   1600196038636,\n",
       "   1600196038646,\n",
       "   1600196038656,\n",
       "   1600196038666,\n",
       "   1600196038676,\n",
       "   1600196038686,\n",
       "   1600196038696,\n",
       "   1600196038706,\n",
       "   1600196038716,\n",
       "   1600196038726,\n",
       "   1600196038736,\n",
       "   1600196038746,\n",
       "   1600196038756,\n",
       "   1600196038766,\n",
       "   1600196038776,\n",
       "   1600196038786,\n",
       "   1600196038796,\n",
       "   1600196038806,\n",
       "   1600196038816,\n",
       "   1600196038826,\n",
       "   1600196038836,\n",
       "   1600196038846,\n",
       "   1600196038856,\n",
       "   1600196038866,\n",
       "   1600196038876,\n",
       "   1600196038886,\n",
       "   1600196038896,\n",
       "   1600196038906,\n",
       "   1600196038916,\n",
       "   1600196038926,\n",
       "   1600196038936,\n",
       "   1600196038946,\n",
       "   1600196038956,\n",
       "   1600196038966,\n",
       "   1600196038976,\n",
       "   1600196038986,\n",
       "   1600196038996,\n",
       "   1600196039006,\n",
       "   1600196039016,\n",
       "   1600196039026,\n",
       "   1600196039036,\n",
       "   1600196039046,\n",
       "   1600196039056,\n",
       "   1600196039066,\n",
       "   1600196039076,\n",
       "   1600196039086,\n",
       "   1600196039096,\n",
       "   1600196039106,\n",
       "   1600196039116,\n",
       "   1600196039126,\n",
       "   1600196039136,\n",
       "   1600196039146,\n",
       "   1600196039156,\n",
       "   1600196039166,\n",
       "   1600196039176,\n",
       "   1600196039186,\n",
       "   1600196039196,\n",
       "   1600196039206,\n",
       "   1600196039216,\n",
       "   1600196039226,\n",
       "   1600196039236,\n",
       "   1600196039246,\n",
       "   1600196039256,\n",
       "   1600196039266,\n",
       "   1600196039276,\n",
       "   1600196039286,\n",
       "   1600196039296,\n",
       "   1600196039306,\n",
       "   1600196039316,\n",
       "   1600196039326,\n",
       "   1600196039336,\n",
       "   1600196039346,\n",
       "   1600196039356,\n",
       "   1600196039366,\n",
       "   1600196039376,\n",
       "   1600196039386,\n",
       "   1600196039396,\n",
       "   1600196039406,\n",
       "   1600196039416,\n",
       "   1600196039426,\n",
       "   1600196039436,\n",
       "   1600196039446,\n",
       "   1600196039456,\n",
       "   1600196039466,\n",
       "   1600196039476,\n",
       "   1600196039486,\n",
       "   1600196039496,\n",
       "   1600196039506,\n",
       "   1600196039516,\n",
       "   1600196039526,\n",
       "   1600196039536,\n",
       "   1600196039546,\n",
       "   1600196039556,\n",
       "   1600196039566,\n",
       "   1600196039576,\n",
       "   1600196039586,\n",
       "   1600196039596,\n",
       "   1600196039606,\n",
       "   1600196039616,\n",
       "   1600196039626,\n",
       "   1600196039636,\n",
       "   1600196039646,\n",
       "   1600196039656,\n",
       "   1600196039666,\n",
       "   1600196039676,\n",
       "   1600196039686,\n",
       "   1600196039696,\n",
       "   1600196039706,\n",
       "   1600196039716,\n",
       "   1600196039726,\n",
       "   1600196039736,\n",
       "   1600196039746,\n",
       "   1600196039756,\n",
       "   1600196039766,\n",
       "   1600196039776,\n",
       "   1600196039786,\n",
       "   1600196039796,\n",
       "   1600196039806,\n",
       "   1600196039816,\n",
       "   1600196039826,\n",
       "   1600196039836,\n",
       "   1600196039846,\n",
       "   1600196039856,\n",
       "   1600196039866,\n",
       "   1600196039876,\n",
       "   1600196039886,\n",
       "   1600196039896,\n",
       "   1600196039906,\n",
       "   1600196039916,\n",
       "   1600196039926,\n",
       "   1600196039936,\n",
       "   1600196039946,\n",
       "   1600196039956,\n",
       "   1600196039966,\n",
       "   1600196039976,\n",
       "   1600196039986,\n",
       "   1600196039996,\n",
       "   1600196040006,\n",
       "   1600196040016,\n",
       "   ...],\n",
       "  'count': [0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   ...]}}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_to_simulate_ms = int(starting_time.timestamp() * 1000) + int(0.0005 * 24 * 60 * 60 * 1000)\n",
    "appm_test.platform_model.compute_usage(10, time_to_simulate_ms)\n",
    "#appm_test.platform_model.nodes_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4., 10., 11., 11., 11., 22., 11., 10., 11., 11., 23., 10., 11.,\n",
       "        11., 13.]),\n",
       " array([ 50.,  74.,  98., 122., 146., 170., 194., 218., 242., 266., 290.,\n",
       "        314., 338., 362., 386., 410.]),\n",
       " <a list of 15 Patch objects>)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n_bins = 15\n",
    "\n",
    "# Generate a normal distribution, center at x=0 and y=5\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs[0].hist(sim.application_model.response_times_by_request['auth'], bins=n_bins)\n",
    "axs[1].hist(sim.application_model.response_times_by_request['buy'], bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.median(sim.application_model.response_times_by_request['auth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider scaling links?\n",
    "class Link:\n",
    "    def __init__(self,\n",
    "                 simulation_step_ms,\n",
    "                 latency,\n",
    "                 bandwidth = 1000):\n",
    "        # Static state\n",
    "        self.latency = latency\n",
    "        self.bandwidth = bandwidth\n",
    "        self.simulation_step_ms = simulation_step_ms\n",
    "        \n",
    "        # Dynamic state\n",
    "        self.requests = [] \n",
    "        \n",
    "    def put_request(self, req):\n",
    "        if len(self.requests) < self.bandwidth:\n",
    "            req.processing_left_ms = self.latency\n",
    "            self.requests.append(req)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_requests(self):\n",
    "        # Called every simulation step thus updating reqs\n",
    "        reqs_to_give = []\n",
    "        for req in self.requests:\n",
    "            min_time_left = min(req.processing_left_ms, self.simulation_step_ms)\n",
    "            if req.processing_left_ms - min_time_left <= 0:\n",
    "                req.processing_left_ms = 0\n",
    "                req.cumulative_time_ms += min_time_left\n",
    "                reqs_to_give.append(req)\n",
    "            else:\n",
    "                req.processing_left_ms -= min_time_left\n",
    "                \n",
    "        return reqs_to_give\n",
    "\n",
    "class NetworkModel:\n",
    "    def __init__(self,\n",
    "                 links_dict_in = None):\n",
    "        self.links_dict_in = links_dict_in\n",
    "        self.links_dict_out = {}\n",
    "        \n",
    "        # TODO: from file\n",
    "        for link_start, outs in self.links_dict.items():\n",
    "            for out, link in outs.items():\n",
    "                if out in self.links_dict_out:\n",
    "                    self.links_dict_out[out].append(link)\n",
    "                else:\n",
    "                    self.links_dict_out[out] = [link]\n",
    "                    \n",
    "    def put_request(self,\n",
    "                    start_service_lbl,\n",
    "                    end_service_lbl,\n",
    "                    req):\n",
    "        \n",
    "        self.links_dict_in[start_service_lbl][end_service_lbl].put_request(req)\n",
    "    \n",
    "    def get_requests(self,\n",
    "                     end_service_lbl):\n",
    "        reqs = []\n",
    "        links = self.links_dict_out[end_service_lbl]\n",
    "        for link in links:\n",
    "            reqs.extend(link.get_requests())\n",
    "            \n",
    "        return reqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
